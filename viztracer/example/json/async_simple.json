{"traceEvents": [{"ph": "M", "pid": 32511, "tid": 32511, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 32511, "tid": 32511, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 32511, "tid": 32511, "ts": 25981274393.691, "dur": 1.3, "name": "_asyncio._get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274396.991, "dur": 0.1, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274397.191, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274398.191, "dur": 0.1, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274395.991, "dur": 2.5, "name": "iscoroutine (/usr/lib/python3.8/asyncio/coroutines.py:177)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274401.791, "dur": 0.3, "name": "str.rpartition", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274401.391, "dur": 1.0, "name": "parent (<frozen importlib._bootstrap>:389)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274403.491, "dur": 0.1, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274403.791, "dur": 0.4, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274403.191, "dur": 1.1, "name": "_handle_fromlist (<frozen importlib._bootstrap>:1017)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274407.191, "dur": 4.0, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:625)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274406.191, "dur": 5.3, "name": "__init__ (/usr/lib/python3.8/asyncio/unix_events.py:1341)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274399.991, "dur": 11.9, "name": "_init_event_loop_policy (/usr/lib/python3.8/asyncio/events.py:711)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274399.591, "dur": 12.5, "name": "get_event_loop_policy (/usr/lib/python3.8/asyncio/events.py:719)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274420.991, "dur": 4.2, "name": "time.get_clock_info", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274430.091, "dur": 0.1, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274430.391, "dur": 0.3, "name": "str.encode", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274429.791, "dur": 1.0, "name": "encode (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/os.py:748)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274429.391, "dur": 2.6, "name": "__getitem__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/os.py:670)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274428.891, "dur": 3.8, "name": "get (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_collections_abc.py:657)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274427.091, "dur": 5.8, "name": "_is_debug_mode (/usr/lib/python3.8/asyncio/coroutines.py:18)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274434.291, "dur": 0.2, "name": "is_running (/usr/lib/python3.8/asyncio/base_events.py:658)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274433.191, "dur": 1.4, "name": "set_debug (/usr/lib/python3.8/asyncio/base_events.py:1880)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274436.691, "dur": 2.2, "name": "__init__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274416.491, "dur": 22.8, "name": "__init__ (/usr/lib/python3.8/asyncio/base_events.py:386)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274443.991, "dur": 0.4, "name": "__init__ (/usr/lib/python3.8/selectors.py:63)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274442.691, "dur": 2.3, "name": "__init__ (/usr/lib/python3.8/selectors.py:209)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274440.991, "dur": 8.6, "name": "__init__ (/usr/lib/python3.8/selectors.py:347)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274454.991, "dur": 0.4, "name": "_thread.RLock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274454.391, "dur": 1.1, "name": "_acquireLock (/usr/lib/python3.8/logging/__init__.py:214)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274456.691, "dur": 1.3, "name": "getEffectiveLevel (/usr/lib/python3.8/logging/__init__.py:1663)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274458.691, "dur": 0.1, "name": "_thread.RLock.release", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274458.391, "dur": 0.5, "name": "_releaseLock (/usr/lib/python3.8/logging/__init__.py:223)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274452.391, "dur": 6.7, "name": "isEnabledFor (/usr/lib/python3.8/logging/__init__.py:1677)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274451.691, "dur": 7.5, "name": "debug (/usr/lib/python3.8/logging/__init__.py:1412)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274461.991, "dur": 6.8, "name": "_socket.socketpair", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274469.591, "dur": 0.02, "name": "_socket.socket.detach", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274470.591, "dur": 3.0, "name": "__init__ (/usr/lib/python3.8/socket.py:219)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274473.991, "dur": 0.02, "name": "_socket.socket.detach", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274474.191, "dur": 0.7, "name": "__init__ (/usr/lib/python3.8/socket.py:219)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274461.091, "dur": 14.1, "name": "socketpair (/usr/lib/python3.8/socket.py:558)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274476.491, "dur": 0.7, "name": "socket.setblocking", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274477.291, "dur": 0.2, "name": "socket.setblocking", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274478.291, "dur": 0.02, "name": "socket.fileno", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274479.491, "dur": 0.2, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274481.491, "dur": 1.5, "name": "_contextvars.copy_context", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274484.791, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274480.691, "dur": 6.0, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274488.091, "dur": 0.1, "name": "get_map (/usr/lib/python3.8/selectors.py:272)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274490.191, "dur": 0.1, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274489.891, "dur": 0.6, "name": "_fileobj_to_fd (/usr/lib/python3.8/selectors.py:21)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274489.491, "dur": 1.1, "name": "_fileobj_lookup (/usr/lib/python3.8/selectors.py:215)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274491.391, "dur": 1.0, "name": "str.format", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274488.791, "dur": 3.9, "name": "__getitem__ (/usr/lib/python3.8/selectors.py:69)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274493.091, "dur": 0.3, "name": "str.format", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274487.491, "dur": 6.1, "name": "get_key (/usr/lib/python3.8/selectors.py:180)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274496.291, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274496.191, "dur": 0.3, "name": "_fileobj_to_fd (/usr/lib/python3.8/selectors.py:21)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274496.091, "dur": 0.42, "name": "_fileobj_lookup (/usr/lib/python3.8/selectors.py:215)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274500.691, "dur": 0.3, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274500.291, "dur": 0.8, "name": "__new__ (<string>:1)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274495.291, "dur": 6.7, "name": "register (/usr/lib/python3.8/selectors.py:234)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274503.091, "dur": 2.8, "name": "select.epoll.register", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274494.591, "dur": 11.5, "name": "register (/usr/lib/python3.8/selectors.py:351)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274478.991, "dur": 27.7, "name": "_add_reader (/usr/lib/python3.8/asyncio/selector_events.py:257)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274459.991, "dur": 46.9, "name": "_make_self_pipe (/usr/lib/python3.8/asyncio/selector_events.py:106)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274511.791, "dur": 0.4, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274512.791, "dur": 0.1, "name": "dict.items", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274513.291, "dur": 0.02, "name": "dict.items", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274511.391, "dur": 2.0, "name": "update (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/weakref.py:284)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274508.691, "dur": 5.1, "name": "__init__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/weakref.py:102)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274415.391, "dur": 99.5, "name": "__init__ (/usr/lib/python3.8/asyncio/selector_events.py:54)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274413.691, "dur": 101.9, "name": "__init__ (/usr/lib/python3.8/asyncio/unix_events.py:53)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274412.491, "dur": 103.2, "name": "new_event_loop (/usr/lib/python3.8/asyncio/events.py:650)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274399.191, "dur": 116.6, "name": "new_event_loop (/usr/lib/python3.8/asyncio/events.py:756)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274516.691, "dur": 0.1, "name": "get_event_loop_policy (/usr/lib/python3.8/asyncio/events.py:719)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274518.991, "dur": 0.1, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274517.891, "dur": 1.4, "name": "set_event_loop (/usr/lib/python3.8/asyncio/events.py:644)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274517.291, "dur": 2.2, "name": "set_event_loop (/usr/lib/python3.8/asyncio/unix_events.py:1353)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274516.391, "dur": 3.2, "name": "set_event_loop (/usr/lib/python3.8/asyncio/events.py:751)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274520.391, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274520.991, "dur": 0.1, "name": "is_running (/usr/lib/python3.8/asyncio/base_events.py:658)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274521.291, "dur": 0.3, "name": "_asyncio._get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274520.791, "dur": 0.9, "name": "_check_running (/usr/lib/python3.8/asyncio/base_events.py:550)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274523.391, "dur": 1.2, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274522.491, "dur": 2.2, "name": "isfuture (/usr/lib/python3.8/asyncio/base_futures.py:13)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274525.891, "dur": 0.4, "name": "iscoroutine (/usr/lib/python3.8/asyncio/coroutines.py:177)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274526.991, "dur": 0.02, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274529.491, "dur": 0.02, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274532.491, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274533.991, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274533.391, "dur": 0.8, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274534.791, "dur": 0.1, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274532.891, "dur": 3.4, "name": "_call_soon (/usr/lib/python3.8/asyncio/base_events.py:738)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274532.291, "dur": 4.1, "name": "call_soon (/usr/lib/python3.8/asyncio/base_events.py:709)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274538.291, "dur": 0.2, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274537.491, "dur": 1.02, "name": "add (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:81)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274526.691, "dur": 12.8, "name": "create_task (/usr/lib/python3.8/asyncio/base_events.py:424)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274525.391, "dur": 14.3, "name": "ensure_future (/usr/lib/python3.8/asyncio/tasks.py:661)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274540.291, "dur": 0.1, "name": "_asyncio.Task.add_done_callback", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274540.991, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274541.391, "dur": 0.1, "name": "is_running (/usr/lib/python3.8/asyncio/base_events.py:658)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274541.691, "dur": 0.1, "name": "_asyncio._get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274541.291, "dur": 0.52, "name": "_check_running (/usr/lib/python3.8/asyncio/base_events.py:550)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274542.291, "dur": 0.7, "name": "_set_coroutine_origin_tracking (/usr/lib/python3.8/asyncio/base_events.py:1862)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274543.291, "dur": 0.2, "name": "_thread.get_ident", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274543.891, "dur": 0.7, "name": "sys.get_asyncgen_hooks", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274545.091, "dur": 1.2, "name": "sys.set_asyncgen_hooks", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274546.591, "dur": 0.5, "name": "_asyncio._set_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274547.991, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274549.591, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274549.791, "dur": 0.4, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274550.591, "dur": 2.2, "name": "select.epoll.poll", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274549.091, "dur": 4.1, "name": "select (/usr/lib/python3.8/selectors.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274553.891, "dur": 0.2, "name": "_process_events (/usr/lib/python3.8/asyncio/selector_events.py:586)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274554.891, "dur": 0.3, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274554.591, "dur": 0.62, "name": "time (/usr/lib/python3.8/asyncio/base_events.py:662)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274555.691, "dur": 0.2, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274556.791, "dur": 0.1, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274563.991, "dur": 0.2, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274564.591, "dur": 0.02, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274565.591, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274566.891, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274567.991, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274567.591, "dur": 0.6, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274568.491, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274567.191, "dur": 1.4, "name": "_call_soon (/usr/lib/python3.8/asyncio/base_events.py:738)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274566.791, "dur": 2.0, "name": "call_soon (/usr/lib/python3.8/asyncio/base_events.py:709)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274569.591, "dur": 0.1, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274569.191, "dur": 0.6, "name": "add (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:81)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274564.391, "dur": 5.7, "name": "create_task (/usr/lib/python3.8/asyncio/base_events.py:424)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274570.391, "dur": 0.3, "name": "_set_task_name (/usr/lib/python3.8/asyncio/tasks.py:88)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274563.791, "dur": 6.92, "name": "create_task (/usr/lib/python3.8/asyncio/tasks.py:376)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274571.291, "dur": 0.1, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274571.691, "dur": 0.02, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274572.091, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274572.791, "dur": 0.02, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274573.491, "dur": 0.02, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274573.191, "dur": 0.4, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274573.791, "dur": 0.1, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274572.991, "dur": 1.9, "name": "_call_soon (/usr/lib/python3.8/asyncio/base_events.py:738)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274572.691, "dur": 2.3, "name": "call_soon (/usr/lib/python3.8/asyncio/base_events.py:709)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274575.591, "dur": 0.02, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274575.291, "dur": 0.4, "name": "add (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:81)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274571.591, "dur": 4.3, "name": "create_task (/usr/lib/python3.8/asyncio/base_events.py:424)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274576.091, "dur": 0.02, "name": "_set_task_name (/usr/lib/python3.8/asyncio/tasks.py:88)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274571.191, "dur": 5.0, "name": "create_task (/usr/lib/python3.8/asyncio/tasks.py:376)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274576.591, "dur": 0.1, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274576.911, "dur": 0.08, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274577.391, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274577.991, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274578.791, "dur": 0.02, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274578.491, "dur": 0.4, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274579.091, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274578.191, "dur": 1.0, "name": "_call_soon (/usr/lib/python3.8/asyncio/base_events.py:738)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274577.891, "dur": 1.4, "name": "call_soon (/usr/lib/python3.8/asyncio/base_events.py:709)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274579.791, "dur": 0.1, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274579.491, "dur": 0.42, "name": "add (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:81)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274576.891, "dur": 3.2, "name": "create_task (/usr/lib/python3.8/asyncio/base_events.py:424)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274580.191, "dur": 0.1, "name": "_set_task_name (/usr/lib/python3.8/asyncio/tasks.py:88)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274576.491, "dur": 3.9, "name": "create_task (/usr/lib/python3.8/asyncio/tasks.py:376)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981274560.891, "dur": 20.0, "name": "main (/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py:6)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274558.591, "dur": 23.0, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274557.591, "dur": 24.3, "name": "_run (/usr/lib/python3.8/asyncio/events.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274547.691, "dur": 34.7, "name": "_run_once (/usr/lib/python3.8/asyncio/base_events.py:1784)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274582.991, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274583.891, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274584.091, "dur": 0.2, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274584.491, "dur": 0.6, "name": "select.epoll.poll", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274583.691, "dur": 1.6, "name": "select (/usr/lib/python3.8/selectors.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274585.591, "dur": 0.1, "name": "_process_events (/usr/lib/python3.8/asyncio/selector_events.py:586)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274585.891, "dur": 0.1, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274585.791, "dur": 0.3, "name": "time (/usr/lib/python3.8/asyncio/base_events.py:662)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274586.391, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274586.991, "dur": 0.02, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274589.891, "dur": 0.1, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274591.791, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274590.491, "dur": 1.6, "name": "create_future (/usr/lib/python3.8/asyncio/base_events.py:420)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274594.791, "dur": 0.1, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274594.691, "dur": 0.22, "name": "time (/usr/lib/python3.8/asyncio/base_events.py:662)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274595.991, "dur": 0.2, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274599.091, "dur": 0.3, "name": "_contextvars.copy_context", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274600.191, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274598.891, "dur": 1.6, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274597.991, "dur": 3.0, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:104)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274601.691, "dur": 0.3, "name": "_heapq.heappush", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274595.791, "dur": 7.3, "name": "call_at (/usr/lib/python3.8/asyncio/base_events.py:693)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274594.191, "dur": 9.3, "name": "call_later (/usr/lib/python3.8/asyncio/base_events.py:671)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274589.391, "dur": 14.5, "name": "sleep (/usr/lib/python3.8/asyncio/tasks.py:638)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981274588.591, "dur": 15.4, "name": "io_task (/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py:3)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274587.691, "dur": 16.6, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274587.291, "dur": 17.2, "name": "_run (/usr/lib/python3.8/asyncio/events.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274604.891, "dur": 0.1, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274606.891, "dur": 0.2, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274607.691, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274607.291, "dur": 0.6, "name": "create_future (/usr/lib/python3.8/asyncio/base_events.py:420)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274608.491, "dur": 0.1, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274608.391, "dur": 0.22, "name": "time (/usr/lib/python3.8/asyncio/base_events.py:662)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274609.191, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274610.291, "dur": 0.1, "name": "_contextvars.copy_context", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274610.791, "dur": 0.02, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274610.091, "dur": 0.8, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274609.691, "dur": 1.4, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:104)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274612.191, "dur": 0.3, "name": "__lt__ (/usr/lib/python3.8/asyncio/events.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274611.391, "dur": 1.2, "name": "_heapq.heappush", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274609.091, "dur": 3.6, "name": "call_at (/usr/lib/python3.8/asyncio/base_events.py:693)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274608.191, "dur": 4.8, "name": "call_later (/usr/lib/python3.8/asyncio/base_events.py:671)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274606.791, "dur": 6.4, "name": "sleep (/usr/lib/python3.8/asyncio/tasks.py:638)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981274606.391, "dur": 6.9, "name": "io_task (/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py:3)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274605.791, "dur": 7.7, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274605.391, "dur": 8.2, "name": "_run (/usr/lib/python3.8/asyncio/events.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274613.691, "dur": 0.1, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274615.391, "dur": 0.1, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274615.891, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274615.691, "dur": 0.4, "name": "create_future (/usr/lib/python3.8/asyncio/base_events.py:420)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274616.591, "dur": 0.02, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274616.491, "dur": 0.2, "name": "time (/usr/lib/python3.8/asyncio/base_events.py:662)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274617.091, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274617.891, "dur": 0.02, "name": "_contextvars.copy_context", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274618.291, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274617.791, "dur": 0.7, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274617.491, "dur": 1.1, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:104)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274619.091, "dur": 0.1, "name": "__lt__ (/usr/lib/python3.8/asyncio/events.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274618.791, "dur": 0.5, "name": "_heapq.heappush", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274616.991, "dur": 2.4, "name": "call_at (/usr/lib/python3.8/asyncio/base_events.py:693)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274616.291, "dur": 3.2, "name": "call_later (/usr/lib/python3.8/asyncio/base_events.py:671)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274615.291, "dur": 4.4, "name": "sleep (/usr/lib/python3.8/asyncio/tasks.py:638)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981274614.991, "dur": 4.8, "name": "io_task (/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py:3)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274614.491, "dur": 5.5, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274614.091, "dur": 6.0, "name": "_run (/usr/lib/python3.8/asyncio/events.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274582.791, "dur": 37.5, "name": "_run_once (/usr/lib/python3.8/asyncio/base_events.py:1784)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274620.691, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274621.691, "dur": 0.1, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274621.591, "dur": 1.1, "name": "time (/usr/lib/python3.8/asyncio/base_events.py:662)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274622.891, "dur": 0.4, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274623.491, "dur": 0.3, "name": "builtins.min", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274624.991, "dur": 3.0, "name": "math.ceil", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274628.391, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274628.591, "dur": 0.2, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274628.991, "dur": 10356.1, "name": "select.epoll.poll", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274624.091, "dur": 10365.6, "name": "select (/usr/lib/python3.8/selectors.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981284995.491, "dur": 0.4, "name": "_process_events (/usr/lib/python3.8/asyncio/selector_events.py:586)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981284999.091, "dur": 0.8, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981284997.391, "dur": 2.7, "name": "time (/usr/lib/python3.8/asyncio/base_events.py:662)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285008.391, "dur": 0.5, "name": "__lt__ (/usr/lib/python3.8/asyncio/events.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285005.191, "dur": 3.8, "name": "_heapq.heappop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285010.991, "dur": 0.4, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285011.691, "dur": 0.3, "name": "_heapq.heappop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285012.191, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285012.391, "dur": 0.1, "name": "_heapq.heappop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285012.591, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285013.191, "dur": 0.5, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285017.591, "dur": 0.2, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285031.991, "dur": 0.3, "name": "_asyncio.Future.cancelled", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285037.391, "dur": 0.6, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285041.991, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285040.191, "dur": 2.2, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285042.891, "dur": 0.1, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285038.491, "dur": 4.6, "name": "_call_soon (/usr/lib/python3.8/asyncio/base_events.py:738)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285036.591, "dur": 6.7, "name": "call_soon (/usr/lib/python3.8/asyncio/base_events.py:709)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285032.791, "dur": 11.3, "name": "_asyncio.Future.set_result", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285030.991, "dur": 13.2, "name": "_set_result_unless_cancelled (/usr/lib/python3.8/asyncio/futures.py:284)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285022.991, "dur": 21.4, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285019.691, "dur": 25.0, "name": "_run (/usr/lib/python3.8/asyncio/events.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285044.991, "dur": 0.1, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285046.091, "dur": 0.02, "name": "_asyncio.Future.cancelled", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285046.691, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285047.491, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285047.191, "dur": 0.5, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285047.791, "dur": 0.1, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285046.891, "dur": 1.1, "name": "_call_soon (/usr/lib/python3.8/asyncio/base_events.py:738)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285046.591, "dur": 1.5, "name": "call_soon (/usr/lib/python3.8/asyncio/base_events.py:709)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285046.191, "dur": 2.0, "name": "_asyncio.Future.set_result", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285045.991, "dur": 2.3, "name": "_set_result_unless_cancelled (/usr/lib/python3.8/asyncio/futures.py:284)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285045.891, "dur": 2.5, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285045.291, "dur": 3.2, "name": "_run (/usr/lib/python3.8/asyncio/events.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285048.591, "dur": 0.1, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285049.391, "dur": 0.02, "name": "_asyncio.Future.cancelled", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285049.791, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285050.491, "dur": 1.6, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285050.191, "dur": 2.0, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285052.391, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285049.991, "dur": 2.5, "name": "_call_soon (/usr/lib/python3.8/asyncio/base_events.py:738)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285049.691, "dur": 2.9, "name": "call_soon (/usr/lib/python3.8/asyncio/base_events.py:709)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285049.491, "dur": 3.3, "name": "_asyncio.Future.set_result", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285049.291, "dur": 3.6, "name": "_set_result_unless_cancelled (/usr/lib/python3.8/asyncio/futures.py:284)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285049.191, "dur": 3.8, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285048.791, "dur": 4.3, "name": "_run (/usr/lib/python3.8/asyncio/events.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274620.591, "dur": 10432.6, "name": "_run_once (/usr/lib/python3.8/asyncio/base_events.py:1784)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285054.291, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285056.791, "dur": 0.2, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285057.091, "dur": 1.5, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285058.991, "dur": 2.9, "name": "select.epoll.poll", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285055.991, "dur": 6.0, "name": "select (/usr/lib/python3.8/selectors.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285062.391, "dur": 0.1, "name": "_process_events (/usr/lib/python3.8/asyncio/selector_events.py:586)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285062.791, "dur": 0.1, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285062.591, "dur": 0.4, "name": "time (/usr/lib/python3.8/asyncio/base_events.py:662)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285063.191, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285063.691, "dur": 0.1, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981285074.691, "dur": 0.5, "name": "_timer_handle_cancelled (/usr/lib/python3.8/asyncio/base_events.py:1779)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981285077.591, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981285076.991, "dur": 0.9, "name": "cancel (/usr/lib/python3.8/asyncio/events.py:65)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981285073.891, "dur": 4.2, "name": "cancel (/usr/lib/python3.8/asyncio/events.py:149)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981285071.391, "dur": 6.8, "name": "sleep (/usr/lib/python3.8/asyncio/tasks.py:638)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51296, "ts": 25981285070.891, "dur": 8.9, "name": "io_task (/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py:3)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285080.491, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285081.491, "dur": 0.02, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285081.091, "dur": 0.5, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285081.891, "dur": 0.1, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285080.791, "dur": 1.22, "name": "_call_soon (/usr/lib/python3.8/asyncio/base_events.py:738)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285080.391, "dur": 1.8, "name": "call_soon (/usr/lib/python3.8/asyncio/base_events.py:709)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285064.391, "dur": 18.9, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285063.991, "dur": 19.5, "name": "_run (/usr/lib/python3.8/asyncio/events.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285083.691, "dur": 0.1, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981285086.991, "dur": 0.1, "name": "_timer_handle_cancelled (/usr/lib/python3.8/asyncio/base_events.py:1779)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981285087.591, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981285087.391, "dur": 0.4, "name": "cancel (/usr/lib/python3.8/asyncio/events.py:65)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981285086.691, "dur": 1.2, "name": "cancel (/usr/lib/python3.8/asyncio/events.py:149)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981285086.391, "dur": 1.6, "name": "sleep (/usr/lib/python3.8/asyncio/tasks.py:638)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51456, "ts": 25981285086.091, "dur": 2.3, "name": "io_task (/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py:3)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285084.691, "dur": 4.0, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285084.391, "dur": 4.4, "name": "_run (/usr/lib/python3.8/asyncio/events.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285088.891, "dur": 0.1, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981285090.391, "dur": 0.1, "name": "_timer_handle_cancelled (/usr/lib/python3.8/asyncio/base_events.py:1779)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981285090.891, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981285090.791, "dur": 0.4, "name": "cancel (/usr/lib/python3.8/asyncio/events.py:65)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981285090.291, "dur": 2.1, "name": "cancel (/usr/lib/python3.8/asyncio/events.py:149)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981285090.011, "dur": 2.48, "name": "sleep (/usr/lib/python3.8/asyncio/tasks.py:638)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51616, "ts": 25981285089.991, "dur": 2.8, "name": "io_task (/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py:3)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285089.491, "dur": 3.5, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285089.191, "dur": 3.9, "name": "_run (/usr/lib/python3.8/asyncio/events.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285053.991, "dur": 39.4, "name": "_run_once (/usr/lib/python3.8/asyncio/base_events.py:1784)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285093.891, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285094.591, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285094.711, "dur": 0.28, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285095.191, "dur": 0.5, "name": "select.epoll.poll", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285094.391, "dur": 1.5, "name": "select (/usr/lib/python3.8/selectors.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285096.091, "dur": 0.1, "name": "_process_events (/usr/lib/python3.8/asyncio/selector_events.py:586)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285096.391, "dur": 0.1, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285096.291, "dur": 0.3, "name": "time (/usr/lib/python3.8/asyncio/base_events.py:662)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285096.791, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285097.491, "dur": 0.1, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51136, "ts": 25981285098.991, "dur": 0.9, "name": "main (/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py:6)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285100.591, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285101.391, "dur": 0.02, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285101.091, "dur": 0.4, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285101.691, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285100.891, "dur": 0.9, "name": "_call_soon (/usr/lib/python3.8/asyncio/base_events.py:738)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285100.491, "dur": 1.4, "name": "call_soon (/usr/lib/python3.8/asyncio/base_events.py:709)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285097.991, "dur": 4.2, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285097.791, "dur": 4.5, "name": "_run (/usr/lib/python3.8/asyncio/events.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285093.691, "dur": 8.8, "name": "_run_once (/usr/lib/python3.8/asyncio/base_events.py:1784)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285102.791, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285103.291, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285103.411, "dur": 0.18, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285103.691, "dur": 0.3, "name": "select.epoll.poll", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285103.191, "dur": 0.9, "name": "select (/usr/lib/python3.8/selectors.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285104.291, "dur": 0.02, "name": "_process_events (/usr/lib/python3.8/asyncio/selector_events.py:586)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285104.591, "dur": 0.1, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285104.491, "dur": 0.22, "name": "time (/usr/lib/python3.8/asyncio/base_events.py:662)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285104.891, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285105.291, "dur": 0.02, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285106.991, "dur": 0.1, "name": "_asyncio.Task.cancelled", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285107.791, "dur": 0.1, "name": "_asyncio.Task.exception", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285108.291, "dur": 0.9, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285110.991, "dur": 0.1, "name": "_asyncio.Task.get_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285110.391, "dur": 0.72, "name": "_get_loop (/usr/lib/python3.8/asyncio/futures.py:272)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285111.791, "dur": 0.3, "name": "stop (/usr/lib/python3.8/asyncio/base_events.py:618)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285106.591, "dur": 5.6, "name": "_run_until_complete_cb (/usr/lib/python3.8/asyncio/base_events.py:184)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285105.791, "dur": 6.6, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285105.491, "dur": 6.92, "name": "_run (/usr/lib/python3.8/asyncio/events.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285102.691, "dur": 10.0, "name": "_run_once (/usr/lib/python3.8/asyncio/base_events.py:1784)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285113.391, "dur": 1.4, "name": "_asyncio._set_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285116.091, "dur": 1.0, "name": "_set_coroutine_origin_tracking (/usr/lib/python3.8/asyncio/base_events.py:1862)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285118.091, "dur": 1.6, "name": "sys.set_asyncgen_hooks", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274540.791, "dur": 10579.0, "name": "run_forever (/usr/lib/python3.8/asyncio/base_events.py:557)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285121.291, "dur": 0.1, "name": "_asyncio.Task.remove_done_callback", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285122.191, "dur": 0.02, "name": "_asyncio.Task.done", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285122.591, "dur": 0.1, "name": "_asyncio.Task.result", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274520.091, "dur": 10602.62, "name": "run_until_complete (/usr/lib/python3.8/asyncio/base_events.py:580)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285127.691, "dur": 0.3, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285128.291, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285126.991, "dur": 1.4, "name": "__len__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:67)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285129.711, "dur": 0.08, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285129.891, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285129.691, "dur": 0.3, "name": "__len__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:67)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285131.491, "dur": 1.0, "name": "__init__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:16)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285134.291, "dur": 0.2, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285133.491, "dur": 1.1, "name": "__enter__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:20)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285130.091, "dur": 5.1, "name": "__iter__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285135.391, "dur": 0.2, "name": "__iter__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285135.691, "dur": 0.1, "name": "__iter__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285135.811, "dur": 0.18, "name": "__iter__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285137.291, "dur": 0.6, "name": "set.remove", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285138.191, "dur": 0.5, "name": "_commit_removals (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:52)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285136.391, "dur": 2.4, "name": "__exit__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:26)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285136.011, "dur": 3.08, "name": "__iter__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285141.191, "dur": 0.1, "name": "_asyncio.Task.get_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285140.991, "dur": 0.32, "name": "_get_loop (/usr/lib/python3.8/asyncio/futures.py:272)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285141.491, "dur": 0.1, "name": "_asyncio.Task.done", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285141.891, "dur": 0.02, "name": "_asyncio.Task.get_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285141.791, "dur": 0.2, "name": "_get_loop (/usr/lib/python3.8/asyncio/futures.py:272)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285142.091, "dur": 0.02, "name": "_asyncio.Task.done", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285142.311, "dur": 0.08, "name": "_asyncio.Task.get_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285142.291, "dur": 0.12, "name": "_get_loop (/usr/lib/python3.8/asyncio/futures.py:272)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285142.591, "dur": 0.02, "name": "_asyncio.Task.done", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285142.791, "dur": 0.02, "name": "_asyncio.Task.get_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285142.691, "dur": 0.1, "name": "_get_loop (/usr/lib/python3.8/asyncio/futures.py:272)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285142.891, "dur": 0.1, "name": "_asyncio.Task.done", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285140.291, "dur": 2.72, "name": "<setcomp> (/usr/lib/python3.8/asyncio/tasks.py:60)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285124.791, "dur": 18.5, "name": "all_tasks (/usr/lib/python3.8/asyncio/tasks.py:41)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285123.791, "dur": 19.9, "name": "_cancel_all_tasks (/usr/lib/python3.8/asyncio/runners.py:54)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285146.491, "dur": 0.2, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285147.191, "dur": 0.3, "name": "is_running (/usr/lib/python3.8/asyncio/base_events.py:658)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285147.791, "dur": 0.1, "name": "_asyncio._get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285146.891, "dur": 1.1, "name": "_check_running (/usr/lib/python3.8/asyncio/base_events.py:550)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285149.291, "dur": 1.8, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285148.691, "dur": 2.5, "name": "isfuture (/usr/lib/python3.8/asyncio/base_futures.py:13)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285152.891, "dur": 1.5, "name": "iscoroutine (/usr/lib/python3.8/asyncio/coroutines.py:177)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285156.291, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285158.291, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285163.091, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285164.191, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285163.691, "dur": 0.7, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285164.691, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285163.391, "dur": 1.4, "name": "_call_soon (/usr/lib/python3.8/asyncio/base_events.py:738)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285162.991, "dur": 1.9, "name": "call_soon (/usr/lib/python3.8/asyncio/base_events.py:709)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285166.391, "dur": 0.4, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285165.591, "dur": 1.3, "name": "add (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:81)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285156.091, "dur": 11.5, "name": "create_task (/usr/lib/python3.8/asyncio/base_events.py:424)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285151.991, "dur": 15.7, "name": "ensure_future (/usr/lib/python3.8/asyncio/tasks.py:661)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285168.191, "dur": 0.1, "name": "_asyncio.Task.add_done_callback", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285169.091, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285169.391, "dur": 0.1, "name": "is_running (/usr/lib/python3.8/asyncio/base_events.py:658)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285169.691, "dur": 0.02, "name": "_asyncio._get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285169.291, "dur": 0.5, "name": "_check_running (/usr/lib/python3.8/asyncio/base_events.py:550)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285169.991, "dur": 0.6, "name": "_set_coroutine_origin_tracking (/usr/lib/python3.8/asyncio/base_events.py:1862)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285171.191, "dur": 0.9, "name": "_thread.get_ident", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285172.391, "dur": 0.5, "name": "sys.get_asyncgen_hooks", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285173.391, "dur": 1.2, "name": "sys.set_asyncgen_hooks", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285174.791, "dur": 0.3, "name": "_asyncio._set_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285175.391, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285176.191, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285176.291, "dur": 0.3, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285176.691, "dur": 0.6, "name": "select.epoll.poll", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285175.891, "dur": 1.6, "name": "select (/usr/lib/python3.8/selectors.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285177.691, "dur": 0.1, "name": "_process_events (/usr/lib/python3.8/asyncio/selector_events.py:586)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285178.091, "dur": 0.1, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285177.991, "dur": 0.22, "name": "time (/usr/lib/python3.8/asyncio/base_events.py:662)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285178.491, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285179.091, "dur": 0.1, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51776, "ts": 25981285182.291, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51776, "ts": 25981285182.491, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51776, "ts": 25981285181.891, "dur": 0.8, "name": "__len__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:67)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51776, "ts": 25981285181.591, "dur": 1.2, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 51776, "ts": 25981285181.091, "dur": 1.9, "name": "shutdown_asyncgens (/usr/lib/python3.8/asyncio/base_events.py:524)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285183.591, "dur": 0.1, "name": "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285184.291, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285183.991, "dur": 0.5, "name": "__init__ (/usr/lib/python3.8/asyncio/events.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285184.691, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285183.791, "dur": 1.0, "name": "_call_soon (/usr/lib/python3.8/asyncio/base_events.py:738)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285183.491, "dur": 1.4, "name": "call_soon (/usr/lib/python3.8/asyncio/base_events.py:709)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285179.991, "dur": 5.3, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285179.591, "dur": 5.8, "name": "_run (/usr/lib/python3.8/asyncio/events.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285175.291, "dur": 10.4, "name": "_run_once (/usr/lib/python3.8/asyncio/base_events.py:1784)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285186.091, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285187.691, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285187.691, "dur": 0.2, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285187.991, "dur": 0.3, "name": "select.epoll.poll", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285187.491, "dur": 1.0, "name": "select (/usr/lib/python3.8/selectors.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285188.591, "dur": 0.1, "name": "_process_events (/usr/lib/python3.8/asyncio/selector_events.py:586)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285188.891, "dur": 0.1, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285188.791, "dur": 0.22, "name": "time (/usr/lib/python3.8/asyncio/base_events.py:662)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285189.191, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285189.591, "dur": 0.02, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285190.291, "dur": 0.02, "name": "_asyncio.Task.cancelled", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285190.591, "dur": 0.02, "name": "_asyncio.Task.exception", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285190.891, "dur": 0.2, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285191.391, "dur": 0.1, "name": "_asyncio.Task.get_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285191.291, "dur": 0.22, "name": "_get_loop (/usr/lib/python3.8/asyncio/futures.py:272)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285191.791, "dur": 0.1, "name": "stop (/usr/lib/python3.8/asyncio/base_events.py:618)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285190.191, "dur": 1.72, "name": "_run_until_complete_cb (/usr/lib/python3.8/asyncio/base_events.py:184)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285190.091, "dur": 1.9, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285189.791, "dur": 2.3, "name": "_run (/usr/lib/python3.8/asyncio/events.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285185.991, "dur": 6.3, "name": "_run_once (/usr/lib/python3.8/asyncio/base_events.py:1784)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285192.691, "dur": 0.3, "name": "_asyncio._set_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285193.091, "dur": 0.4, "name": "_set_coroutine_origin_tracking (/usr/lib/python3.8/asyncio/base_events.py:1862)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285193.891, "dur": 0.3, "name": "sys.set_asyncgen_hooks", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285168.991, "dur": 25.22, "name": "run_forever (/usr/lib/python3.8/asyncio/base_events.py:557)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285194.591, "dur": 0.1, "name": "_asyncio.Task.remove_done_callback", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285194.791, "dur": 0.1, "name": "_asyncio.Task.done", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285194.991, "dur": 0.02, "name": "_asyncio.Task.result", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285146.391, "dur": 48.7, "name": "run_until_complete (/usr/lib/python3.8/asyncio/base_events.py:580)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285195.991, "dur": 0.2, "name": "get_event_loop_policy (/usr/lib/python3.8/asyncio/events.py:719)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285198.791, "dur": 1.8, "name": "set_event_loop (/usr/lib/python3.8/asyncio/events.py:644)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285197.491, "dur": 3.3, "name": "set_event_loop (/usr/lib/python3.8/asyncio/unix_events.py:1353)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285195.591, "dur": 5.3, "name": "set_event_loop (/usr/lib/python3.8/asyncio/events.py:751)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285202.791, "dur": 0.1, "name": "is_running (/usr/lib/python3.8/asyncio/base_events.py:658)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285203.391, "dur": 0.1, "name": "is_closed (/usr/lib/python3.8/asyncio/base_events.py:648)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285204.891, "dur": 0.2, "name": "socket.fileno", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285205.691, "dur": 0.02, "name": "is_closed (/usr/lib/python3.8/asyncio/base_events.py:648)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285206.691, "dur": 0.2, "name": "get_map (/usr/lib/python3.8/selectors.py:272)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285209.391, "dur": 0.1, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285209.091, "dur": 0.7, "name": "_fileobj_to_fd (/usr/lib/python3.8/selectors.py:21)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285208.591, "dur": 1.22, "name": "_fileobj_lookup (/usr/lib/python3.8/selectors.py:215)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285207.591, "dur": 2.8, "name": "__getitem__ (/usr/lib/python3.8/selectors.py:69)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285206.191, "dur": 4.22, "name": "get_key (/usr/lib/python3.8/selectors.py:180)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285214.491, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285214.311, "dur": 0.28, "name": "_fileobj_to_fd (/usr/lib/python3.8/selectors.py:21)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285214.291, "dur": 0.4, "name": "_fileobj_lookup (/usr/lib/python3.8/selectors.py:215)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285214.891, "dur": 0.2, "name": "dict.pop", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285213.991, "dur": 1.2, "name": "unregister (/usr/lib/python3.8/selectors.py:247)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285217.091, "dur": 2.4, "name": "select.epoll.unregister", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285213.291, "dur": 6.3, "name": "unregister (/usr/lib/python3.8/selectors.py:365)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285220.391, "dur": 0.1, "name": "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285219.891, "dur": 0.9, "name": "cancel (/usr/lib/python3.8/asyncio/events.py:65)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285205.391, "dur": 15.5, "name": "_remove_reader (/usr/lib/python3.8/asyncio/selector_events.py:272)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285224.491, "dur": 7.8, "name": "socket.close", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285223.291, "dur": 9.2, "name": "_real_close (/usr/lib/python3.8/socket.py:492)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285222.391, "dur": 10.2, "name": "close (/usr/lib/python3.8/socket.py:496)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285233.691, "dur": 2.2, "name": "socket.close", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285233.591, "dur": 2.32, "name": "_real_close (/usr/lib/python3.8/socket.py:492)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285233.291, "dur": 2.7, "name": "close (/usr/lib/python3.8/socket.py:496)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285203.991, "dur": 32.5, "name": "_close_self_pipe (/usr/lib/python3.8/asyncio/selector_events.py:98)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285237.391, "dur": 0.1, "name": "is_running (/usr/lib/python3.8/asyncio/base_events.py:658)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285238.091, "dur": 0.4, "name": "collections.deque.clear", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285239.391, "dur": 0.2, "name": "list.clear", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285237.091, "dur": 2.7, "name": "close (/usr/lib/python3.8/asyncio/base_events.py:626)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285241.691, "dur": 1.8, "name": "select.epoll.close", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285244.391, "dur": 0.2, "name": "dict.clear", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285244.191, "dur": 1.1, "name": "close (/usr/lib/python3.8/selectors.py:268)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285241.391, "dur": 4.0, "name": "close (/usr/lib/python3.8/selectors.py:483)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285202.491, "dur": 43.3, "name": "close (/usr/lib/python3.8/asyncio/selector_events.py:87)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285246.391, "dur": 0.2, "name": "sys.is_finalizing", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981285201.691, "dur": 46.7, "name": "close (/usr/lib/python3.8/asyncio/unix_events.py:57)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274392.791, "dur": 10855.7, "name": "run (/usr/lib/python3.8/asyncio/runners.py:8)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274387.291, "dur": 10861.7, "name": "<module> (/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py:1)", "ph": "X", "cat": "FEE"}, {"pid": 32511, "tid": 32511, "ts": 25981274386.691, "dur": 10862.9, "name": "builtins.exec", "ph": "X", "cat": "FEE"}, {"ph": "M", "pid": 32511, "tid": 51136, "name": "thread_name", "args": {"name": "Task-1"}}, {"ph": "M", "pid": 32511, "tid": 51296, "name": "thread_name", "args": {"name": "Task-2"}}, {"ph": "M", "pid": 32511, "tid": 51456, "name": "thread_name", "args": {"name": "Task-3"}}, {"ph": "M", "pid": 32511, "tid": 51616, "name": "thread_name", "args": {"name": "Task-4"}}, {"ph": "M", "pid": 32511, "tid": 51776, "name": "thread_name", "args": {"name": "Task-5"}}], "viztracer_metadata": {"version": "0.12.3"}, "displayTimeUnit": "ms", "file_info": {"files": {"/usr/lib/python3.8/asyncio/coroutines.py": ["__all__ = 'coroutine', 'iscoroutinefunction', 'iscoroutine'\n\nimport collections.abc\nimport functools\nimport inspect\nimport os\nimport sys\nimport traceback\nimport types\nimport warnings\n\nfrom . import base_futures\nfrom . import constants\nfrom . import format_helpers\nfrom .log import logger\n\n\ndef _is_debug_mode():\n    # If you set _DEBUG to true, @coroutine will wrap the resulting\n    # generator objects in a CoroWrapper instance (defined below).  That\n    # instance will log a message when the generator is never iterated\n    # over, which may happen when you forget to use \"await\" or \"yield from\"\n    # with a coroutine call.\n    # Note that the value of the _DEBUG flag is taken\n    # when the decorator is used, so to be of any use it must be set\n    # before you define your coroutines.  A downside of using this feature\n    # is that tracebacks show entries for the CoroWrapper.__next__ method\n    # when _DEBUG is true.\n    return sys.flags.dev_mode or (not sys.flags.ignore_environment and\n                                  bool(os.environ.get('PYTHONASYNCIODEBUG')))\n\n\n_DEBUG = _is_debug_mode()\n\n\nclass CoroWrapper:\n    # Wrapper for coroutine object in _DEBUG mode.\n\n    def __init__(self, gen, func=None):\n        assert inspect.isgenerator(gen) or inspect.iscoroutine(gen), gen\n        self.gen = gen\n        self.func = func  # Used to unwrap @coroutine decorator\n        self._source_traceback = format_helpers.extract_stack(sys._getframe(1))\n        self.__name__ = getattr(gen, '__name__', None)\n        self.__qualname__ = getattr(gen, '__qualname__', None)\n\n    def __repr__(self):\n        coro_repr = _format_coroutine(self)\n        if self._source_traceback:\n            frame = self._source_traceback[-1]\n            coro_repr += f', created at {frame[0]}:{frame[1]}'\n\n        return f'<{self.__class__.__name__} {coro_repr}>'\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self.gen.send(None)\n\n    def send(self, value):\n        return self.gen.send(value)\n\n    def throw(self, type, value=None, traceback=None):\n        return self.gen.throw(type, value, traceback)\n\n    def close(self):\n        return self.gen.close()\n\n    @property\n    def gi_frame(self):\n        return self.gen.gi_frame\n\n    @property\n    def gi_running(self):\n        return self.gen.gi_running\n\n    @property\n    def gi_code(self):\n        return self.gen.gi_code\n\n    def __await__(self):\n        return self\n\n    @property\n    def gi_yieldfrom(self):\n        return self.gen.gi_yieldfrom\n\n    def __del__(self):\n        # Be careful accessing self.gen.frame -- self.gen might not exist.\n        gen = getattr(self, 'gen', None)\n        frame = getattr(gen, 'gi_frame', None)\n        if frame is not None and frame.f_lasti == -1:\n            msg = f'{self!r} was never yielded from'\n            tb = getattr(self, '_source_traceback', ())\n            if tb:\n                tb = ''.join(traceback.format_list(tb))\n                msg += (f'\\nCoroutine object created at '\n                        f'(most recent call last, truncated to '\n                        f'{constants.DEBUG_STACK_DEPTH} last lines):\\n')\n                msg += tb.rstrip()\n            logger.error(msg)\n\n\ndef coroutine(func):\n    \"\"\"Decorator to mark coroutines.\n\n    If the coroutine is not yielded from before it is destroyed,\n    an error message is logged.\n    \"\"\"\n    warnings.warn('\"@coroutine\" decorator is deprecated since Python 3.8, use \"async def\" instead',\n                  DeprecationWarning,\n                  stacklevel=2)\n    if inspect.iscoroutinefunction(func):\n        # In Python 3.5 that's all we need to do for coroutines\n        # defined with \"async def\".\n        return func\n\n    if inspect.isgeneratorfunction(func):\n        coro = func\n    else:\n        @functools.wraps(func)\n        def coro(*args, **kw):\n            res = func(*args, **kw)\n            if (base_futures.isfuture(res) or inspect.isgenerator(res) or\n                    isinstance(res, CoroWrapper)):\n                res = yield from res\n            else:\n                # If 'res' is an awaitable, run it.\n                try:\n                    await_meth = res.__await__\n                except AttributeError:\n                    pass\n                else:\n                    if isinstance(res, collections.abc.Awaitable):\n                        res = yield from await_meth()\n            return res\n\n    coro = types.coroutine(coro)\n    if not _DEBUG:\n        wrapper = coro\n    else:\n        @functools.wraps(func)\n        def wrapper(*args, **kwds):\n            w = CoroWrapper(coro(*args, **kwds), func=func)\n            if w._source_traceback:\n                del w._source_traceback[-1]\n            # Python < 3.5 does not implement __qualname__\n            # on generator objects, so we set it manually.\n            # We use getattr as some callables (such as\n            # functools.partial may lack __qualname__).\n            w.__name__ = getattr(func, '__name__', None)\n            w.__qualname__ = getattr(func, '__qualname__', None)\n            return w\n\n    wrapper._is_coroutine = _is_coroutine  # For iscoroutinefunction().\n    return wrapper\n\n\n# A marker for iscoroutinefunction.\n_is_coroutine = object()\n\n\ndef iscoroutinefunction(func):\n    \"\"\"Return True if func is a decorated coroutine function.\"\"\"\n    return (inspect.iscoroutinefunction(func) or\n            getattr(func, '_is_coroutine', None) is _is_coroutine)\n\n\n# Prioritize native coroutine check to speed-up\n# asyncio.iscoroutine.\n_COROUTINE_TYPES = (types.CoroutineType, types.GeneratorType,\n                    collections.abc.Coroutine, CoroWrapper)\n_iscoroutine_typecache = set()\n\n\ndef iscoroutine(obj):\n    \"\"\"Return True if obj is a coroutine object.\"\"\"\n    if type(obj) in _iscoroutine_typecache:\n        return True\n\n    if isinstance(obj, _COROUTINE_TYPES):\n        # Just in case we don't want to cache more than 100\n        # positive types.  That shouldn't ever happen, unless\n        # someone stressing the system on purpose.\n        if len(_iscoroutine_typecache) < 100:\n            _iscoroutine_typecache.add(type(obj))\n        return True\n    else:\n        return False\n\n\ndef _format_coroutine(coro):\n    assert iscoroutine(coro)\n\n    is_corowrapper = isinstance(coro, CoroWrapper)\n\n    def get_name(coro):\n        # Coroutines compiled with Cython sometimes don't have\n        # proper __qualname__ or __name__.  While that is a bug\n        # in Cython, asyncio shouldn't crash with an AttributeError\n        # in its __repr__ functions.\n        if is_corowrapper:\n            return format_helpers._format_callback(coro.func, (), {})\n\n        if hasattr(coro, '__qualname__') and coro.__qualname__:\n            coro_name = coro.__qualname__\n        elif hasattr(coro, '__name__') and coro.__name__:\n            coro_name = coro.__name__\n        else:\n            # Stop masking Cython bugs, expose them in a friendly way.\n            coro_name = f'<{type(coro).__name__} without __name__>'\n        return f'{coro_name}()'\n\n    def is_running(coro):\n        try:\n            return coro.cr_running\n        except AttributeError:\n            try:\n                return coro.gi_running\n            except AttributeError:\n                return False\n\n    coro_code = None\n    if hasattr(coro, 'cr_code') and coro.cr_code:\n        coro_code = coro.cr_code\n    elif hasattr(coro, 'gi_code') and coro.gi_code:\n        coro_code = coro.gi_code\n\n    coro_name = get_name(coro)\n\n    if not coro_code:\n        # Built-in types might not have __qualname__ or __name__.\n        if is_running(coro):\n            return f'{coro_name} running'\n        else:\n            return coro_name\n\n    coro_frame = None\n    if hasattr(coro, 'gi_frame') and coro.gi_frame:\n        coro_frame = coro.gi_frame\n    elif hasattr(coro, 'cr_frame') and coro.cr_frame:\n        coro_frame = coro.cr_frame\n\n    # If Cython's coroutine has a fake code object without proper\n    # co_filename -- expose that.\n    filename = coro_code.co_filename or '<empty co_filename>'\n\n    lineno = 0\n    if (is_corowrapper and\n            coro.func is not None and\n            not inspect.isgeneratorfunction(coro.func)):\n        source = format_helpers._get_function_source(coro.func)\n        if source is not None:\n            filename, lineno = source\n        if coro_frame is None:\n            coro_repr = f'{coro_name} done, defined at {filename}:{lineno}'\n        else:\n            coro_repr = f'{coro_name} running, defined at {filename}:{lineno}'\n\n    elif coro_frame is not None:\n        lineno = coro_frame.f_lineno\n        coro_repr = f'{coro_name} running at {filename}:{lineno}'\n\n    else:\n        lineno = coro_code.co_firstlineno\n        coro_repr = f'{coro_name} done, defined at {filename}:{lineno}'\n\n    return coro_repr\n", 269], "/usr/lib/python3.8/asyncio/events.py": ["\"\"\"Event loop and event loop policy.\"\"\"\n\n__all__ = (\n    'AbstractEventLoopPolicy',\n    'AbstractEventLoop', 'AbstractServer',\n    'Handle', 'TimerHandle',\n    'get_event_loop_policy', 'set_event_loop_policy',\n    'get_event_loop', 'set_event_loop', 'new_event_loop',\n    'get_child_watcher', 'set_child_watcher',\n    '_set_running_loop', 'get_running_loop',\n    '_get_running_loop',\n)\n\nimport contextvars\nimport os\nimport socket\nimport subprocess\nimport sys\nimport threading\n\nfrom . import format_helpers\nfrom . import exceptions\n\n\nclass Handle:\n    \"\"\"Object returned by callback registration methods.\"\"\"\n\n    __slots__ = ('_callback', '_args', '_cancelled', '_loop',\n                 '_source_traceback', '_repr', '__weakref__',\n                 '_context')\n\n    def __init__(self, callback, args, loop, context=None):\n        if context is None:\n            context = contextvars.copy_context()\n        self._context = context\n        self._loop = loop\n        self._callback = callback\n        self._args = args\n        self._cancelled = False\n        self._repr = None\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n        else:\n            self._source_traceback = None\n\n    def _repr_info(self):\n        info = [self.__class__.__name__]\n        if self._cancelled:\n            info.append('cancelled')\n        if self._callback is not None:\n            info.append(format_helpers._format_callback_source(\n                self._callback, self._args))\n        if self._source_traceback:\n            frame = self._source_traceback[-1]\n            info.append(f'created at {frame[0]}:{frame[1]}')\n        return info\n\n    def __repr__(self):\n        if self._repr is not None:\n            return self._repr\n        info = self._repr_info()\n        return '<{}>'.format(' '.join(info))\n\n    def cancel(self):\n        if not self._cancelled:\n            self._cancelled = True\n            if self._loop.get_debug():\n                # Keep a representation in debug mode to keep callback and\n                # parameters. For example, to log the warning\n                # \"Executing <Handle...> took 2.5 second\"\n                self._repr = repr(self)\n            self._callback = None\n            self._args = None\n\n    def cancelled(self):\n        return self._cancelled\n\n    def _run(self):\n        try:\n            self._context.run(self._callback, *self._args)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            cb = format_helpers._format_callback_source(\n                self._callback, self._args)\n            msg = f'Exception in callback {cb}'\n            context = {\n                'message': msg,\n                'exception': exc,\n                'handle': self,\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        self = None  # Needed to break cycles when an exception occurs.\n\n\nclass TimerHandle(Handle):\n    \"\"\"Object returned by timed callback registration methods.\"\"\"\n\n    __slots__ = ['_scheduled', '_when']\n\n    def __init__(self, when, callback, args, loop, context=None):\n        assert when is not None\n        super().__init__(callback, args, loop, context)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        self._when = when\n        self._scheduled = False\n\n    def _repr_info(self):\n        info = super()._repr_info()\n        pos = 2 if self._cancelled else 1\n        info.insert(pos, f'when={self._when}')\n        return info\n\n    def __hash__(self):\n        return hash(self._when)\n\n    def __lt__(self, other):\n        return self._when < other._when\n\n    def __le__(self, other):\n        if self._when < other._when:\n            return True\n        return self.__eq__(other)\n\n    def __gt__(self, other):\n        return self._when > other._when\n\n    def __ge__(self, other):\n        if self._when > other._when:\n            return True\n        return self.__eq__(other)\n\n    def __eq__(self, other):\n        if isinstance(other, TimerHandle):\n            return (self._when == other._when and\n                    self._callback == other._callback and\n                    self._args == other._args and\n                    self._cancelled == other._cancelled)\n        return NotImplemented\n\n    def __ne__(self, other):\n        equal = self.__eq__(other)\n        return NotImplemented if equal is NotImplemented else not equal\n\n    def cancel(self):\n        if not self._cancelled:\n            self._loop._timer_handle_cancelled(self)\n        super().cancel()\n\n    def when(self):\n        \"\"\"Return a scheduled callback time.\n\n        The time is an absolute timestamp, using the same time\n        reference as loop.time().\n        \"\"\"\n        return self._when\n\n\nclass AbstractServer:\n    \"\"\"Abstract server returned by create_server().\"\"\"\n\n    def close(self):\n        \"\"\"Stop serving.  This leaves existing connections open.\"\"\"\n        raise NotImplementedError\n\n    def get_loop(self):\n        \"\"\"Get the event loop the Server object is attached to.\"\"\"\n        raise NotImplementedError\n\n    def is_serving(self):\n        \"\"\"Return True if the server is accepting connections.\"\"\"\n        raise NotImplementedError\n\n    async def start_serving(self):\n        \"\"\"Start accepting connections.\n\n        This method is idempotent, so it can be called when\n        the server is already being serving.\n        \"\"\"\n        raise NotImplementedError\n\n    async def serve_forever(self):\n        \"\"\"Start accepting connections until the coroutine is cancelled.\n\n        The server is closed when the coroutine is cancelled.\n        \"\"\"\n        raise NotImplementedError\n\n    async def wait_closed(self):\n        \"\"\"Coroutine to wait until service is closed.\"\"\"\n        raise NotImplementedError\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc):\n        self.close()\n        await self.wait_closed()\n\n\nclass AbstractEventLoop:\n    \"\"\"Abstract event loop.\"\"\"\n\n    # Running and stopping the event loop.\n\n    def run_forever(self):\n        \"\"\"Run the event loop until stop() is called.\"\"\"\n        raise NotImplementedError\n\n    def run_until_complete(self, future):\n        \"\"\"Run the event loop until a Future is done.\n\n        Return the Future's result, or raise its exception.\n        \"\"\"\n        raise NotImplementedError\n\n    def stop(self):\n        \"\"\"Stop the event loop as soon as reasonable.\n\n        Exactly how soon that is may depend on the implementation, but\n        no more I/O callbacks should be scheduled.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_running(self):\n        \"\"\"Return whether the event loop is currently running.\"\"\"\n        raise NotImplementedError\n\n    def is_closed(self):\n        \"\"\"Returns True if the event loop was closed.\"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Close the loop.\n\n        The loop should not be running.\n\n        This is idempotent and irreversible.\n\n        No other methods should be called after this one.\n        \"\"\"\n        raise NotImplementedError\n\n    async def shutdown_asyncgens(self):\n        \"\"\"Shutdown all active asynchronous generators.\"\"\"\n        raise NotImplementedError\n\n    # Methods scheduling callbacks.  All these return Handles.\n\n    def _timer_handle_cancelled(self, handle):\n        \"\"\"Notification that a TimerHandle has been cancelled.\"\"\"\n        raise NotImplementedError\n\n    def call_soon(self, callback, *args):\n        return self.call_later(0, callback, *args)\n\n    def call_later(self, delay, callback, *args):\n        raise NotImplementedError\n\n    def call_at(self, when, callback, *args):\n        raise NotImplementedError\n\n    def time(self):\n        raise NotImplementedError\n\n    def create_future(self):\n        raise NotImplementedError\n\n    # Method scheduling a coroutine object: create a task.\n\n    def create_task(self, coro, *, name=None):\n        raise NotImplementedError\n\n    # Methods for interacting with threads.\n\n    def call_soon_threadsafe(self, callback, *args):\n        raise NotImplementedError\n\n    def run_in_executor(self, executor, func, *args):\n        raise NotImplementedError\n\n    def set_default_executor(self, executor):\n        raise NotImplementedError\n\n    # Network I/O methods returning Futures.\n\n    async def getaddrinfo(self, host, port, *,\n                          family=0, type=0, proto=0, flags=0):\n        raise NotImplementedError\n\n    async def getnameinfo(self, sockaddr, flags=0):\n        raise NotImplementedError\n\n    async def create_connection(\n            self, protocol_factory, host=None, port=None,\n            *, ssl=None, family=0, proto=0,\n            flags=0, sock=None, local_addr=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None,\n            happy_eyeballs_delay=None, interleave=None):\n        raise NotImplementedError\n\n    async def create_server(\n            self, protocol_factory, host=None, port=None,\n            *, family=socket.AF_UNSPEC,\n            flags=socket.AI_PASSIVE, sock=None, backlog=100,\n            ssl=None, reuse_address=None, reuse_port=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        \"\"\"A coroutine which creates a TCP server bound to host and port.\n\n        The return value is a Server object which can be used to stop\n        the service.\n\n        If host is an empty string or None all interfaces are assumed\n        and a list of multiple sockets will be returned (most likely\n        one for IPv4 and another one for IPv6). The host parameter can also be\n        a sequence (e.g. list) of hosts to bind to.\n\n        family can be set to either AF_INET or AF_INET6 to force the\n        socket to use IPv4 or IPv6. If not set it will be determined\n        from host (defaults to AF_UNSPEC).\n\n        flags is a bitmask for getaddrinfo().\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n\n        backlog is the maximum number of queued connections passed to\n        listen() (defaults to 100).\n\n        ssl can be set to an SSLContext to enable SSL over the\n        accepted connections.\n\n        reuse_address tells the kernel to reuse a local socket in\n        TIME_WAIT state, without waiting for its natural timeout to\n        expire. If not specified will automatically be set to True on\n        UNIX.\n\n        reuse_port tells the kernel to allow this endpoint to be bound to\n        the same port as other existing endpoints are bound to, so long as\n        they all set this flag when being created. This option is not\n        supported on Windows.\n\n        ssl_handshake_timeout is the time in seconds that an SSL server\n        will wait for completion of the SSL handshake before aborting the\n        connection. Default is 60s.\n\n        start_serving set to True (default) causes the created server\n        to start accepting connections immediately.  When set to False,\n        the user should await Server.start_serving() or Server.serve_forever()\n        to make the server to start accepting connections.\n        \"\"\"\n        raise NotImplementedError\n\n    async def sendfile(self, transport, file, offset=0, count=None,\n                       *, fallback=True):\n        \"\"\"Send a file through a transport.\n\n        Return an amount of sent bytes.\n        \"\"\"\n        raise NotImplementedError\n\n    async def start_tls(self, transport, protocol, sslcontext, *,\n                        server_side=False,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None):\n        \"\"\"Upgrade a transport to TLS.\n\n        Return a new transport that *protocol* should start using\n        immediately.\n        \"\"\"\n        raise NotImplementedError\n\n    async def create_unix_connection(\n            self, protocol_factory, path=None, *,\n            ssl=None, sock=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None):\n        raise NotImplementedError\n\n    async def create_unix_server(\n            self, protocol_factory, path=None, *,\n            sock=None, backlog=100, ssl=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        \"\"\"A coroutine which creates a UNIX Domain Socket server.\n\n        The return value is a Server object, which can be used to stop\n        the service.\n\n        path is a str, representing a file systsem path to bind the\n        server socket to.\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n\n        backlog is the maximum number of queued connections passed to\n        listen() (defaults to 100).\n\n        ssl can be set to an SSLContext to enable SSL over the\n        accepted connections.\n\n        ssl_handshake_timeout is the time in seconds that an SSL server\n        will wait for the SSL handshake to complete (defaults to 60s).\n\n        start_serving set to True (default) causes the created server\n        to start accepting connections immediately.  When set to False,\n        the user should await Server.start_serving() or Server.serve_forever()\n        to make the server to start accepting connections.\n        \"\"\"\n        raise NotImplementedError\n\n    async def create_datagram_endpoint(self, protocol_factory,\n                                       local_addr=None, remote_addr=None, *,\n                                       family=0, proto=0, flags=0,\n                                       reuse_address=None, reuse_port=None,\n                                       allow_broadcast=None, sock=None):\n        \"\"\"A coroutine which creates a datagram endpoint.\n\n        This method will try to establish the endpoint in the background.\n        When successful, the coroutine returns a (transport, protocol) pair.\n\n        protocol_factory must be a callable returning a protocol instance.\n\n        socket family AF_INET, socket.AF_INET6 or socket.AF_UNIX depending on\n        host (or family if specified), socket type SOCK_DGRAM.\n\n        reuse_address tells the kernel to reuse a local socket in\n        TIME_WAIT state, without waiting for its natural timeout to\n        expire. If not specified it will automatically be set to True on\n        UNIX.\n\n        reuse_port tells the kernel to allow this endpoint to be bound to\n        the same port as other existing endpoints are bound to, so long as\n        they all set this flag when being created. This option is not\n        supported on Windows and some UNIX's. If the\n        :py:data:`~socket.SO_REUSEPORT` constant is not defined then this\n        capability is unsupported.\n\n        allow_broadcast tells the kernel to allow this endpoint to send\n        messages to the broadcast address.\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n        \"\"\"\n        raise NotImplementedError\n\n    # Pipes and subprocesses.\n\n    async def connect_read_pipe(self, protocol_factory, pipe):\n        \"\"\"Register read pipe in event loop. Set the pipe to non-blocking mode.\n\n        protocol_factory should instantiate object with Protocol interface.\n        pipe is a file-like object.\n        Return pair (transport, protocol), where transport supports the\n        ReadTransport interface.\"\"\"\n        # The reason to accept file-like object instead of just file descriptor\n        # is: we need to own pipe and close it at transport finishing\n        # Can got complicated errors if pass f.fileno(),\n        # close fd in pipe transport then close f and vise versa.\n        raise NotImplementedError\n\n    async def connect_write_pipe(self, protocol_factory, pipe):\n        \"\"\"Register write pipe in event loop.\n\n        protocol_factory should instantiate object with BaseProtocol interface.\n        Pipe is file-like object already switched to nonblocking.\n        Return pair (transport, protocol), where transport support\n        WriteTransport interface.\"\"\"\n        # The reason to accept file-like object instead of just file descriptor\n        # is: we need to own pipe and close it at transport finishing\n        # Can got complicated errors if pass f.fileno(),\n        # close fd in pipe transport then close f and vise versa.\n        raise NotImplementedError\n\n    async def subprocess_shell(self, protocol_factory, cmd, *,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               **kwargs):\n        raise NotImplementedError\n\n    async def subprocess_exec(self, protocol_factory, *args,\n                              stdin=subprocess.PIPE,\n                              stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE,\n                              **kwargs):\n        raise NotImplementedError\n\n    # Ready-based callback registration methods.\n    # The add_*() methods return None.\n    # The remove_*() methods return True if something was removed,\n    # False if there was nothing to delete.\n\n    def add_reader(self, fd, callback, *args):\n        raise NotImplementedError\n\n    def remove_reader(self, fd):\n        raise NotImplementedError\n\n    def add_writer(self, fd, callback, *args):\n        raise NotImplementedError\n\n    def remove_writer(self, fd):\n        raise NotImplementedError\n\n    # Completion based I/O methods returning Futures.\n\n    async def sock_recv(self, sock, nbytes):\n        raise NotImplementedError\n\n    async def sock_recv_into(self, sock, buf):\n        raise NotImplementedError\n\n    async def sock_sendall(self, sock, data):\n        raise NotImplementedError\n\n    async def sock_connect(self, sock, address):\n        raise NotImplementedError\n\n    async def sock_accept(self, sock):\n        raise NotImplementedError\n\n    async def sock_sendfile(self, sock, file, offset=0, count=None,\n                            *, fallback=None):\n        raise NotImplementedError\n\n    # Signal handling.\n\n    def add_signal_handler(self, sig, callback, *args):\n        raise NotImplementedError\n\n    def remove_signal_handler(self, sig):\n        raise NotImplementedError\n\n    # Task factory.\n\n    def set_task_factory(self, factory):\n        raise NotImplementedError\n\n    def get_task_factory(self):\n        raise NotImplementedError\n\n    # Error handlers.\n\n    def get_exception_handler(self):\n        raise NotImplementedError\n\n    def set_exception_handler(self, handler):\n        raise NotImplementedError\n\n    def default_exception_handler(self, context):\n        raise NotImplementedError\n\n    def call_exception_handler(self, context):\n        raise NotImplementedError\n\n    # Debug flag management.\n\n    def get_debug(self):\n        raise NotImplementedError\n\n    def set_debug(self, enabled):\n        raise NotImplementedError\n\n\nclass AbstractEventLoopPolicy:\n    \"\"\"Abstract policy for accessing the event loop.\"\"\"\n\n    def get_event_loop(self):\n        \"\"\"Get the event loop for the current context.\n\n        Returns an event loop object implementing the BaseEventLoop interface,\n        or raises an exception in case no event loop has been set for the\n        current context and the current policy does not specify to create one.\n\n        It should never return None.\"\"\"\n        raise NotImplementedError\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop for the current context to loop.\"\"\"\n        raise NotImplementedError\n\n    def new_event_loop(self):\n        \"\"\"Create and return a new event loop object according to this\n        policy's rules. If there's need to set this loop as the event loop for\n        the current context, set_event_loop must be called explicitly.\"\"\"\n        raise NotImplementedError\n\n    # Child processes handling (Unix only).\n\n    def get_child_watcher(self):\n        \"Get the watcher for child processes.\"\n        raise NotImplementedError\n\n    def set_child_watcher(self, watcher):\n        \"\"\"Set the watcher for child processes.\"\"\"\n        raise NotImplementedError\n\n\nclass BaseDefaultEventLoopPolicy(AbstractEventLoopPolicy):\n    \"\"\"Default policy implementation for accessing the event loop.\n\n    In this policy, each thread has its own event loop.  However, we\n    only automatically create an event loop by default for the main\n    thread; other threads by default have no event loop.\n\n    Other policies may have different rules (e.g. a single global\n    event loop, or automatically creating an event loop per thread, or\n    using some other notion of context to which an event loop is\n    associated).\n    \"\"\"\n\n    _loop_factory = None\n\n    class _Local(threading.local):\n        _loop = None\n        _set_called = False\n\n    def __init__(self):\n        self._local = self._Local()\n\n    def get_event_loop(self):\n        \"\"\"Get the event loop for the current context.\n\n        Returns an instance of EventLoop or raises an exception.\n        \"\"\"\n        if (self._local._loop is None and\n                not self._local._set_called and\n                isinstance(threading.current_thread(), threading._MainThread)):\n            self.set_event_loop(self.new_event_loop())\n\n        if self._local._loop is None:\n            raise RuntimeError('There is no current event loop in thread %r.'\n                               % threading.current_thread().name)\n\n        return self._local._loop\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop.\"\"\"\n        self._local._set_called = True\n        assert loop is None or isinstance(loop, AbstractEventLoop)\n        self._local._loop = loop\n\n    def new_event_loop(self):\n        \"\"\"Create a new event loop.\n\n        You must call set_event_loop() to make this the current event\n        loop.\n        \"\"\"\n        return self._loop_factory()\n\n\n# Event loop policy.  The policy itself is always global, even if the\n# policy's rules say that there is an event loop per thread (or other\n# notion of context).  The default policy is installed by the first\n# call to get_event_loop_policy().\n_event_loop_policy = None\n\n# Lock for protecting the on-the-fly creation of the event loop policy.\n_lock = threading.Lock()\n\n\n# A TLS for the running event loop, used by _get_running_loop.\nclass _RunningLoop(threading.local):\n    loop_pid = (None, None)\n\n\n_running_loop = _RunningLoop()\n\n\ndef get_running_loop():\n    \"\"\"Return the running event loop.  Raise a RuntimeError if there is none.\n\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    loop = _get_running_loop()\n    if loop is None:\n        raise RuntimeError('no running event loop')\n    return loop\n\n\ndef _get_running_loop():\n    \"\"\"Return the running event loop or None.\n\n    This is a low-level function intended to be used by event loops.\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    running_loop, pid = _running_loop.loop_pid\n    if running_loop is not None and pid == os.getpid():\n        return running_loop\n\n\ndef _set_running_loop(loop):\n    \"\"\"Set the running event loop.\n\n    This is a low-level function intended to be used by event loops.\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    _running_loop.loop_pid = (loop, os.getpid())\n\n\ndef _init_event_loop_policy():\n    global _event_loop_policy\n    with _lock:\n        if _event_loop_policy is None:  # pragma: no branch\n            from . import DefaultEventLoopPolicy\n            _event_loop_policy = DefaultEventLoopPolicy()\n\n\ndef get_event_loop_policy():\n    \"\"\"Get the current event loop policy.\"\"\"\n    if _event_loop_policy is None:\n        _init_event_loop_policy()\n    return _event_loop_policy\n\n\ndef set_event_loop_policy(policy):\n    \"\"\"Set the current event loop policy.\n\n    If policy is None, the default policy is restored.\"\"\"\n    global _event_loop_policy\n    assert policy is None or isinstance(policy, AbstractEventLoopPolicy)\n    _event_loop_policy = policy\n\n\ndef get_event_loop():\n    \"\"\"Return an asyncio event loop.\n\n    When called from a coroutine or a callback (e.g. scheduled with call_soon\n    or similar API), this function will always return the running event loop.\n\n    If there is no running event loop set, the function will return\n    the result of `get_event_loop_policy().get_event_loop()` call.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    current_loop = _get_running_loop()\n    if current_loop is not None:\n        return current_loop\n    return get_event_loop_policy().get_event_loop()\n\n\ndef set_event_loop(loop):\n    \"\"\"Equivalent to calling get_event_loop_policy().set_event_loop(loop).\"\"\"\n    get_event_loop_policy().set_event_loop(loop)\n\n\ndef new_event_loop():\n    \"\"\"Equivalent to calling get_event_loop_policy().new_event_loop().\"\"\"\n    return get_event_loop_policy().new_event_loop()\n\n\ndef get_child_watcher():\n    \"\"\"Equivalent to calling get_event_loop_policy().get_child_watcher().\"\"\"\n    return get_event_loop_policy().get_child_watcher()\n\n\ndef set_child_watcher(watcher):\n    \"\"\"Equivalent to calling\n    get_event_loop_policy().set_child_watcher(watcher).\"\"\"\n    return get_event_loop_policy().set_child_watcher(watcher)\n\n\n# Alias pure-Python implementations for testing purposes.\n_py__get_running_loop = _get_running_loop\n_py__set_running_loop = _set_running_loop\n_py_get_running_loop = get_running_loop\n_py_get_event_loop = get_event_loop\n\n\ntry:\n    # get_event_loop() is one of the most frequently called\n    # functions in asyncio.  Pure Python implementation is\n    # about 4 times slower than C-accelerated.\n    from _asyncio import (_get_running_loop, _set_running_loop,\n                          get_running_loop, get_event_loop)\nexcept ImportError:\n    pass\nelse:\n    # Alias C implementations for testing purposes.\n    _c__get_running_loop = _get_running_loop\n    _c__set_running_loop = _set_running_loop\n    _c_get_running_loop = get_running_loop\n    _c_get_event_loop = get_event_loop\n", 792], "/usr/lib/python3.8/asyncio/unix_events.py": ["\"\"\"Selector event loop for Unix with signal handling.\"\"\"\n\nimport errno\nimport io\nimport itertools\nimport os\nimport selectors\nimport signal\nimport socket\nimport stat\nimport subprocess\nimport sys\nimport threading\nimport warnings\n\nfrom . import base_events\nfrom . import base_subprocess\nfrom . import constants\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom . import selector_events\nfrom . import tasks\nfrom . import transports\nfrom .log import logger\n\n\n__all__ = (\n    'SelectorEventLoop',\n    'AbstractChildWatcher', 'SafeChildWatcher',\n    'FastChildWatcher',\n    'MultiLoopChildWatcher', 'ThreadedChildWatcher',\n    'DefaultEventLoopPolicy',\n)\n\n\nif sys.platform == 'win32':  # pragma: no cover\n    raise ImportError('Signals are not really supported on Windows')\n\n\ndef _sighandler_noop(signum, frame):\n    \"\"\"Dummy signal handler.\"\"\"\n    pass\n\n\nclass _UnixSelectorEventLoop(selector_events.BaseSelectorEventLoop):\n    \"\"\"Unix event loop.\n\n    Adds signal handling and UNIX Domain Socket support to SelectorEventLoop.\n    \"\"\"\n\n    def __init__(self, selector=None):\n        super().__init__(selector)\n        self._signal_handlers = {}\n\n    def close(self):\n        super().close()\n        if not sys.is_finalizing():\n            for sig in list(self._signal_handlers):\n                self.remove_signal_handler(sig)\n        else:\n            if self._signal_handlers:\n                warnings.warn(f\"Closing the loop {self!r} \"\n                              f\"on interpreter shutdown \"\n                              f\"stage, skipping signal handlers removal\",\n                              ResourceWarning,\n                              source=self)\n                self._signal_handlers.clear()\n\n    def _process_self_data(self, data):\n        for signum in data:\n            if not signum:\n                # ignore null bytes written by _write_to_self()\n                continue\n            self._handle_signal(signum)\n\n    def add_signal_handler(self, sig, callback, *args):\n        \"\"\"Add a handler for a signal.  UNIX only.\n\n        Raise ValueError if the signal number is invalid or uncatchable.\n        Raise RuntimeError if there is a problem setting up the handler.\n        \"\"\"\n        if (coroutines.iscoroutine(callback) or\n                coroutines.iscoroutinefunction(callback)):\n            raise TypeError(\"coroutines cannot be used \"\n                            \"with add_signal_handler()\")\n        self._check_signal(sig)\n        self._check_closed()\n        try:\n            # set_wakeup_fd() raises ValueError if this is not the\n            # main thread.  By calling it early we ensure that an\n            # event loop running in another thread cannot add a signal\n            # handler.\n            signal.set_wakeup_fd(self._csock.fileno())\n        except (ValueError, OSError) as exc:\n            raise RuntimeError(str(exc))\n\n        handle = events.Handle(callback, args, self, None)\n        self._signal_handlers[sig] = handle\n\n        try:\n            # Register a dummy signal handler to ask Python to write the signal\n            # number in the wakeup file descriptor. _process_self_data() will\n            # read signal numbers from this file descriptor to handle signals.\n            signal.signal(sig, _sighandler_noop)\n\n            # Set SA_RESTART to limit EINTR occurrences.\n            signal.siginterrupt(sig, False)\n        except OSError as exc:\n            del self._signal_handlers[sig]\n            if not self._signal_handlers:\n                try:\n                    signal.set_wakeup_fd(-1)\n                except (ValueError, OSError) as nexc:\n                    logger.info('set_wakeup_fd(-1) failed: %s', nexc)\n\n            if exc.errno == errno.EINVAL:\n                raise RuntimeError(f'sig {sig} cannot be caught')\n            else:\n                raise\n\n    def _handle_signal(self, sig):\n        \"\"\"Internal helper that is the actual signal handler.\"\"\"\n        handle = self._signal_handlers.get(sig)\n        if handle is None:\n            return  # Assume it's some race condition.\n        if handle._cancelled:\n            self.remove_signal_handler(sig)  # Remove it properly.\n        else:\n            self._add_callback_signalsafe(handle)\n\n    def remove_signal_handler(self, sig):\n        \"\"\"Remove a handler for a signal.  UNIX only.\n\n        Return True if a signal handler was removed, False if not.\n        \"\"\"\n        self._check_signal(sig)\n        try:\n            del self._signal_handlers[sig]\n        except KeyError:\n            return False\n\n        if sig == signal.SIGINT:\n            handler = signal.default_int_handler\n        else:\n            handler = signal.SIG_DFL\n\n        try:\n            signal.signal(sig, handler)\n        except OSError as exc:\n            if exc.errno == errno.EINVAL:\n                raise RuntimeError(f'sig {sig} cannot be caught')\n            else:\n                raise\n\n        if not self._signal_handlers:\n            try:\n                signal.set_wakeup_fd(-1)\n            except (ValueError, OSError) as exc:\n                logger.info('set_wakeup_fd(-1) failed: %s', exc)\n\n        return True\n\n    def _check_signal(self, sig):\n        \"\"\"Internal helper to validate a signal.\n\n        Raise ValueError if the signal number is invalid or uncatchable.\n        Raise RuntimeError if there is a problem setting up the handler.\n        \"\"\"\n        if not isinstance(sig, int):\n            raise TypeError(f'sig must be an int, not {sig!r}')\n\n        if sig not in signal.valid_signals():\n            raise ValueError(f'invalid signal number {sig}')\n\n    def _make_read_pipe_transport(self, pipe, protocol, waiter=None,\n                                  extra=None):\n        return _UnixReadPipeTransport(self, pipe, protocol, waiter, extra)\n\n    def _make_write_pipe_transport(self, pipe, protocol, waiter=None,\n                                   extra=None):\n        return _UnixWritePipeTransport(self, pipe, protocol, waiter, extra)\n\n    async def _make_subprocess_transport(self, protocol, args, shell,\n                                         stdin, stdout, stderr, bufsize,\n                                         extra=None, **kwargs):\n        with events.get_child_watcher() as watcher:\n            if not watcher.is_active():\n                # Check early.\n                # Raising exception before process creation\n                # prevents subprocess execution if the watcher\n                # is not ready to handle it.\n                raise RuntimeError(\"asyncio.get_child_watcher() is not activated, \"\n                                   \"subprocess support is not installed.\")\n            waiter = self.create_future()\n            transp = _UnixSubprocessTransport(self, protocol, args, shell,\n                                              stdin, stdout, stderr, bufsize,\n                                              waiter=waiter, extra=extra,\n                                              **kwargs)\n\n            watcher.add_child_handler(transp.get_pid(),\n                                      self._child_watcher_callback, transp)\n            try:\n                await waiter\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException:\n                transp.close()\n                await transp._wait()\n                raise\n\n        return transp\n\n    def _child_watcher_callback(self, pid, returncode, transp):\n        self.call_soon_threadsafe(transp._process_exited, returncode)\n\n    async def create_unix_connection(\n            self, protocol_factory, path=None, *,\n            ssl=None, sock=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None):\n        assert server_hostname is None or isinstance(server_hostname, str)\n        if ssl:\n            if server_hostname is None:\n                raise ValueError(\n                    'you have to pass server_hostname when using ssl')\n        else:\n            if server_hostname is not None:\n                raise ValueError('server_hostname is only meaningful with ssl')\n            if ssl_handshake_timeout is not None:\n                raise ValueError(\n                    'ssl_handshake_timeout is only meaningful with ssl')\n\n        if path is not None:\n            if sock is not None:\n                raise ValueError(\n                    'path and sock can not be specified at the same time')\n\n            path = os.fspath(path)\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM, 0)\n            try:\n                sock.setblocking(False)\n                await self.sock_connect(sock, path)\n            except:\n                sock.close()\n                raise\n\n        else:\n            if sock is None:\n                raise ValueError('no path and sock were specified')\n            if (sock.family != socket.AF_UNIX or\n                    sock.type != socket.SOCK_STREAM):\n                raise ValueError(\n                    f'A UNIX Domain Stream Socket was expected, got {sock!r}')\n            sock.setblocking(False)\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout)\n        return transport, protocol\n\n    async def create_unix_server(\n            self, protocol_factory, path=None, *,\n            sock=None, backlog=100, ssl=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        if isinstance(ssl, bool):\n            raise TypeError('ssl argument must be an SSLContext or None')\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if path is not None:\n            if sock is not None:\n                raise ValueError(\n                    'path and sock can not be specified at the same time')\n\n            path = os.fspath(path)\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n\n            # Check for abstract socket. `str` and `bytes` paths are supported.\n            if path[0] not in (0, '\\x00'):\n                try:\n                    if stat.S_ISSOCK(os.stat(path).st_mode):\n                        os.remove(path)\n                except FileNotFoundError:\n                    pass\n                except OSError as err:\n                    # Directory may have permissions only to create socket.\n                    logger.error('Unable to check or remove stale UNIX socket '\n                                 '%r: %r', path, err)\n\n            try:\n                sock.bind(path)\n            except OSError as exc:\n                sock.close()\n                if exc.errno == errno.EADDRINUSE:\n                    # Let's improve the error message by adding\n                    # with what exact address it occurs.\n                    msg = f'Address {path!r} is already in use'\n                    raise OSError(errno.EADDRINUSE, msg) from None\n                else:\n                    raise\n            except:\n                sock.close()\n                raise\n        else:\n            if sock is None:\n                raise ValueError(\n                    'path was not specified, and no sock specified')\n\n            if (sock.family != socket.AF_UNIX or\n                    sock.type != socket.SOCK_STREAM):\n                raise ValueError(\n                    f'A UNIX Domain Stream Socket was expected, got {sock!r}')\n\n        sock.setblocking(False)\n        server = base_events.Server(self, [sock], protocol_factory,\n                                    ssl, backlog, ssl_handshake_timeout)\n        if start_serving:\n            server._start_serving()\n            # Skip one loop iteration so that all 'loop.add_reader'\n            # go through.\n            await tasks.sleep(0, loop=self)\n\n        return server\n\n    async def _sock_sendfile_native(self, sock, file, offset, count):\n        try:\n            os.sendfile\n        except AttributeError as exc:\n            raise exceptions.SendfileNotAvailableError(\n                \"os.sendfile() is not available\")\n        try:\n            fileno = file.fileno()\n        except (AttributeError, io.UnsupportedOperation) as err:\n            raise exceptions.SendfileNotAvailableError(\"not a regular file\")\n        try:\n            fsize = os.fstat(fileno).st_size\n        except OSError as err:\n            raise exceptions.SendfileNotAvailableError(\"not a regular file\")\n        blocksize = count if count else fsize\n        if not blocksize:\n            return 0  # empty file\n\n        fut = self.create_future()\n        self._sock_sendfile_native_impl(fut, None, sock, fileno,\n                                        offset, count, blocksize, 0)\n        return await fut\n\n    def _sock_sendfile_native_impl(self, fut, registered_fd, sock, fileno,\n                                   offset, count, blocksize, total_sent):\n        fd = sock.fileno()\n        if registered_fd is not None:\n            # Remove the callback early.  It should be rare that the\n            # selector says the fd is ready but the call still returns\n            # EAGAIN, and I am willing to take a hit in that case in\n            # order to simplify the common case.\n            self.remove_writer(registered_fd)\n        if fut.cancelled():\n            self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n            return\n        if count:\n            blocksize = count - total_sent\n            if blocksize <= 0:\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_result(total_sent)\n                return\n\n        try:\n            sent = os.sendfile(fd, fileno, offset, blocksize)\n        except (BlockingIOError, InterruptedError):\n            if registered_fd is None:\n                self._sock_add_cancellation_callback(fut, sock)\n            self.add_writer(fd, self._sock_sendfile_native_impl, fut,\n                            fd, sock, fileno,\n                            offset, count, blocksize, total_sent)\n        except OSError as exc:\n            if (registered_fd is not None and\n                    exc.errno == errno.ENOTCONN and\n                    type(exc) is not ConnectionError):\n                # If we have an ENOTCONN and this isn't a first call to\n                # sendfile(), i.e. the connection was closed in the middle\n                # of the operation, normalize the error to ConnectionError\n                # to make it consistent across all Posix systems.\n                new_exc = ConnectionError(\n                    \"socket is not connected\", errno.ENOTCONN)\n                new_exc.__cause__ = exc\n                exc = new_exc\n            if total_sent == 0:\n                # We can get here for different reasons, the main\n                # one being 'file' is not a regular mmap(2)-like\n                # file, in which case we'll fall back on using\n                # plain send().\n                err = exceptions.SendfileNotAvailableError(\n                    \"os.sendfile call failed\")\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_exception(err)\n            else:\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_exception(exc)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n            fut.set_exception(exc)\n        else:\n            if sent == 0:\n                # EOF\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_result(total_sent)\n            else:\n                offset += sent\n                total_sent += sent\n                if registered_fd is None:\n                    self._sock_add_cancellation_callback(fut, sock)\n                self.add_writer(fd, self._sock_sendfile_native_impl, fut,\n                                fd, sock, fileno,\n                                offset, count, blocksize, total_sent)\n\n    def _sock_sendfile_update_filepos(self, fileno, offset, total_sent):\n        if total_sent > 0:\n            os.lseek(fileno, offset, os.SEEK_SET)\n\n    def _sock_add_cancellation_callback(self, fut, sock):\n        def cb(fut):\n            if fut.cancelled():\n                fd = sock.fileno()\n                if fd != -1:\n                    self.remove_writer(fd)\n        fut.add_done_callback(cb)\n\n\nclass _UnixReadPipeTransport(transports.ReadTransport):\n\n    max_size = 256 * 1024  # max bytes we read in one event loop iteration\n\n    def __init__(self, loop, pipe, protocol, waiter=None, extra=None):\n        super().__init__(extra)\n        self._extra['pipe'] = pipe\n        self._loop = loop\n        self._pipe = pipe\n        self._fileno = pipe.fileno()\n        self._protocol = protocol\n        self._closing = False\n        self._paused = False\n\n        mode = os.fstat(self._fileno).st_mode\n        if not (stat.S_ISFIFO(mode) or\n                stat.S_ISSOCK(mode) or\n                stat.S_ISCHR(mode)):\n            self._pipe = None\n            self._fileno = None\n            self._protocol = None\n            raise ValueError(\"Pipe transport is for pipes/sockets only.\")\n\n        os.set_blocking(self._fileno, False)\n\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._loop._add_reader,\n                             self._fileno, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._pipe is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._fileno}')\n        selector = getattr(self._loop, '_selector', None)\n        if self._pipe is not None and selector is not None:\n            polling = selector_events._test_selector_event(\n                selector, self._fileno, selectors.EVENT_READ)\n            if polling:\n                info.append('polling')\n            else:\n                info.append('idle')\n        elif self._pipe is not None:\n            info.append('open')\n        else:\n            info.append('closed')\n        return '<{}>'.format(' '.join(info))\n\n    def _read_ready(self):\n        try:\n            data = os.read(self._fileno, self.max_size)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except OSError as exc:\n            self._fatal_error(exc, 'Fatal read error on pipe transport')\n        else:\n            if data:\n                self._protocol.data_received(data)\n            else:\n                if self._loop.get_debug():\n                    logger.info(\"%r was closed by peer\", self)\n                self._closing = True\n                self._loop._remove_reader(self._fileno)\n                self._loop.call_soon(self._protocol.eof_received)\n                self._loop.call_soon(self._call_connection_lost, None)\n\n    def pause_reading(self):\n        if self._closing or self._paused:\n            return\n        self._paused = True\n        self._loop._remove_reader(self._fileno)\n        if self._loop.get_debug():\n            logger.debug(\"%r pauses reading\", self)\n\n    def resume_reading(self):\n        if self._closing or not self._paused:\n            return\n        self._paused = False\n        self._loop._add_reader(self._fileno, self._read_ready)\n        if self._loop.get_debug():\n            logger.debug(\"%r resumes reading\", self)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def close(self):\n        if not self._closing:\n            self._close(None)\n\n    def __del__(self, _warn=warnings.warn):\n        if self._pipe is not None:\n            _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n            self._pipe.close()\n\n    def _fatal_error(self, exc, message='Fatal error on pipe transport'):\n        # should be called by exception handler only\n        if (isinstance(exc, OSError) and exc.errno == errno.EIO):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._close(exc)\n\n    def _close(self, exc):\n        self._closing = True\n        self._loop._remove_reader(self._fileno)\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            self._protocol.connection_lost(exc)\n        finally:\n            self._pipe.close()\n            self._pipe = None\n            self._protocol = None\n            self._loop = None\n\n\nclass _UnixWritePipeTransport(transports._FlowControlMixin,\n                              transports.WriteTransport):\n\n    def __init__(self, loop, pipe, protocol, waiter=None, extra=None):\n        super().__init__(extra, loop)\n        self._extra['pipe'] = pipe\n        self._pipe = pipe\n        self._fileno = pipe.fileno()\n        self._protocol = protocol\n        self._buffer = bytearray()\n        self._conn_lost = 0\n        self._closing = False  # Set when close() or write_eof() called.\n\n        mode = os.fstat(self._fileno).st_mode\n        is_char = stat.S_ISCHR(mode)\n        is_fifo = stat.S_ISFIFO(mode)\n        is_socket = stat.S_ISSOCK(mode)\n        if not (is_char or is_fifo or is_socket):\n            self._pipe = None\n            self._fileno = None\n            self._protocol = None\n            raise ValueError(\"Pipe transport is only for \"\n                             \"pipes, sockets and character devices\")\n\n        os.set_blocking(self._fileno, False)\n        self._loop.call_soon(self._protocol.connection_made, self)\n\n        # On AIX, the reader trick (to be notified when the read end of the\n        # socket is closed) only works for sockets. On other platforms it\n        # works for pipes and sockets. (Exception: OS X 10.4?  Issue #19294.)\n        if is_socket or (is_fifo and not sys.platform.startswith(\"aix\")):\n            # only start reading when connection_made() has been called\n            self._loop.call_soon(self._loop._add_reader,\n                                 self._fileno, self._read_ready)\n\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._pipe is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._fileno}')\n        selector = getattr(self._loop, '_selector', None)\n        if self._pipe is not None and selector is not None:\n            polling = selector_events._test_selector_event(\n                selector, self._fileno, selectors.EVENT_WRITE)\n            if polling:\n                info.append('polling')\n            else:\n                info.append('idle')\n\n            bufsize = self.get_write_buffer_size()\n            info.append(f'bufsize={bufsize}')\n        elif self._pipe is not None:\n            info.append('open')\n        else:\n            info.append('closed')\n        return '<{}>'.format(' '.join(info))\n\n    def get_write_buffer_size(self):\n        return len(self._buffer)\n\n    def _read_ready(self):\n        # Pipe was closed by peer.\n        if self._loop.get_debug():\n            logger.info(\"%r was closed by peer\", self)\n        if self._buffer:\n            self._close(BrokenPipeError())\n        else:\n            self._close()\n\n    def write(self, data):\n        assert isinstance(data, (bytes, bytearray, memoryview)), repr(data)\n        if isinstance(data, bytearray):\n            data = memoryview(data)\n        if not data:\n            return\n\n        if self._conn_lost or self._closing:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('pipe closed by peer or '\n                               'os.write(pipe, data) raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Attempt to send it right away first.\n            try:\n                n = os.write(self._fileno, data)\n            except (BlockingIOError, InterruptedError):\n                n = 0\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._conn_lost += 1\n                self._fatal_error(exc, 'Fatal write error on pipe transport')\n                return\n            if n == len(data):\n                return\n            elif n > 0:\n                data = memoryview(data)[n:]\n            self._loop._add_writer(self._fileno, self._write_ready)\n\n        self._buffer += data\n        self._maybe_pause_protocol()\n\n    def _write_ready(self):\n        assert self._buffer, 'Data should not be empty'\n\n        try:\n            n = os.write(self._fileno, self._buffer)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._buffer.clear()\n            self._conn_lost += 1\n            # Remove writer here, _fatal_error() doesn't it\n            # because _buffer is empty.\n            self._loop._remove_writer(self._fileno)\n            self._fatal_error(exc, 'Fatal write error on pipe transport')\n        else:\n            if n == len(self._buffer):\n                self._buffer.clear()\n                self._loop._remove_writer(self._fileno)\n                self._maybe_resume_protocol()  # May append to buffer.\n                if self._closing:\n                    self._loop._remove_reader(self._fileno)\n                    self._call_connection_lost(None)\n                return\n            elif n > 0:\n                del self._buffer[:n]\n\n    def can_write_eof(self):\n        return True\n\n    def write_eof(self):\n        if self._closing:\n            return\n        assert self._pipe\n        self._closing = True\n        if not self._buffer:\n            self._loop._remove_reader(self._fileno)\n            self._loop.call_soon(self._call_connection_lost, None)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def close(self):\n        if self._pipe is not None and not self._closing:\n            # write_eof is all what we needed to close the write pipe\n            self.write_eof()\n\n    def __del__(self, _warn=warnings.warn):\n        if self._pipe is not None:\n            _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n            self._pipe.close()\n\n    def abort(self):\n        self._close(None)\n\n    def _fatal_error(self, exc, message='Fatal error on pipe transport'):\n        # should be called by exception handler only\n        if isinstance(exc, OSError):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._close(exc)\n\n    def _close(self, exc=None):\n        self._closing = True\n        if self._buffer:\n            self._loop._remove_writer(self._fileno)\n        self._buffer.clear()\n        self._loop._remove_reader(self._fileno)\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            self._protocol.connection_lost(exc)\n        finally:\n            self._pipe.close()\n            self._pipe = None\n            self._protocol = None\n            self._loop = None\n\n\nclass _UnixSubprocessTransport(base_subprocess.BaseSubprocessTransport):\n\n    def _start(self, args, shell, stdin, stdout, stderr, bufsize, **kwargs):\n        stdin_w = None\n        if stdin == subprocess.PIPE:\n            # Use a socket pair for stdin, since not all platforms\n            # support selecting read events on the write end of a\n            # socket (which we use in order to detect closing of the\n            # other end).  Notably this is needed on AIX, and works\n            # just fine on other platforms.\n            stdin, stdin_w = socket.socketpair()\n        try:\n            self._proc = subprocess.Popen(\n                args, shell=shell, stdin=stdin, stdout=stdout, stderr=stderr,\n                universal_newlines=False, bufsize=bufsize, **kwargs)\n            if stdin_w is not None:\n                stdin.close()\n                self._proc.stdin = open(stdin_w.detach(), 'wb', buffering=bufsize)\n                stdin_w = None\n        finally:\n            if stdin_w is not None:\n                stdin.close()\n                stdin_w.close()\n\n\nclass AbstractChildWatcher:\n    \"\"\"Abstract base class for monitoring child processes.\n\n    Objects derived from this class monitor a collection of subprocesses and\n    report their termination or interruption by a signal.\n\n    New callbacks are registered with .add_child_handler(). Starting a new\n    process must be done within a 'with' block to allow the watcher to suspend\n    its activity until the new process if fully registered (this is needed to\n    prevent a race condition in some implementations).\n\n    Example:\n        with watcher:\n            proc = subprocess.Popen(\"sleep 1\")\n            watcher.add_child_handler(proc.pid, callback)\n\n    Notes:\n        Implementations of this class must be thread-safe.\n\n        Since child watcher objects may catch the SIGCHLD signal and call\n        waitpid(-1), there should be only one active object per process.\n    \"\"\"\n\n    def add_child_handler(self, pid, callback, *args):\n        \"\"\"Register a new child handler.\n\n        Arrange for callback(pid, returncode, *args) to be called when\n        process 'pid' terminates. Specifying another callback for the same\n        process replaces the previous handler.\n\n        Note: callback() must be thread-safe.\n        \"\"\"\n        raise NotImplementedError()\n\n    def remove_child_handler(self, pid):\n        \"\"\"Removes the handler for process 'pid'.\n\n        The function returns True if the handler was successfully removed,\n        False if there was nothing to remove.\"\"\"\n\n        raise NotImplementedError()\n\n    def attach_loop(self, loop):\n        \"\"\"Attach the watcher to an event loop.\n\n        If the watcher was previously attached to an event loop, then it is\n        first detached before attaching to the new loop.\n\n        Note: loop may be None.\n        \"\"\"\n        raise NotImplementedError()\n\n    def close(self):\n        \"\"\"Close the watcher.\n\n        This must be called to make sure that any underlying resource is freed.\n        \"\"\"\n        raise NotImplementedError()\n\n    def is_active(self):\n        \"\"\"Return ``True`` if the watcher is active and is used by the event loop.\n\n        Return True if the watcher is installed and ready to handle process exit\n        notifications.\n\n        \"\"\"\n        raise NotImplementedError()\n\n    def __enter__(self):\n        \"\"\"Enter the watcher's context and allow starting new processes\n\n        This function must return self\"\"\"\n        raise NotImplementedError()\n\n    def __exit__(self, a, b, c):\n        \"\"\"Exit the watcher's context\"\"\"\n        raise NotImplementedError()\n\n\ndef _compute_returncode(status):\n    if os.WIFSIGNALED(status):\n        # The child process died because of a signal.\n        return -os.WTERMSIG(status)\n    elif os.WIFEXITED(status):\n        # The child process exited (e.g sys.exit()).\n        return os.WEXITSTATUS(status)\n    else:\n        # The child exited, but we don't understand its status.\n        # This shouldn't happen, but if it does, let's just\n        # return that status; perhaps that helps debug it.\n        return status\n\n\nclass BaseChildWatcher(AbstractChildWatcher):\n\n    def __init__(self):\n        self._loop = None\n        self._callbacks = {}\n\n    def close(self):\n        self.attach_loop(None)\n\n    def is_active(self):\n        return self._loop is not None and self._loop.is_running()\n\n    def _do_waitpid(self, expected_pid):\n        raise NotImplementedError()\n\n    def _do_waitpid_all(self):\n        raise NotImplementedError()\n\n    def attach_loop(self, loop):\n        assert loop is None or isinstance(loop, events.AbstractEventLoop)\n\n        if self._loop is not None and loop is None and self._callbacks:\n            warnings.warn(\n                'A loop is being detached '\n                'from a child watcher with pending handlers',\n                RuntimeWarning)\n\n        if self._loop is not None:\n            self._loop.remove_signal_handler(signal.SIGCHLD)\n\n        self._loop = loop\n        if loop is not None:\n            loop.add_signal_handler(signal.SIGCHLD, self._sig_chld)\n\n            # Prevent a race condition in case a child terminated\n            # during the switch.\n            self._do_waitpid_all()\n\n    def _sig_chld(self):\n        try:\n            self._do_waitpid_all()\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            # self._loop should always be available here\n            # as '_sig_chld' is added as a signal handler\n            # in 'attach_loop'\n            self._loop.call_exception_handler({\n                'message': 'Unknown exception in SIGCHLD handler',\n                'exception': exc,\n            })\n\n\nclass SafeChildWatcher(BaseChildWatcher):\n    \"\"\"'Safe' child watcher implementation.\n\n    This implementation avoids disrupting other code spawning processes by\n    polling explicitly each process in the SIGCHLD handler instead of calling\n    os.waitpid(-1).\n\n    This is a safe solution but it has a significant overhead when handling a\n    big number of children (O(n) each time SIGCHLD is raised)\n    \"\"\"\n\n    def close(self):\n        self._callbacks.clear()\n        super().close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, a, b, c):\n        pass\n\n    def add_child_handler(self, pid, callback, *args):\n        self._callbacks[pid] = (callback, args)\n\n        # Prevent a race condition in case the child is already terminated.\n        self._do_waitpid(pid)\n\n    def remove_child_handler(self, pid):\n        try:\n            del self._callbacks[pid]\n            return True\n        except KeyError:\n            return False\n\n    def _do_waitpid_all(self):\n\n        for pid in list(self._callbacks):\n            self._do_waitpid(pid)\n\n    def _do_waitpid(self, expected_pid):\n        assert expected_pid > 0\n\n        try:\n            pid, status = os.waitpid(expected_pid, os.WNOHANG)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            pid = expected_pid\n            returncode = 255\n            logger.warning(\n                \"Unknown child process pid %d, will report returncode 255\",\n                pid)\n        else:\n            if pid == 0:\n                # The child process is still alive.\n                return\n\n            returncode = _compute_returncode(status)\n            if self._loop.get_debug():\n                logger.debug('process %s exited with returncode %s',\n                             expected_pid, returncode)\n\n        try:\n            callback, args = self._callbacks.pop(pid)\n        except KeyError:  # pragma: no cover\n            # May happen if .remove_child_handler() is called\n            # after os.waitpid() returns.\n            if self._loop.get_debug():\n                logger.warning(\"Child watcher got an unexpected pid: %r\",\n                               pid, exc_info=True)\n        else:\n            callback(pid, returncode, *args)\n\n\nclass FastChildWatcher(BaseChildWatcher):\n    \"\"\"'Fast' child watcher implementation.\n\n    This implementation reaps every terminated processes by calling\n    os.waitpid(-1) directly, possibly breaking other code spawning processes\n    and waiting for their termination.\n\n    There is no noticeable overhead when handling a big number of children\n    (O(1) each time a child terminates).\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._lock = threading.Lock()\n        self._zombies = {}\n        self._forks = 0\n\n    def close(self):\n        self._callbacks.clear()\n        self._zombies.clear()\n        super().close()\n\n    def __enter__(self):\n        with self._lock:\n            self._forks += 1\n\n            return self\n\n    def __exit__(self, a, b, c):\n        with self._lock:\n            self._forks -= 1\n\n            if self._forks or not self._zombies:\n                return\n\n            collateral_victims = str(self._zombies)\n            self._zombies.clear()\n\n        logger.warning(\n            \"Caught subprocesses termination from unknown pids: %s\",\n            collateral_victims)\n\n    def add_child_handler(self, pid, callback, *args):\n        assert self._forks, \"Must use the context manager\"\n\n        with self._lock:\n            try:\n                returncode = self._zombies.pop(pid)\n            except KeyError:\n                # The child is running.\n                self._callbacks[pid] = callback, args\n                return\n\n        # The child is dead already. We can fire the callback.\n        callback(pid, returncode, *args)\n\n    def remove_child_handler(self, pid):\n        try:\n            del self._callbacks[pid]\n            return True\n        except KeyError:\n            return False\n\n    def _do_waitpid_all(self):\n        # Because of signal coalescing, we must keep calling waitpid() as\n        # long as we're able to reap a child.\n        while True:\n            try:\n                pid, status = os.waitpid(-1, os.WNOHANG)\n            except ChildProcessError:\n                # No more child processes exist.\n                return\n            else:\n                if pid == 0:\n                    # A child process is still alive.\n                    return\n\n                returncode = _compute_returncode(status)\n\n            with self._lock:\n                try:\n                    callback, args = self._callbacks.pop(pid)\n                except KeyError:\n                    # unknown child\n                    if self._forks:\n                        # It may not be registered yet.\n                        self._zombies[pid] = returncode\n                        if self._loop.get_debug():\n                            logger.debug('unknown process %s exited '\n                                         'with returncode %s',\n                                         pid, returncode)\n                        continue\n                    callback = None\n                else:\n                    if self._loop.get_debug():\n                        logger.debug('process %s exited with returncode %s',\n                                     pid, returncode)\n\n            if callback is None:\n                logger.warning(\n                    \"Caught subprocess termination from unknown pid: \"\n                    \"%d -> %d\", pid, returncode)\n            else:\n                callback(pid, returncode, *args)\n\n\nclass MultiLoopChildWatcher(AbstractChildWatcher):\n    \"\"\"A watcher that doesn't require running loop in the main thread.\n\n    This implementation registers a SIGCHLD signal handler on\n    instantiation (which may conflict with other code that\n    install own handler for this signal).\n\n    The solution is safe but it has a significant overhead when\n    handling a big number of processes (*O(n)* each time a\n    SIGCHLD is received).\n    \"\"\"\n\n    # Implementation note:\n    # The class keeps compatibility with AbstractChildWatcher ABC\n    # To achieve this it has empty attach_loop() method\n    # and doesn't accept explicit loop argument\n    # for add_child_handler()/remove_child_handler()\n    # but retrieves the current loop by get_running_loop()\n\n    def __init__(self):\n        self._callbacks = {}\n        self._saved_sighandler = None\n\n    def is_active(self):\n        return self._saved_sighandler is not None\n\n    def close(self):\n        self._callbacks.clear()\n        if self._saved_sighandler is not None:\n            handler = signal.getsignal(signal.SIGCHLD)\n            if handler != self._sig_chld:\n                logger.warning(\"SIGCHLD handler was changed by outside code\")\n            else:\n                signal.signal(signal.SIGCHLD, self._saved_sighandler)\n            self._saved_sighandler = None\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    def add_child_handler(self, pid, callback, *args):\n        loop = events.get_running_loop()\n        self._callbacks[pid] = (loop, callback, args)\n\n        # Prevent a race condition in case the child is already terminated.\n        self._do_waitpid(pid)\n\n    def remove_child_handler(self, pid):\n        try:\n            del self._callbacks[pid]\n            return True\n        except KeyError:\n            return False\n\n    def attach_loop(self, loop):\n        # Don't save the loop but initialize itself if called first time\n        # The reason to do it here is that attach_loop() is called from\n        # unix policy only for the main thread.\n        # Main thread is required for subscription on SIGCHLD signal\n        if self._saved_sighandler is None:\n            self._saved_sighandler = signal.signal(signal.SIGCHLD, self._sig_chld)\n            if self._saved_sighandler is None:\n                logger.warning(\"Previous SIGCHLD handler was set by non-Python code, \"\n                               \"restore to default handler on watcher close.\")\n                self._saved_sighandler = signal.SIG_DFL\n\n            # Set SA_RESTART to limit EINTR occurrences.\n            signal.siginterrupt(signal.SIGCHLD, False)\n\n    def _do_waitpid_all(self):\n        for pid in list(self._callbacks):\n            self._do_waitpid(pid)\n\n    def _do_waitpid(self, expected_pid):\n        assert expected_pid > 0\n\n        try:\n            pid, status = os.waitpid(expected_pid, os.WNOHANG)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            pid = expected_pid\n            returncode = 255\n            logger.warning(\n                \"Unknown child process pid %d, will report returncode 255\",\n                pid)\n            debug_log = False\n        else:\n            if pid == 0:\n                # The child process is still alive.\n                return\n\n            returncode = _compute_returncode(status)\n            debug_log = True\n        try:\n            loop, callback, args = self._callbacks.pop(pid)\n        except KeyError:  # pragma: no cover\n            # May happen if .remove_child_handler() is called\n            # after os.waitpid() returns.\n            logger.warning(\"Child watcher got an unexpected pid: %r\",\n                           pid, exc_info=True)\n        else:\n            if loop.is_closed():\n                logger.warning(\"Loop %r that handles pid %r is closed\", loop, pid)\n            else:\n                if debug_log and loop.get_debug():\n                    logger.debug('process %s exited with returncode %s',\n                                 expected_pid, returncode)\n                loop.call_soon_threadsafe(callback, pid, returncode, *args)\n\n    def _sig_chld(self, signum, frame):\n        try:\n            self._do_waitpid_all()\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException:\n            logger.warning('Unknown exception in SIGCHLD handler', exc_info=True)\n\n\nclass ThreadedChildWatcher(AbstractChildWatcher):\n    \"\"\"Threaded child watcher implementation.\n\n    The watcher uses a thread per process\n    for waiting for the process finish.\n\n    It doesn't require subscription on POSIX signal\n    but a thread creation is not free.\n\n    The watcher has O(1) complexity, its performance doesn't depend\n    on amount of spawn processes.\n    \"\"\"\n\n    def __init__(self):\n        self._pid_counter = itertools.count(0)\n        self._threads = {}\n\n    def is_active(self):\n        return True\n\n    def close(self):\n        self._join_threads()\n\n    def _join_threads(self):\n        \"\"\"Internal: Join all non-daemon threads\"\"\"\n        threads = [thread for thread in list(self._threads.values())\n                   if thread.is_alive() and not thread.daemon]\n        for thread in threads:\n            thread.join()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    def __del__(self, _warn=warnings.warn):\n        threads = [thread for thread in list(self._threads.values())\n                   if thread.is_alive()]\n        if threads:\n            _warn(f\"{self.__class__} has registered but not finished child processes\",\n                  ResourceWarning,\n                  source=self)\n\n    def add_child_handler(self, pid, callback, *args):\n        loop = events.get_running_loop()\n        thread = threading.Thread(target=self._do_waitpid,\n                                  name=f\"waitpid-{next(self._pid_counter)}\",\n                                  args=(loop, pid, callback, args),\n                                  daemon=True)\n        self._threads[pid] = thread\n        thread.start()\n\n    def remove_child_handler(self, pid):\n        # asyncio never calls remove_child_handler() !!!\n        # The method is no-op but is implemented because\n        # abstract base classe requires it\n        return True\n\n    def attach_loop(self, loop):\n        pass\n\n    def _do_waitpid(self, loop, expected_pid, callback, args):\n        assert expected_pid > 0\n\n        try:\n            pid, status = os.waitpid(expected_pid, 0)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            pid = expected_pid\n            returncode = 255\n            logger.warning(\n                \"Unknown child process pid %d, will report returncode 255\",\n                pid)\n        else:\n            returncode = _compute_returncode(status)\n            if loop.get_debug():\n                logger.debug('process %s exited with returncode %s',\n                             expected_pid, returncode)\n\n        if loop.is_closed():\n            logger.warning(\"Loop %r that handles pid %r is closed\", loop, pid)\n        else:\n            loop.call_soon_threadsafe(callback, pid, returncode, *args)\n\n        self._threads.pop(expected_pid)\n\n\nclass _UnixDefaultEventLoopPolicy(events.BaseDefaultEventLoopPolicy):\n    \"\"\"UNIX event loop policy with a watcher for child processes.\"\"\"\n    _loop_factory = _UnixSelectorEventLoop\n\n    def __init__(self):\n        super().__init__()\n        self._watcher = None\n\n    def _init_watcher(self):\n        with events._lock:\n            if self._watcher is None:  # pragma: no branch\n                self._watcher = ThreadedChildWatcher()\n                if isinstance(threading.current_thread(),\n                              threading._MainThread):\n                    self._watcher.attach_loop(self._local._loop)\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop.\n\n        As a side effect, if a child watcher was set before, then calling\n        .set_event_loop() from the main thread will call .attach_loop(loop) on\n        the child watcher.\n        \"\"\"\n\n        super().set_event_loop(loop)\n\n        if (self._watcher is not None and\n                isinstance(threading.current_thread(), threading._MainThread)):\n            self._watcher.attach_loop(loop)\n\n    def get_child_watcher(self):\n        \"\"\"Get the watcher for child processes.\n\n        If not yet set, a ThreadedChildWatcher object is automatically created.\n        \"\"\"\n        if self._watcher is None:\n            self._init_watcher()\n\n        return self._watcher\n\n    def set_child_watcher(self, watcher):\n        \"\"\"Set the watcher for child processes.\"\"\"\n\n        assert watcher is None or isinstance(watcher, AbstractChildWatcher)\n\n        if self._watcher is not None:\n            self._watcher.close()\n\n        self._watcher = watcher\n\n\nSelectorEventLoop = _UnixSelectorEventLoop\nDefaultEventLoopPolicy = _UnixDefaultEventLoopPolicy\n", 1389], "/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/os.py": ["r\"\"\"OS routines for NT or Posix depending on what system we're on.\n\nThis exports:\n  - all functions from posix or nt, e.g. unlink, stat, etc.\n  - os.path is either posixpath or ntpath\n  - os.name is either 'posix' or 'nt'\n  - os.curdir is a string representing the current directory (always '.')\n  - os.pardir is a string representing the parent directory (always '..')\n  - os.sep is the (or a most common) pathname separator ('/' or '\\\\')\n  - os.extsep is the extension separator (always '.')\n  - os.altsep is the alternate pathname separator (None or '/')\n  - os.pathsep is the component separator used in $PATH etc\n  - os.linesep is the line separator in text files ('\\r' or '\\n' or '\\r\\n')\n  - os.defpath is the default search path for executables\n  - os.devnull is the file path of the null device ('/dev/null', etc.)\n\nPrograms that import and use 'os' stand a better chance of being\nportable between different platforms.  Of course, they must then\nonly use functions that are defined by all platforms (e.g., unlink\nand opendir), and leave all pathname manipulation to os.path\n(e.g., split and join).\n\"\"\"\n\n#'\nimport abc\nimport sys\nimport stat as st\n\nfrom _collections_abc import _check_methods\n\n_names = sys.builtin_module_names\n\n# Note:  more names are added to __all__ later.\n__all__ = [\"altsep\", \"curdir\", \"pardir\", \"sep\", \"pathsep\", \"linesep\",\n           \"defpath\", \"name\", \"path\", \"devnull\", \"SEEK_SET\", \"SEEK_CUR\",\n           \"SEEK_END\", \"fsencode\", \"fsdecode\", \"get_exec_path\", \"fdopen\",\n           \"popen\", \"extsep\"]\n\ndef _exists(name):\n    return name in globals()\n\ndef _get_exports_list(module):\n    try:\n        return list(module.__all__)\n    except AttributeError:\n        return [n for n in dir(module) if n[0] != '_']\n\n# Any new dependencies of the os module and/or changes in path separator\n# requires updating importlib as well.\nif 'posix' in _names:\n    name = 'posix'\n    linesep = '\\n'\n    from posix import *\n    try:\n        from posix import _exit\n        __all__.append('_exit')\n    except ImportError:\n        pass\n    import posixpath as path\n\n    try:\n        from posix import _have_functions\n    except ImportError:\n        pass\n\n    import posix\n    __all__.extend(_get_exports_list(posix))\n    del posix\n\nelif 'nt' in _names:\n    name = 'nt'\n    linesep = '\\r\\n'\n    from nt import *\n    try:\n        from nt import _exit\n        __all__.append('_exit')\n    except ImportError:\n        pass\n    import ntpath as path\n\n    import nt\n    __all__.extend(_get_exports_list(nt))\n    del nt\n\n    try:\n        from nt import _have_functions\n    except ImportError:\n        pass\n\nelse:\n    raise ImportError('no os specific module found')\n\nsys.modules['os.path'] = path\nfrom os.path import (curdir, pardir, sep, pathsep, defpath, extsep, altsep,\n    devnull)\n\ndel _names\n\n\nif _exists(\"_have_functions\"):\n    _globals = globals()\n    def _add(str, fn):\n        if (fn in _globals) and (str in _have_functions):\n            _set.add(_globals[fn])\n\n    _set = set()\n    _add(\"HAVE_FACCESSAT\",  \"access\")\n    _add(\"HAVE_FCHMODAT\",   \"chmod\")\n    _add(\"HAVE_FCHOWNAT\",   \"chown\")\n    _add(\"HAVE_FSTATAT\",    \"stat\")\n    _add(\"HAVE_FUTIMESAT\",  \"utime\")\n    _add(\"HAVE_LINKAT\",     \"link\")\n    _add(\"HAVE_MKDIRAT\",    \"mkdir\")\n    _add(\"HAVE_MKFIFOAT\",   \"mkfifo\")\n    _add(\"HAVE_MKNODAT\",    \"mknod\")\n    _add(\"HAVE_OPENAT\",     \"open\")\n    _add(\"HAVE_READLINKAT\", \"readlink\")\n    _add(\"HAVE_RENAMEAT\",   \"rename\")\n    _add(\"HAVE_SYMLINKAT\",  \"symlink\")\n    _add(\"HAVE_UNLINKAT\",   \"unlink\")\n    _add(\"HAVE_UNLINKAT\",   \"rmdir\")\n    _add(\"HAVE_UTIMENSAT\",  \"utime\")\n    supports_dir_fd = _set\n\n    _set = set()\n    _add(\"HAVE_FACCESSAT\",  \"access\")\n    supports_effective_ids = _set\n\n    _set = set()\n    _add(\"HAVE_FCHDIR\",     \"chdir\")\n    _add(\"HAVE_FCHMOD\",     \"chmod\")\n    _add(\"HAVE_FCHOWN\",     \"chown\")\n    _add(\"HAVE_FDOPENDIR\",  \"listdir\")\n    _add(\"HAVE_FDOPENDIR\",  \"scandir\")\n    _add(\"HAVE_FEXECVE\",    \"execve\")\n    _set.add(stat) # fstat always works\n    _add(\"HAVE_FTRUNCATE\",  \"truncate\")\n    _add(\"HAVE_FUTIMENS\",   \"utime\")\n    _add(\"HAVE_FUTIMES\",    \"utime\")\n    _add(\"HAVE_FPATHCONF\",  \"pathconf\")\n    if _exists(\"statvfs\") and _exists(\"fstatvfs\"): # mac os x10.3\n        _add(\"HAVE_FSTATVFS\", \"statvfs\")\n    supports_fd = _set\n\n    _set = set()\n    _add(\"HAVE_FACCESSAT\",  \"access\")\n    # Some platforms don't support lchmod().  Often the function exists\n    # anyway, as a stub that always returns ENOSUP or perhaps EOPNOTSUPP.\n    # (No, I don't know why that's a good design.)  ./configure will detect\n    # this and reject it--so HAVE_LCHMOD still won't be defined on such\n    # platforms.  This is Very Helpful.\n    #\n    # However, sometimes platforms without a working lchmod() *do* have\n    # fchmodat().  (Examples: Linux kernel 3.2 with glibc 2.15,\n    # OpenIndiana 3.x.)  And fchmodat() has a flag that theoretically makes\n    # it behave like lchmod().  So in theory it would be a suitable\n    # replacement for lchmod().  But when lchmod() doesn't work, fchmodat()'s\n    # flag doesn't work *either*.  Sadly ./configure isn't sophisticated\n    # enough to detect this condition--it only determines whether or not\n    # fchmodat() minimally works.\n    #\n    # Therefore we simply ignore fchmodat() when deciding whether or not\n    # os.chmod supports follow_symlinks.  Just checking lchmod() is\n    # sufficient.  After all--if you have a working fchmodat(), your\n    # lchmod() almost certainly works too.\n    #\n    # _add(\"HAVE_FCHMODAT\",   \"chmod\")\n    _add(\"HAVE_FCHOWNAT\",   \"chown\")\n    _add(\"HAVE_FSTATAT\",    \"stat\")\n    _add(\"HAVE_LCHFLAGS\",   \"chflags\")\n    _add(\"HAVE_LCHMOD\",     \"chmod\")\n    if _exists(\"lchown\"): # mac os x10.3\n        _add(\"HAVE_LCHOWN\", \"chown\")\n    _add(\"HAVE_LINKAT\",     \"link\")\n    _add(\"HAVE_LUTIMES\",    \"utime\")\n    _add(\"HAVE_LSTAT\",      \"stat\")\n    _add(\"HAVE_FSTATAT\",    \"stat\")\n    _add(\"HAVE_UTIMENSAT\",  \"utime\")\n    _add(\"MS_WINDOWS\",      \"stat\")\n    supports_follow_symlinks = _set\n\n    del _set\n    del _have_functions\n    del _globals\n    del _add\n\n\n# Python uses fixed values for the SEEK_ constants; they are mapped\n# to native constants if necessary in posixmodule.c\n# Other possible SEEK values are directly imported from posixmodule.c\nSEEK_SET = 0\nSEEK_CUR = 1\nSEEK_END = 2\n\n# Super directory utilities.\n# (Inspired by Eric Raymond; the doc strings are mostly his)\n\ndef makedirs(name, mode=0o777, exist_ok=False):\n    \"\"\"makedirs(name [, mode=0o777][, exist_ok=False])\n\n    Super-mkdir; create a leaf directory and all intermediate ones.  Works like\n    mkdir, except that any intermediate path segment (not just the rightmost)\n    will be created if it does not exist. If the target directory already\n    exists, raise an OSError if exist_ok is False. Otherwise no exception is\n    raised.  This is recursive.\n\n    \"\"\"\n    head, tail = path.split(name)\n    if not tail:\n        head, tail = path.split(head)\n    if head and tail and not path.exists(head):\n        try:\n            makedirs(head, exist_ok=exist_ok)\n        except FileExistsError:\n            # Defeats race condition when another thread created the path\n            pass\n        cdir = curdir\n        if isinstance(tail, bytes):\n            cdir = bytes(curdir, 'ASCII')\n        if tail == cdir:           # xxx/newdir/. exists if xxx/newdir exists\n            return\n    try:\n        mkdir(name, mode)\n    except OSError:\n        # Cannot rely on checking for EEXIST, since the operating system\n        # could give priority to other errors like EACCES or EROFS\n        if not exist_ok or not path.isdir(name):\n            raise\n\ndef removedirs(name):\n    \"\"\"removedirs(name)\n\n    Super-rmdir; remove a leaf directory and all empty intermediate\n    ones.  Works like rmdir except that, if the leaf directory is\n    successfully removed, directories corresponding to rightmost path\n    segments will be pruned away until either the whole path is\n    consumed or an error occurs.  Errors during this latter phase are\n    ignored -- they generally mean that a directory was not empty.\n\n    \"\"\"\n    rmdir(name)\n    head, tail = path.split(name)\n    if not tail:\n        head, tail = path.split(head)\n    while head and tail:\n        try:\n            rmdir(head)\n        except OSError:\n            break\n        head, tail = path.split(head)\n\ndef renames(old, new):\n    \"\"\"renames(old, new)\n\n    Super-rename; create directories as necessary and delete any left\n    empty.  Works like rename, except creation of any intermediate\n    directories needed to make the new pathname good is attempted\n    first.  After the rename, directories corresponding to rightmost\n    path segments of the old name will be pruned until either the\n    whole path is consumed or a nonempty directory is found.\n\n    Note: this function can fail with the new directory structure made\n    if you lack permissions needed to unlink the leaf directory or\n    file.\n\n    \"\"\"\n    head, tail = path.split(new)\n    if head and tail and not path.exists(head):\n        makedirs(head)\n    rename(old, new)\n    head, tail = path.split(old)\n    if head and tail:\n        try:\n            removedirs(head)\n        except OSError:\n            pass\n\n__all__.extend([\"makedirs\", \"removedirs\", \"renames\"])\n\ndef walk(top, topdown=True, onerror=None, followlinks=False):\n    \"\"\"Directory tree generator.\n\n    For each directory in the directory tree rooted at top (including top\n    itself, but excluding '.' and '..'), yields a 3-tuple\n\n        dirpath, dirnames, filenames\n\n    dirpath is a string, the path to the directory.  dirnames is a list of\n    the names of the subdirectories in dirpath (excluding '.' and '..').\n    filenames is a list of the names of the non-directory files in dirpath.\n    Note that the names in the lists are just names, with no path components.\n    To get a full path (which begins with top) to a file or directory in\n    dirpath, do os.path.join(dirpath, name).\n\n    If optional arg 'topdown' is true or not specified, the triple for a\n    directory is generated before the triples for any of its subdirectories\n    (directories are generated top down).  If topdown is false, the triple\n    for a directory is generated after the triples for all of its\n    subdirectories (directories are generated bottom up).\n\n    When topdown is true, the caller can modify the dirnames list in-place\n    (e.g., via del or slice assignment), and walk will only recurse into the\n    subdirectories whose names remain in dirnames; this can be used to prune the\n    search, or to impose a specific order of visiting.  Modifying dirnames when\n    topdown is false has no effect on the behavior of os.walk(), since the\n    directories in dirnames have already been generated by the time dirnames\n    itself is generated. No matter the value of topdown, the list of\n    subdirectories is retrieved before the tuples for the directory and its\n    subdirectories are generated.\n\n    By default errors from the os.scandir() call are ignored.  If\n    optional arg 'onerror' is specified, it should be a function; it\n    will be called with one argument, an OSError instance.  It can\n    report the error to continue with the walk, or raise the exception\n    to abort the walk.  Note that the filename is available as the\n    filename attribute of the exception object.\n\n    By default, os.walk does not follow symbolic links to subdirectories on\n    systems that support them.  In order to get this functionality, set the\n    optional argument 'followlinks' to true.\n\n    Caution:  if you pass a relative pathname for top, don't change the\n    current working directory between resumptions of walk.  walk never\n    changes the current directory, and assumes that the client doesn't\n    either.\n\n    Example:\n\n    import os\n    from os.path import join, getsize\n    for root, dirs, files in os.walk('python/Lib/email'):\n        print(root, \"consumes\", end=\"\")\n        print(sum(getsize(join(root, name)) for name in files), end=\"\")\n        print(\"bytes in\", len(files), \"non-directory files\")\n        if 'CVS' in dirs:\n            dirs.remove('CVS')  # don't visit CVS directories\n\n    \"\"\"\n    top = fspath(top)\n    dirs = []\n    nondirs = []\n    walk_dirs = []\n\n    # We may not have read permission for top, in which case we can't\n    # get a list of the files the directory contains.  os.walk\n    # always suppressed the exception then, rather than blow up for a\n    # minor reason when (say) a thousand readable directories are still\n    # left to visit.  That logic is copied here.\n    try:\n        # Note that scandir is global in this module due\n        # to earlier import-*.\n        scandir_it = scandir(top)\n    except OSError as error:\n        if onerror is not None:\n            onerror(error)\n        return\n\n    with scandir_it:\n        while True:\n            try:\n                try:\n                    entry = next(scandir_it)\n                except StopIteration:\n                    break\n            except OSError as error:\n                if onerror is not None:\n                    onerror(error)\n                return\n\n            try:\n                is_dir = entry.is_dir()\n            except OSError:\n                # If is_dir() raises an OSError, consider that the entry is not\n                # a directory, same behaviour than os.path.isdir().\n                is_dir = False\n\n            if is_dir:\n                dirs.append(entry.name)\n            else:\n                nondirs.append(entry.name)\n\n            if not topdown and is_dir:\n                # Bottom-up: recurse into sub-directory, but exclude symlinks to\n                # directories if followlinks is False\n                if followlinks:\n                    walk_into = True\n                else:\n                    try:\n                        is_symlink = entry.is_symlink()\n                    except OSError:\n                        # If is_symlink() raises an OSError, consider that the\n                        # entry is not a symbolic link, same behaviour than\n                        # os.path.islink().\n                        is_symlink = False\n                    walk_into = not is_symlink\n\n                if walk_into:\n                    walk_dirs.append(entry.path)\n\n    # Yield before recursion if going top down\n    if topdown:\n        yield top, dirs, nondirs\n\n        # Recurse into sub-directories\n        islink, join = path.islink, path.join\n        for dirname in dirs:\n            new_path = join(top, dirname)\n            # Issue #23605: os.path.islink() is used instead of caching\n            # entry.is_symlink() result during the loop on os.scandir() because\n            # the caller can replace the directory entry during the \"yield\"\n            # above.\n            if followlinks or not islink(new_path):\n                yield from walk(new_path, topdown, onerror, followlinks)\n    else:\n        # Recurse into sub-directories\n        for new_path in walk_dirs:\n            yield from walk(new_path, topdown, onerror, followlinks)\n        # Yield after recursion if going bottom up\n        yield top, dirs, nondirs\n\n__all__.append(\"walk\")\n\nif {open, stat} <= supports_dir_fd and {scandir, stat} <= supports_fd:\n\n    def fwalk(top=\".\", topdown=True, onerror=None, *, follow_symlinks=False, dir_fd=None):\n        \"\"\"Directory tree generator.\n\n        This behaves exactly like walk(), except that it yields a 4-tuple\n\n            dirpath, dirnames, filenames, dirfd\n\n        `dirpath`, `dirnames` and `filenames` are identical to walk() output,\n        and `dirfd` is a file descriptor referring to the directory `dirpath`.\n\n        The advantage of fwalk() over walk() is that it's safe against symlink\n        races (when follow_symlinks is False).\n\n        If dir_fd is not None, it should be a file descriptor open to a directory,\n          and top should be relative; top will then be relative to that directory.\n          (dir_fd is always supported for fwalk.)\n\n        Caution:\n        Since fwalk() yields file descriptors, those are only valid until the\n        next iteration step, so you should dup() them if you want to keep them\n        for a longer period.\n\n        Example:\n\n        import os\n        for root, dirs, files, rootfd in os.fwalk('python/Lib/email'):\n            print(root, \"consumes\", end=\"\")\n            print(sum(os.stat(name, dir_fd=rootfd).st_size for name in files),\n                  end=\"\")\n            print(\"bytes in\", len(files), \"non-directory files\")\n            if 'CVS' in dirs:\n                dirs.remove('CVS')  # don't visit CVS directories\n        \"\"\"\n        if not isinstance(top, int) or not hasattr(top, '__index__'):\n            top = fspath(top)\n        # Note: To guard against symlink races, we use the standard\n        # lstat()/open()/fstat() trick.\n        if not follow_symlinks:\n            orig_st = stat(top, follow_symlinks=False, dir_fd=dir_fd)\n        topfd = open(top, O_RDONLY, dir_fd=dir_fd)\n        try:\n            if (follow_symlinks or (st.S_ISDIR(orig_st.st_mode) and\n                                    path.samestat(orig_st, stat(topfd)))):\n                yield from _fwalk(topfd, top, isinstance(top, bytes),\n                                  topdown, onerror, follow_symlinks)\n        finally:\n            close(topfd)\n\n    def _fwalk(topfd, toppath, isbytes, topdown, onerror, follow_symlinks):\n        # Note: This uses O(depth of the directory tree) file descriptors: if\n        # necessary, it can be adapted to only require O(1) FDs, see issue\n        # #13734.\n\n        scandir_it = scandir(topfd)\n        dirs = []\n        nondirs = []\n        entries = None if topdown or follow_symlinks else []\n        for entry in scandir_it:\n            name = entry.name\n            if isbytes:\n                name = fsencode(name)\n            try:\n                if entry.is_dir():\n                    dirs.append(name)\n                    if entries is not None:\n                        entries.append(entry)\n                else:\n                    nondirs.append(name)\n            except OSError:\n                try:\n                    # Add dangling symlinks, ignore disappeared files\n                    if entry.is_symlink():\n                        nondirs.append(name)\n                except OSError:\n                    pass\n\n        if topdown:\n            yield toppath, dirs, nondirs, topfd\n\n        for name in dirs if entries is None else zip(dirs, entries):\n            try:\n                if not follow_symlinks:\n                    if topdown:\n                        orig_st = stat(name, dir_fd=topfd, follow_symlinks=False)\n                    else:\n                        assert entries is not None\n                        name, entry = name\n                        orig_st = entry.stat(follow_symlinks=False)\n                dirfd = open(name, O_RDONLY, dir_fd=topfd)\n            except OSError as err:\n                if onerror is not None:\n                    onerror(err)\n                continue\n            try:\n                if follow_symlinks or path.samestat(orig_st, stat(dirfd)):\n                    dirpath = path.join(toppath, name)\n                    yield from _fwalk(dirfd, dirpath, isbytes,\n                                      topdown, onerror, follow_symlinks)\n            finally:\n                close(dirfd)\n\n        if not topdown:\n            yield toppath, dirs, nondirs, topfd\n\n    __all__.append(\"fwalk\")\n\ndef execl(file, *args):\n    \"\"\"execl(file, *args)\n\n    Execute the executable file with argument list args, replacing the\n    current process. \"\"\"\n    execv(file, args)\n\ndef execle(file, *args):\n    \"\"\"execle(file, *args, env)\n\n    Execute the executable file with argument list args and\n    environment env, replacing the current process. \"\"\"\n    env = args[-1]\n    execve(file, args[:-1], env)\n\ndef execlp(file, *args):\n    \"\"\"execlp(file, *args)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args, replacing the current process. \"\"\"\n    execvp(file, args)\n\ndef execlpe(file, *args):\n    \"\"\"execlpe(file, *args, env)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args and environment env, replacing the current\n    process. \"\"\"\n    env = args[-1]\n    execvpe(file, args[:-1], env)\n\ndef execvp(file, args):\n    \"\"\"execvp(file, args)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args, replacing the current process.\n    args may be a list or tuple of strings. \"\"\"\n    _execvpe(file, args)\n\ndef execvpe(file, args, env):\n    \"\"\"execvpe(file, args, env)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args and environment env, replacing the\n    current process.\n    args may be a list or tuple of strings. \"\"\"\n    _execvpe(file, args, env)\n\n__all__.extend([\"execl\",\"execle\",\"execlp\",\"execlpe\",\"execvp\",\"execvpe\"])\n\ndef _execvpe(file, args, env=None):\n    if env is not None:\n        exec_func = execve\n        argrest = (args, env)\n    else:\n        exec_func = execv\n        argrest = (args,)\n        env = environ\n\n    if path.dirname(file):\n        exec_func(file, *argrest)\n        return\n    saved_exc = None\n    path_list = get_exec_path(env)\n    if name != 'nt':\n        file = fsencode(file)\n        path_list = map(fsencode, path_list)\n    for dir in path_list:\n        fullname = path.join(dir, file)\n        try:\n            exec_func(fullname, *argrest)\n        except (FileNotFoundError, NotADirectoryError) as e:\n            last_exc = e\n        except OSError as e:\n            last_exc = e\n            if saved_exc is None:\n                saved_exc = e\n    if saved_exc is not None:\n        raise saved_exc\n    raise last_exc\n\n\ndef get_exec_path(env=None):\n    \"\"\"Returns the sequence of directories that will be searched for the\n    named executable (similar to a shell) when launching a process.\n\n    *env* must be an environment variable dict or None.  If *env* is None,\n    os.environ will be used.\n    \"\"\"\n    # Use a local import instead of a global import to limit the number of\n    # modules loaded at startup: the os module is always loaded at startup by\n    # Python. It may also avoid a bootstrap issue.\n    import warnings\n\n    if env is None:\n        env = environ\n\n    # {b'PATH': ...}.get('PATH') and {'PATH': ...}.get(b'PATH') emit a\n    # BytesWarning when using python -b or python -bb: ignore the warning\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", BytesWarning)\n\n        try:\n            path_list = env.get('PATH')\n        except TypeError:\n            path_list = None\n\n        if supports_bytes_environ:\n            try:\n                path_listb = env[b'PATH']\n            except (KeyError, TypeError):\n                pass\n            else:\n                if path_list is not None:\n                    raise ValueError(\n                        \"env cannot contain 'PATH' and b'PATH' keys\")\n                path_list = path_listb\n\n            if path_list is not None and isinstance(path_list, bytes):\n                path_list = fsdecode(path_list)\n\n    if path_list is None:\n        path_list = defpath\n    return path_list.split(pathsep)\n\n\n# Change environ to automatically call putenv(), unsetenv if they exist.\nfrom _collections_abc import MutableMapping\n\nclass _Environ(MutableMapping):\n    def __init__(self, data, encodekey, decodekey, encodevalue, decodevalue, putenv, unsetenv):\n        self.encodekey = encodekey\n        self.decodekey = decodekey\n        self.encodevalue = encodevalue\n        self.decodevalue = decodevalue\n        self.putenv = putenv\n        self.unsetenv = unsetenv\n        self._data = data\n\n    def __getitem__(self, key):\n        try:\n            value = self._data[self.encodekey(key)]\n        except KeyError:\n            # raise KeyError with the original key value\n            raise KeyError(key) from None\n        return self.decodevalue(value)\n\n    def __setitem__(self, key, value):\n        key = self.encodekey(key)\n        value = self.encodevalue(value)\n        self.putenv(key, value)\n        self._data[key] = value\n\n    def __delitem__(self, key):\n        encodedkey = self.encodekey(key)\n        self.unsetenv(encodedkey)\n        try:\n            del self._data[encodedkey]\n        except KeyError:\n            # raise KeyError with the original key value\n            raise KeyError(key) from None\n\n    def __iter__(self):\n        # list() from dict object is an atomic operation\n        keys = list(self._data)\n        for key in keys:\n            yield self.decodekey(key)\n\n    def __len__(self):\n        return len(self._data)\n\n    def __repr__(self):\n        return 'environ({{{}}})'.format(', '.join(\n            ('{!r}: {!r}'.format(self.decodekey(key), self.decodevalue(value))\n            for key, value in self._data.items())))\n\n    def copy(self):\n        return dict(self)\n\n    def setdefault(self, key, value):\n        if key not in self:\n            self[key] = value\n        return self[key]\n\ntry:\n    _putenv = putenv\nexcept NameError:\n    _putenv = lambda key, value: None\nelse:\n    if \"putenv\" not in __all__:\n        __all__.append(\"putenv\")\n\ntry:\n    _unsetenv = unsetenv\nexcept NameError:\n    _unsetenv = lambda key: _putenv(key, \"\")\nelse:\n    if \"unsetenv\" not in __all__:\n        __all__.append(\"unsetenv\")\n\ndef _createenviron():\n    if name == 'nt':\n        # Where Env Var Names Must Be UPPERCASE\n        def check_str(value):\n            if not isinstance(value, str):\n                raise TypeError(\"str expected, not %s\" % type(value).__name__)\n            return value\n        encode = check_str\n        decode = str\n        def encodekey(key):\n            return encode(key).upper()\n        data = {}\n        for key, value in environ.items():\n            data[encodekey(key)] = value\n    else:\n        # Where Env Var Names Can Be Mixed Case\n        encoding = sys.getfilesystemencoding()\n        def encode(value):\n            if not isinstance(value, str):\n                raise TypeError(\"str expected, not %s\" % type(value).__name__)\n            return value.encode(encoding, 'surrogateescape')\n        def decode(value):\n            return value.decode(encoding, 'surrogateescape')\n        encodekey = encode\n        data = environ\n    return _Environ(data,\n        encodekey, decode,\n        encode, decode,\n        _putenv, _unsetenv)\n\n# unicode environ\nenviron = _createenviron()\ndel _createenviron\n\n\ndef getenv(key, default=None):\n    \"\"\"Get an environment variable, return None if it doesn't exist.\n    The optional second argument can specify an alternate default.\n    key, default and the result are str.\"\"\"\n    return environ.get(key, default)\n\nsupports_bytes_environ = (name != 'nt')\n__all__.extend((\"getenv\", \"supports_bytes_environ\"))\n\nif supports_bytes_environ:\n    def _check_bytes(value):\n        if not isinstance(value, bytes):\n            raise TypeError(\"bytes expected, not %s\" % type(value).__name__)\n        return value\n\n    # bytes environ\n    environb = _Environ(environ._data,\n        _check_bytes, bytes,\n        _check_bytes, bytes,\n        _putenv, _unsetenv)\n    del _check_bytes\n\n    def getenvb(key, default=None):\n        \"\"\"Get an environment variable, return None if it doesn't exist.\n        The optional second argument can specify an alternate default.\n        key, default and the result are bytes.\"\"\"\n        return environb.get(key, default)\n\n    __all__.extend((\"environb\", \"getenvb\"))\n\ndef _fscodec():\n    encoding = sys.getfilesystemencoding()\n    errors = sys.getfilesystemencodeerrors()\n\n    def fsencode(filename):\n        \"\"\"Encode filename (an os.PathLike, bytes, or str) to the filesystem\n        encoding with 'surrogateescape' error handler, return bytes unchanged.\n        On Windows, use 'strict' error handler if the file system encoding is\n        'mbcs' (which is the default encoding).\n        \"\"\"\n        filename = fspath(filename)  # Does type-checking of `filename`.\n        if isinstance(filename, str):\n            return filename.encode(encoding, errors)\n        else:\n            return filename\n\n    def fsdecode(filename):\n        \"\"\"Decode filename (an os.PathLike, bytes, or str) from the filesystem\n        encoding with 'surrogateescape' error handler, return str unchanged. On\n        Windows, use 'strict' error handler if the file system encoding is\n        'mbcs' (which is the default encoding).\n        \"\"\"\n        filename = fspath(filename)  # Does type-checking of `filename`.\n        if isinstance(filename, bytes):\n            return filename.decode(encoding, errors)\n        else:\n            return filename\n\n    return fsencode, fsdecode\n\nfsencode, fsdecode = _fscodec()\ndel _fscodec\n\n# Supply spawn*() (probably only for Unix)\nif _exists(\"fork\") and not _exists(\"spawnv\") and _exists(\"execv\"):\n\n    P_WAIT = 0\n    P_NOWAIT = P_NOWAITO = 1\n\n    __all__.extend([\"P_WAIT\", \"P_NOWAIT\", \"P_NOWAITO\"])\n\n    # XXX Should we support P_DETACH?  I suppose it could fork()**2\n    # and close the std I/O streams.  Also, P_OVERLAY is the same\n    # as execv*()?\n\n    def _spawnvef(mode, file, args, env, func):\n        # Internal helper; func is the exec*() function to use\n        if not isinstance(args, (tuple, list)):\n            raise TypeError('argv must be a tuple or a list')\n        if not args or not args[0]:\n            raise ValueError('argv first element cannot be empty')\n        pid = fork()\n        if not pid:\n            # Child\n            try:\n                if env is None:\n                    func(file, args)\n                else:\n                    func(file, args, env)\n            except:\n                _exit(127)\n        else:\n            # Parent\n            if mode == P_NOWAIT:\n                return pid # Caller is responsible for waiting!\n            while 1:\n                wpid, sts = waitpid(pid, 0)\n                if WIFSTOPPED(sts):\n                    continue\n                elif WIFSIGNALED(sts):\n                    return -WTERMSIG(sts)\n                elif WIFEXITED(sts):\n                    return WEXITSTATUS(sts)\n                else:\n                    raise OSError(\"Not stopped, signaled or exited???\")\n\n    def spawnv(mode, file, args):\n        \"\"\"spawnv(mode, file, args) -> integer\n\nExecute file with arguments from args in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, None, execv)\n\n    def spawnve(mode, file, args, env):\n        \"\"\"spawnve(mode, file, args, env) -> integer\n\nExecute file with arguments from args in a subprocess with the\nspecified environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, env, execve)\n\n    # Note: spawnvp[e] isn't currently supported on Windows\n\n    def spawnvp(mode, file, args):\n        \"\"\"spawnvp(mode, file, args) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, None, execvp)\n\n    def spawnvpe(mode, file, args, env):\n        \"\"\"spawnvpe(mode, file, args, env) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, env, execvpe)\n\n\n    __all__.extend([\"spawnv\", \"spawnve\", \"spawnvp\", \"spawnvpe\"])\n\n\nif _exists(\"spawnv\"):\n    # These aren't supplied by the basic Windows code\n    # but can be easily implemented in Python\n\n    def spawnl(mode, file, *args):\n        \"\"\"spawnl(mode, file, *args) -> integer\n\nExecute file with arguments from args in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return spawnv(mode, file, args)\n\n    def spawnle(mode, file, *args):\n        \"\"\"spawnle(mode, file, *args, env) -> integer\n\nExecute file with arguments from args in a subprocess with the\nsupplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        env = args[-1]\n        return spawnve(mode, file, args[:-1], env)\n\n\n    __all__.extend([\"spawnl\", \"spawnle\"])\n\n\nif _exists(\"spawnvp\"):\n    # At the moment, Windows doesn't implement spawnvp[e],\n    # so it won't have spawnlp[e] either.\n    def spawnlp(mode, file, *args):\n        \"\"\"spawnlp(mode, file, *args) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return spawnvp(mode, file, args)\n\n    def spawnlpe(mode, file, *args):\n        \"\"\"spawnlpe(mode, file, *args, env) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        env = args[-1]\n        return spawnvpe(mode, file, args[:-1], env)\n\n\n    __all__.extend([\"spawnlp\", \"spawnlpe\"])\n\n\n# Supply os.popen()\ndef popen(cmd, mode=\"r\", buffering=-1):\n    if not isinstance(cmd, str):\n        raise TypeError(\"invalid cmd type (%s, expected string)\" % type(cmd))\n    if mode not in (\"r\", \"w\"):\n        raise ValueError(\"invalid mode %r\" % mode)\n    if buffering == 0 or buffering is None:\n        raise ValueError(\"popen() does not support unbuffered streams\")\n    import subprocess, io\n    if mode == \"r\":\n        proc = subprocess.Popen(cmd,\n                                shell=True,\n                                stdout=subprocess.PIPE,\n                                bufsize=buffering)\n        return _wrap_close(io.TextIOWrapper(proc.stdout), proc)\n    else:\n        proc = subprocess.Popen(cmd,\n                                shell=True,\n                                stdin=subprocess.PIPE,\n                                bufsize=buffering)\n        return _wrap_close(io.TextIOWrapper(proc.stdin), proc)\n\n# Helper for popen() -- a proxy for a file whose close waits for the process\nclass _wrap_close:\n    def __init__(self, stream, proc):\n        self._stream = stream\n        self._proc = proc\n    def close(self):\n        self._stream.close()\n        returncode = self._proc.wait()\n        if returncode == 0:\n            return None\n        if name == 'nt':\n            return returncode\n        else:\n            return returncode << 8  # Shift left to match old behavior\n    def __enter__(self):\n        return self\n    def __exit__(self, *args):\n        self.close()\n    def __getattr__(self, name):\n        return getattr(self._stream, name)\n    def __iter__(self):\n        return iter(self._stream)\n\n# Supply os.fdopen()\ndef fdopen(fd, *args, **kwargs):\n    if not isinstance(fd, int):\n        raise TypeError(\"invalid fd type (%s, expected integer)\" % type(fd))\n    import io\n    return io.open(fd, *args, **kwargs)\n\n\n# For testing purposes, make sure the function is available when the C\n# implementation exists.\ndef _fspath(path):\n    \"\"\"Return the path representation of a path-like object.\n\n    If str or bytes is passed in, it is returned unchanged. Otherwise the\n    os.PathLike interface is used to get the path representation. If the\n    path representation is not str or bytes, TypeError is raised. If the\n    provided path is not str, bytes, or os.PathLike, TypeError is raised.\n    \"\"\"\n    if isinstance(path, (str, bytes)):\n        return path\n\n    # Work from the object's type to match method resolution of other magic\n    # methods.\n    path_type = type(path)\n    try:\n        path_repr = path_type.__fspath__(path)\n    except AttributeError:\n        if hasattr(path_type, '__fspath__'):\n            raise\n        else:\n            raise TypeError(\"expected str, bytes or os.PathLike object, \"\n                            \"not \" + path_type.__name__)\n    if isinstance(path_repr, (str, bytes)):\n        return path_repr\n    else:\n        raise TypeError(\"expected {}.__fspath__() to return str or bytes, \"\n                        \"not {}\".format(path_type.__name__,\n                                        type(path_repr).__name__))\n\n# If there is no C implementation, make the pure Python version the\n# implementation as transparently as possible.\nif not _exists('fspath'):\n    fspath = _fspath\n    fspath.__name__ = \"fspath\"\n\n\nclass PathLike(abc.ABC):\n\n    \"\"\"Abstract base class for implementing the file system path protocol.\"\"\"\n\n    @abc.abstractmethod\n    def __fspath__(self):\n        \"\"\"Return the file system path representation of the object.\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def __subclasshook__(cls, subclass):\n        if cls is PathLike:\n            return _check_methods(subclass, '__fspath__')\n        return NotImplemented\n\n\nif name == 'nt':\n    class _AddedDllDirectory:\n        def __init__(self, path, cookie, remove_dll_directory):\n            self.path = path\n            self._cookie = cookie\n            self._remove_dll_directory = remove_dll_directory\n        def close(self):\n            self._remove_dll_directory(self._cookie)\n            self.path = None\n        def __enter__(self):\n            return self\n        def __exit__(self, *args):\n            self.close()\n        def __repr__(self):\n            if self.path:\n                return \"<AddedDllDirectory({!r})>\".format(self.path)\n            return \"<AddedDllDirectory()>\"\n\n    def add_dll_directory(path):\n        \"\"\"Add a path to the DLL search path.\n\n        This search path is used when resolving dependencies for imported\n        extension modules (the module itself is resolved through sys.path),\n        and also by ctypes.\n\n        Remove the directory by calling close() on the returned object or\n        using it in a with statement.\n        \"\"\"\n        import nt\n        cookie = nt._add_dll_directory(path)\n        return _AddedDllDirectory(\n            path,\n            cookie,\n            nt._remove_dll_directory\n        )\n", 1114], "/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_collections_abc.py": ["# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) for collections, according to PEP 3119.\n\nUnit tests are in test_collections.\n\"\"\"\n\nfrom abc import ABCMeta, abstractmethod\nimport sys\n\n__all__ = [\"Awaitable\", \"Coroutine\",\n           \"AsyncIterable\", \"AsyncIterator\", \"AsyncGenerator\",\n           \"Hashable\", \"Iterable\", \"Iterator\", \"Generator\", \"Reversible\",\n           \"Sized\", \"Container\", \"Callable\", \"Collection\",\n           \"Set\", \"MutableSet\",\n           \"Mapping\", \"MutableMapping\",\n           \"MappingView\", \"KeysView\", \"ItemsView\", \"ValuesView\",\n           \"Sequence\", \"MutableSequence\",\n           \"ByteString\",\n           ]\n\n# This module has been renamed from collections.abc to _collections_abc to\n# speed up interpreter startup. Some of the types such as MutableMapping are\n# required early but collections module imports a lot of other modules.\n# See issue #19218\n__name__ = \"collections.abc\"\n\n# Private list of types that we want to register with the various ABCs\n# so that they will pass tests like:\n#       it = iter(somebytearray)\n#       assert isinstance(it, Iterable)\n# Note:  in other implementations, these types might not be distinct\n# and they may have their own implementation specific types that\n# are not included on this list.\nbytes_iterator = type(iter(b''))\nbytearray_iterator = type(iter(bytearray()))\n#callable_iterator = ???\ndict_keyiterator = type(iter({}.keys()))\ndict_valueiterator = type(iter({}.values()))\ndict_itemiterator = type(iter({}.items()))\nlist_iterator = type(iter([]))\nlist_reverseiterator = type(iter(reversed([])))\nrange_iterator = type(iter(range(0)))\nlongrange_iterator = type(iter(range(1 << 1000)))\nset_iterator = type(iter(set()))\nstr_iterator = type(iter(\"\"))\ntuple_iterator = type(iter(()))\nzip_iterator = type(iter(zip()))\n## views ##\ndict_keys = type({}.keys())\ndict_values = type({}.values())\ndict_items = type({}.items())\n## misc ##\nmappingproxy = type(type.__dict__)\ngenerator = type((lambda: (yield))())\n## coroutine ##\nasync def _coro(): pass\n_coro = _coro()\ncoroutine = type(_coro)\n_coro.close()  # Prevent ResourceWarning\ndel _coro\n## asynchronous generator ##\nasync def _ag(): yield\n_ag = _ag()\nasync_generator = type(_ag)\ndel _ag\n\n\n### ONE-TRICK PONIES ###\n\ndef _check_methods(C, *methods):\n    mro = C.__mro__\n    for method in methods:\n        for B in mro:\n            if method in B.__dict__:\n                if B.__dict__[method] is None:\n                    return NotImplemented\n                break\n        else:\n            return NotImplemented\n    return True\n\nclass Hashable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __hash__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Hashable:\n            return _check_methods(C, \"__hash__\")\n        return NotImplemented\n\n\nclass Awaitable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __await__(self):\n        yield\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Awaitable:\n            return _check_methods(C, \"__await__\")\n        return NotImplemented\n\n\nclass Coroutine(Awaitable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def send(self, value):\n        \"\"\"Send a value into the coroutine.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        raise StopIteration\n\n    @abstractmethod\n    def throw(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the coroutine.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    def close(self):\n        \"\"\"Raise GeneratorExit inside coroutine.\n        \"\"\"\n        try:\n            self.throw(GeneratorExit)\n        except (GeneratorExit, StopIteration):\n            pass\n        else:\n            raise RuntimeError(\"coroutine ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Coroutine:\n            return _check_methods(C, '__await__', 'send', 'throw', 'close')\n        return NotImplemented\n\n\nCoroutine.register(coroutine)\n\n\nclass AsyncIterable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __aiter__(self):\n        return AsyncIterator()\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncIterable:\n            return _check_methods(C, \"__aiter__\")\n        return NotImplemented\n\n\nclass AsyncIterator(AsyncIterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    async def __anext__(self):\n        \"\"\"Return the next item or raise StopAsyncIteration when exhausted.\"\"\"\n        raise StopAsyncIteration\n\n    def __aiter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncIterator:\n            return _check_methods(C, \"__anext__\", \"__aiter__\")\n        return NotImplemented\n\n\nclass AsyncGenerator(AsyncIterator):\n\n    __slots__ = ()\n\n    async def __anext__(self):\n        \"\"\"Return the next item from the asynchronous generator.\n        When exhausted, raise StopAsyncIteration.\n        \"\"\"\n        return await self.asend(None)\n\n    @abstractmethod\n    async def asend(self, value):\n        \"\"\"Send a value into the asynchronous generator.\n        Return next yielded value or raise StopAsyncIteration.\n        \"\"\"\n        raise StopAsyncIteration\n\n    @abstractmethod\n    async def athrow(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the asynchronous generator.\n        Return next yielded value or raise StopAsyncIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    async def aclose(self):\n        \"\"\"Raise GeneratorExit inside coroutine.\n        \"\"\"\n        try:\n            await self.athrow(GeneratorExit)\n        except (GeneratorExit, StopAsyncIteration):\n            pass\n        else:\n            raise RuntimeError(\"asynchronous generator ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncGenerator:\n            return _check_methods(C, '__aiter__', '__anext__',\n                                  'asend', 'athrow', 'aclose')\n        return NotImplemented\n\n\nAsyncGenerator.register(async_generator)\n\n\nclass Iterable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __iter__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterable:\n            return _check_methods(C, \"__iter__\")\n        return NotImplemented\n\n\nclass Iterator(Iterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __next__(self):\n        'Return the next item from the iterator. When exhausted, raise StopIteration'\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterator:\n            return _check_methods(C, '__iter__', '__next__')\n        return NotImplemented\n\nIterator.register(bytes_iterator)\nIterator.register(bytearray_iterator)\n#Iterator.register(callable_iterator)\nIterator.register(dict_keyiterator)\nIterator.register(dict_valueiterator)\nIterator.register(dict_itemiterator)\nIterator.register(list_iterator)\nIterator.register(list_reverseiterator)\nIterator.register(range_iterator)\nIterator.register(longrange_iterator)\nIterator.register(set_iterator)\nIterator.register(str_iterator)\nIterator.register(tuple_iterator)\nIterator.register(zip_iterator)\n\n\nclass Reversible(Iterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __reversed__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Reversible:\n            return _check_methods(C, \"__reversed__\", \"__iter__\")\n        return NotImplemented\n\n\nclass Generator(Iterator):\n\n    __slots__ = ()\n\n    def __next__(self):\n        \"\"\"Return the next item from the generator.\n        When exhausted, raise StopIteration.\n        \"\"\"\n        return self.send(None)\n\n    @abstractmethod\n    def send(self, value):\n        \"\"\"Send a value into the generator.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        raise StopIteration\n\n    @abstractmethod\n    def throw(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the generator.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    def close(self):\n        \"\"\"Raise GeneratorExit inside generator.\n        \"\"\"\n        try:\n            self.throw(GeneratorExit)\n        except (GeneratorExit, StopIteration):\n            pass\n        else:\n            raise RuntimeError(\"generator ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Generator:\n            return _check_methods(C, '__iter__', '__next__',\n                                  'send', 'throw', 'close')\n        return NotImplemented\n\nGenerator.register(generator)\n\n\nclass Sized(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __len__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Sized:\n            return _check_methods(C, \"__len__\")\n        return NotImplemented\n\n\nclass Container(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __contains__(self, x):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Container:\n            return _check_methods(C, \"__contains__\")\n        return NotImplemented\n\nclass Collection(Sized, Iterable, Container):\n\n    __slots__ = ()\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Collection:\n            return _check_methods(C,  \"__len__\", \"__iter__\", \"__contains__\")\n        return NotImplemented\n\nclass Callable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __call__(self, *args, **kwds):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Callable:\n            return _check_methods(C, \"__call__\")\n        return NotImplemented\n\n\n### SETS ###\n\n\nclass Set(Collection):\n\n    \"\"\"A set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__ and __len__.\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), redefine __le__ and __ge__,\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __le__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) > len(other):\n            return False\n        for elem in self:\n            if elem not in other:\n                return False\n        return True\n\n    def __lt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) < len(other) and self.__le__(other)\n\n    def __gt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) > len(other) and self.__ge__(other)\n\n    def __ge__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) < len(other):\n            return False\n        for elem in other:\n            if elem not in self:\n                return False\n        return True\n\n    def __eq__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) == len(other) and self.__le__(other)\n\n    @classmethod\n    def _from_iterable(cls, it):\n        '''Construct an instance of the class from any iterable input.\n\n        Must override this method if the class constructor signature\n        does not accept an iterable for an input.\n        '''\n        return cls(it)\n\n    def __and__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        return self._from_iterable(value for value in other if value in self)\n\n    __rand__ = __and__\n\n    def isdisjoint(self, other):\n        'Return True if two sets have a null intersection.'\n        for value in other:\n            if value in self:\n                return False\n        return True\n\n    def __or__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        chain = (e for s in (self, other) for e in s)\n        return self._from_iterable(chain)\n\n    __ror__ = __or__\n\n    def __sub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in self\n                                   if value not in other)\n\n    def __rsub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in other\n                                   if value not in self)\n\n    def __xor__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return (self - other) | (other - self)\n\n    __rxor__ = __xor__\n\n    def _hash(self):\n        \"\"\"Compute the hash value of a set.\n\n        Note that we don't define __hash__: not all sets are hashable.\n        But if you define a hashable set type, its __hash__ should\n        call this function.\n\n        This must be compatible __eq__.\n\n        All sets ought to compare equal if they contain the same\n        elements, regardless of how they are implemented, and\n        regardless of the order of the elements; so there's not much\n        freedom for __eq__ or __hash__.  We match the algorithm used\n        by the built-in frozenset type.\n        \"\"\"\n        MAX = sys.maxsize\n        MASK = 2 * MAX + 1\n        n = len(self)\n        h = 1927868237 * (n + 1)\n        h &= MASK\n        for x in self:\n            hx = hash(x)\n            h ^= (hx ^ (hx << 16) ^ 89869747)  * 3644798167\n            h &= MASK\n        h = h * 69069 + 907133923\n        h &= MASK\n        if h > MAX:\n            h -= MASK + 1\n        if h == -1:\n            h = 590923713\n        return h\n\nSet.register(frozenset)\n\n\nclass MutableSet(Set):\n    \"\"\"A mutable set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__, __len__,\n    add(), and discard().\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), all you have to do is redefine __le__ and\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def add(self, value):\n        \"\"\"Add an element.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def discard(self, value):\n        \"\"\"Remove an element.  Do not raise an exception if absent.\"\"\"\n        raise NotImplementedError\n\n    def remove(self, value):\n        \"\"\"Remove an element. If not a member, raise a KeyError.\"\"\"\n        if value not in self:\n            raise KeyError(value)\n        self.discard(value)\n\n    def pop(self):\n        \"\"\"Return the popped value.  Raise KeyError if empty.\"\"\"\n        it = iter(self)\n        try:\n            value = next(it)\n        except StopIteration:\n            raise KeyError from None\n        self.discard(value)\n        return value\n\n    def clear(self):\n        \"\"\"This is slow (creates N new iterators!) but effective.\"\"\"\n        try:\n            while True:\n                self.pop()\n        except KeyError:\n            pass\n\n    def __ior__(self, it):\n        for value in it:\n            self.add(value)\n        return self\n\n    def __iand__(self, it):\n        for value in (self - it):\n            self.discard(value)\n        return self\n\n    def __ixor__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            if not isinstance(it, Set):\n                it = self._from_iterable(it)\n            for value in it:\n                if value in self:\n                    self.discard(value)\n                else:\n                    self.add(value)\n        return self\n\n    def __isub__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            for value in it:\n                self.discard(value)\n        return self\n\nMutableSet.register(set)\n\n\n### MAPPINGS ###\n\n\nclass Mapping(Collection):\n\n    __slots__ = ()\n\n    \"\"\"A Mapping is a generic container for associating key/value\n    pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __iter__, and __len__.\n\n    \"\"\"\n\n    @abstractmethod\n    def __getitem__(self, key):\n        raise KeyError\n\n    def get(self, key, default=None):\n        'D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.'\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __contains__(self, key):\n        try:\n            self[key]\n        except KeyError:\n            return False\n        else:\n            return True\n\n    def keys(self):\n        \"D.keys() -> a set-like object providing a view on D's keys\"\n        return KeysView(self)\n\n    def items(self):\n        \"D.items() -> a set-like object providing a view on D's items\"\n        return ItemsView(self)\n\n    def values(self):\n        \"D.values() -> an object providing a view on D's values\"\n        return ValuesView(self)\n\n    def __eq__(self, other):\n        if not isinstance(other, Mapping):\n            return NotImplemented\n        return dict(self.items()) == dict(other.items())\n\n    __reversed__ = None\n\nMapping.register(mappingproxy)\n\n\nclass MappingView(Sized):\n\n    __slots__ = '_mapping',\n\n    def __init__(self, mapping):\n        self._mapping = mapping\n\n    def __len__(self):\n        return len(self._mapping)\n\n    def __repr__(self):\n        return '{0.__class__.__name__}({0._mapping!r})'.format(self)\n\n\nclass KeysView(MappingView, Set):\n\n    __slots__ = ()\n\n    @classmethod\n    def _from_iterable(self, it):\n        return set(it)\n\n    def __contains__(self, key):\n        return key in self._mapping\n\n    def __iter__(self):\n        yield from self._mapping\n\nKeysView.register(dict_keys)\n\n\nclass ItemsView(MappingView, Set):\n\n    __slots__ = ()\n\n    @classmethod\n    def _from_iterable(self, it):\n        return set(it)\n\n    def __contains__(self, item):\n        key, value = item\n        try:\n            v = self._mapping[key]\n        except KeyError:\n            return False\n        else:\n            return v is value or v == value\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield (key, self._mapping[key])\n\nItemsView.register(dict_items)\n\n\nclass ValuesView(MappingView, Collection):\n\n    __slots__ = ()\n\n    def __contains__(self, value):\n        for key in self._mapping:\n            v = self._mapping[key]\n            if v is value or v == value:\n                return True\n        return False\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield self._mapping[key]\n\nValuesView.register(dict_values)\n\n\nclass MutableMapping(Mapping):\n\n    __slots__ = ()\n\n    \"\"\"A MutableMapping is a generic container for associating\n    key/value pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __setitem__, __delitem__,\n    __iter__, and __len__.\n\n    \"\"\"\n\n    @abstractmethod\n    def __setitem__(self, key, value):\n        raise KeyError\n\n    @abstractmethod\n    def __delitem__(self, key):\n        raise KeyError\n\n    __marker = object()\n\n    def pop(self, key, default=__marker):\n        '''D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n          If key is not found, d is returned if given, otherwise KeyError is raised.\n        '''\n        try:\n            value = self[key]\n        except KeyError:\n            if default is self.__marker:\n                raise\n            return default\n        else:\n            del self[key]\n            return value\n\n    def popitem(self):\n        '''D.popitem() -> (k, v), remove and return some (key, value) pair\n           as a 2-tuple; but raise KeyError if D is empty.\n        '''\n        try:\n            key = next(iter(self))\n        except StopIteration:\n            raise KeyError from None\n        value = self[key]\n        del self[key]\n        return key, value\n\n    def clear(self):\n        'D.clear() -> None.  Remove all items from D.'\n        try:\n            while True:\n                self.popitem()\n        except KeyError:\n            pass\n\n    def update(self, other=(), /, **kwds):\n        ''' D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n            If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n            If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n            In either case, this is followed by: for k, v in F.items(): D[k] = v\n        '''\n        if isinstance(other, Mapping):\n            for key in other:\n                self[key] = other[key]\n        elif hasattr(other, \"keys\"):\n            for key in other.keys():\n                self[key] = other[key]\n        else:\n            for key, value in other:\n                self[key] = value\n        for key, value in kwds.items():\n            self[key] = value\n\n    def setdefault(self, key, default=None):\n        'D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D'\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n        return default\n\nMutableMapping.register(dict)\n\n\n### SEQUENCES ###\n\n\nclass Sequence(Reversible, Collection):\n\n    \"\"\"All the operations on a read-only sequence.\n\n    Concrete subclasses must override __new__ or __init__,\n    __getitem__, and __len__.\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __getitem__(self, index):\n        raise IndexError\n\n    def __iter__(self):\n        i = 0\n        try:\n            while True:\n                v = self[i]\n                yield v\n                i += 1\n        except IndexError:\n            return\n\n    def __contains__(self, value):\n        for v in self:\n            if v is value or v == value:\n                return True\n        return False\n\n    def __reversed__(self):\n        for i in reversed(range(len(self))):\n            yield self[i]\n\n    def index(self, value, start=0, stop=None):\n        '''S.index(value, [start, [stop]]) -> integer -- return first index of value.\n           Raises ValueError if the value is not present.\n\n           Supporting start and stop arguments is optional, but\n           recommended.\n        '''\n        if start is not None and start < 0:\n            start = max(len(self) + start, 0)\n        if stop is not None and stop < 0:\n            stop += len(self)\n\n        i = start\n        while stop is None or i < stop:\n            try:\n                v = self[i]\n                if v is value or v == value:\n                    return i\n            except IndexError:\n                break\n            i += 1\n        raise ValueError\n\n    def count(self, value):\n        'S.count(value) -> integer -- return number of occurrences of value'\n        return sum(1 for v in self if v is value or v == value)\n\nSequence.register(tuple)\nSequence.register(str)\nSequence.register(range)\nSequence.register(memoryview)\n\n\nclass ByteString(Sequence):\n\n    \"\"\"This unifies bytes and bytearray.\n\n    XXX Should add all their methods.\n    \"\"\"\n\n    __slots__ = ()\n\nByteString.register(bytes)\nByteString.register(bytearray)\n\n\nclass MutableSequence(Sequence):\n\n    __slots__ = ()\n\n    \"\"\"All the operations on a read-write sequence.\n\n    Concrete subclasses must provide __new__ or __init__,\n    __getitem__, __setitem__, __delitem__, __len__, and insert().\n\n    \"\"\"\n\n    @abstractmethod\n    def __setitem__(self, index, value):\n        raise IndexError\n\n    @abstractmethod\n    def __delitem__(self, index):\n        raise IndexError\n\n    @abstractmethod\n    def insert(self, index, value):\n        'S.insert(index, value) -- insert value before index'\n        raise IndexError\n\n    def append(self, value):\n        'S.append(value) -- append value to the end of the sequence'\n        self.insert(len(self), value)\n\n    def clear(self):\n        'S.clear() -> None -- remove all items from S'\n        try:\n            while True:\n                self.pop()\n        except IndexError:\n            pass\n\n    def reverse(self):\n        'S.reverse() -- reverse *IN PLACE*'\n        n = len(self)\n        for i in range(n//2):\n            self[i], self[n-i-1] = self[n-i-1], self[i]\n\n    def extend(self, values):\n        'S.extend(iterable) -- extend sequence by appending elements from the iterable'\n        if values is self:\n            values = list(values)\n        for v in values:\n            self.append(v)\n\n    def pop(self, index=-1):\n        '''S.pop([index]) -> item -- remove and return item at index (default last).\n           Raise IndexError if list is empty or index is out of range.\n        '''\n        v = self[index]\n        del self[index]\n        return v\n\n    def remove(self, value):\n        '''S.remove(value) -- remove first occurrence of value.\n           Raise ValueError if the value is not present.\n        '''\n        del self[self.index(value)]\n\n    def __iadd__(self, values):\n        self.extend(values)\n        return self\n\nMutableSequence.register(list)\nMutableSequence.register(bytearray)  # Multiply inheriting, see ByteString\n", 1004], "/usr/lib/python3.8/asyncio/base_events.py": ["\"\"\"Base implementation of event loop.\n\nThe event loop can be broken up into a multiplexer (the part\nresponsible for notifying us of I/O events) and the event loop proper,\nwhich wraps a multiplexer with functionality for scheduling callbacks,\nimmediately or at a given time in the future.\n\nWhenever a public API takes a callback, subsequent positional\narguments will be passed to the callback if/when it is called.  This\navoids the proliferation of trivial lambdas implementing closures.\nKeyword arguments for the callback are not supported; this is a\nconscious design decision, leaving the door open for keyword arguments\nto modify the meaning of the API call itself.\n\"\"\"\n\nimport collections\nimport collections.abc\nimport concurrent.futures\nimport functools\nimport heapq\nimport itertools\nimport os\nimport socket\nimport stat\nimport subprocess\nimport threading\nimport time\nimport traceback\nimport sys\nimport warnings\nimport weakref\n\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nfrom . import constants\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom . import protocols\nfrom . import sslproto\nfrom . import staggered\nfrom . import tasks\nfrom . import transports\nfrom . import trsock\nfrom .log import logger\n\n\n__all__ = 'BaseEventLoop',\n\n\n# Minimum number of _scheduled timer handles before cleanup of\n# cancelled handles is performed.\n_MIN_SCHEDULED_TIMER_HANDLES = 100\n\n# Minimum fraction of _scheduled timer handles that are cancelled\n# before cleanup of cancelled handles is performed.\n_MIN_CANCELLED_TIMER_HANDLES_FRACTION = 0.5\n\n\n_HAS_IPv6 = hasattr(socket, 'AF_INET6')\n\n# Maximum timeout passed to select to avoid OS limitations\nMAXIMUM_SELECT_TIMEOUT = 24 * 3600\n\n# Used for deprecation and removal of `loop.create_datagram_endpoint()`'s\n# *reuse_address* parameter\n_unset = object()\n\n\ndef _format_handle(handle):\n    cb = handle._callback\n    if isinstance(getattr(cb, '__self__', None), tasks.Task):\n        # format the task\n        return repr(cb.__self__)\n    else:\n        return str(handle)\n\n\ndef _format_pipe(fd):\n    if fd == subprocess.PIPE:\n        return '<pipe>'\n    elif fd == subprocess.STDOUT:\n        return '<stdout>'\n    else:\n        return repr(fd)\n\n\ndef _set_reuseport(sock):\n    if not hasattr(socket, 'SO_REUSEPORT'):\n        raise ValueError('reuse_port not supported by socket module')\n    else:\n        try:\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n        except OSError:\n            raise ValueError('reuse_port not supported by socket module, '\n                             'SO_REUSEPORT defined but not implemented.')\n\n\ndef _ipaddr_info(host, port, family, type, proto, flowinfo=0, scopeid=0):\n    # Try to skip getaddrinfo if \"host\" is already an IP. Users might have\n    # handled name resolution in their own code and pass in resolved IPs.\n    if not hasattr(socket, 'inet_pton'):\n        return\n\n    if proto not in {0, socket.IPPROTO_TCP, socket.IPPROTO_UDP} or \\\n            host is None:\n        return None\n\n    if type == socket.SOCK_STREAM:\n        proto = socket.IPPROTO_TCP\n    elif type == socket.SOCK_DGRAM:\n        proto = socket.IPPROTO_UDP\n    else:\n        return None\n\n    if port is None:\n        port = 0\n    elif isinstance(port, bytes) and port == b'':\n        port = 0\n    elif isinstance(port, str) and port == '':\n        port = 0\n    else:\n        # If port's a service name like \"http\", don't skip getaddrinfo.\n        try:\n            port = int(port)\n        except (TypeError, ValueError):\n            return None\n\n    if family == socket.AF_UNSPEC:\n        afs = [socket.AF_INET]\n        if _HAS_IPv6:\n            afs.append(socket.AF_INET6)\n    else:\n        afs = [family]\n\n    if isinstance(host, bytes):\n        host = host.decode('idna')\n    if '%' in host:\n        # Linux's inet_pton doesn't accept an IPv6 zone index after host,\n        # like '::1%lo0'.\n        return None\n\n    for af in afs:\n        try:\n            socket.inet_pton(af, host)\n            # The host has already been resolved.\n            if _HAS_IPv6 and af == socket.AF_INET6:\n                return af, type, proto, '', (host, port, flowinfo, scopeid)\n            else:\n                return af, type, proto, '', (host, port)\n        except OSError:\n            pass\n\n    # \"host\" is not an IP address.\n    return None\n\n\ndef _interleave_addrinfos(addrinfos, first_address_family_count=1):\n    \"\"\"Interleave list of addrinfo tuples by family.\"\"\"\n    # Group addresses by family\n    addrinfos_by_family = collections.OrderedDict()\n    for addr in addrinfos:\n        family = addr[0]\n        if family not in addrinfos_by_family:\n            addrinfos_by_family[family] = []\n        addrinfos_by_family[family].append(addr)\n    addrinfos_lists = list(addrinfos_by_family.values())\n\n    reordered = []\n    if first_address_family_count > 1:\n        reordered.extend(addrinfos_lists[0][:first_address_family_count - 1])\n        del addrinfos_lists[0][:first_address_family_count - 1]\n    reordered.extend(\n        a for a in itertools.chain.from_iterable(\n            itertools.zip_longest(*addrinfos_lists)\n        ) if a is not None)\n    return reordered\n\n\ndef _run_until_complete_cb(fut):\n    if not fut.cancelled():\n        exc = fut.exception()\n        if isinstance(exc, (SystemExit, KeyboardInterrupt)):\n            # Issue #22429: run_forever() already finished, no need to\n            # stop it.\n            return\n    futures._get_loop(fut).stop()\n\n\nif hasattr(socket, 'TCP_NODELAY'):\n    def _set_nodelay(sock):\n        if (sock.family in {socket.AF_INET, socket.AF_INET6} and\n                sock.type == socket.SOCK_STREAM and\n                sock.proto == socket.IPPROTO_TCP):\n            sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\nelse:\n    def _set_nodelay(sock):\n        pass\n\n\nclass _SendfileFallbackProtocol(protocols.Protocol):\n    def __init__(self, transp):\n        if not isinstance(transp, transports._FlowControlMixin):\n            raise TypeError(\"transport should be _FlowControlMixin instance\")\n        self._transport = transp\n        self._proto = transp.get_protocol()\n        self._should_resume_reading = transp.is_reading()\n        self._should_resume_writing = transp._protocol_paused\n        transp.pause_reading()\n        transp.set_protocol(self)\n        if self._should_resume_writing:\n            self._write_ready_fut = self._transport._loop.create_future()\n        else:\n            self._write_ready_fut = None\n\n    async def drain(self):\n        if self._transport.is_closing():\n            raise ConnectionError(\"Connection closed by peer\")\n        fut = self._write_ready_fut\n        if fut is None:\n            return\n        await fut\n\n    def connection_made(self, transport):\n        raise RuntimeError(\"Invalid state: \"\n                           \"connection should have been established already.\")\n\n    def connection_lost(self, exc):\n        if self._write_ready_fut is not None:\n            # Never happens if peer disconnects after sending the whole content\n            # Thus disconnection is always an exception from user perspective\n            if exc is None:\n                self._write_ready_fut.set_exception(\n                    ConnectionError(\"Connection is closed by peer\"))\n            else:\n                self._write_ready_fut.set_exception(exc)\n        self._proto.connection_lost(exc)\n\n    def pause_writing(self):\n        if self._write_ready_fut is not None:\n            return\n        self._write_ready_fut = self._transport._loop.create_future()\n\n    def resume_writing(self):\n        if self._write_ready_fut is None:\n            return\n        self._write_ready_fut.set_result(False)\n        self._write_ready_fut = None\n\n    def data_received(self, data):\n        raise RuntimeError(\"Invalid state: reading should be paused\")\n\n    def eof_received(self):\n        raise RuntimeError(\"Invalid state: reading should be paused\")\n\n    async def restore(self):\n        self._transport.set_protocol(self._proto)\n        if self._should_resume_reading:\n            self._transport.resume_reading()\n        if self._write_ready_fut is not None:\n            # Cancel the future.\n            # Basically it has no effect because protocol is switched back,\n            # no code should wait for it anymore.\n            self._write_ready_fut.cancel()\n        if self._should_resume_writing:\n            self._proto.resume_writing()\n\n\nclass Server(events.AbstractServer):\n\n    def __init__(self, loop, sockets, protocol_factory, ssl_context, backlog,\n                 ssl_handshake_timeout):\n        self._loop = loop\n        self._sockets = sockets\n        self._active_count = 0\n        self._waiters = []\n        self._protocol_factory = protocol_factory\n        self._backlog = backlog\n        self._ssl_context = ssl_context\n        self._ssl_handshake_timeout = ssl_handshake_timeout\n        self._serving = False\n        self._serving_forever_fut = None\n\n    def __repr__(self):\n        return f'<{self.__class__.__name__} sockets={self.sockets!r}>'\n\n    def _attach(self):\n        assert self._sockets is not None\n        self._active_count += 1\n\n    def _detach(self):\n        assert self._active_count > 0\n        self._active_count -= 1\n        if self._active_count == 0 and self._sockets is None:\n            self._wakeup()\n\n    def _wakeup(self):\n        waiters = self._waiters\n        self._waiters = None\n        for waiter in waiters:\n            if not waiter.done():\n                waiter.set_result(waiter)\n\n    def _start_serving(self):\n        if self._serving:\n            return\n        self._serving = True\n        for sock in self._sockets:\n            sock.listen(self._backlog)\n            self._loop._start_serving(\n                self._protocol_factory, sock, self._ssl_context,\n                self, self._backlog, self._ssl_handshake_timeout)\n\n    def get_loop(self):\n        return self._loop\n\n    def is_serving(self):\n        return self._serving\n\n    @property\n    def sockets(self):\n        if self._sockets is None:\n            return ()\n        return tuple(trsock.TransportSocket(s) for s in self._sockets)\n\n    def close(self):\n        sockets = self._sockets\n        if sockets is None:\n            return\n        self._sockets = None\n\n        for sock in sockets:\n            self._loop._stop_serving(sock)\n\n        self._serving = False\n\n        if (self._serving_forever_fut is not None and\n                not self._serving_forever_fut.done()):\n            self._serving_forever_fut.cancel()\n            self._serving_forever_fut = None\n\n        if self._active_count == 0:\n            self._wakeup()\n\n    async def start_serving(self):\n        self._start_serving()\n        # Skip one loop iteration so that all 'loop.add_reader'\n        # go through.\n        await tasks.sleep(0, loop=self._loop)\n\n    async def serve_forever(self):\n        if self._serving_forever_fut is not None:\n            raise RuntimeError(\n                f'server {self!r} is already being awaited on serve_forever()')\n        if self._sockets is None:\n            raise RuntimeError(f'server {self!r} is closed')\n\n        self._start_serving()\n        self._serving_forever_fut = self._loop.create_future()\n\n        try:\n            await self._serving_forever_fut\n        except exceptions.CancelledError:\n            try:\n                self.close()\n                await self.wait_closed()\n            finally:\n                raise\n        finally:\n            self._serving_forever_fut = None\n\n    async def wait_closed(self):\n        if self._sockets is None or self._waiters is None:\n            return\n        waiter = self._loop.create_future()\n        self._waiters.append(waiter)\n        await waiter\n\n\nclass BaseEventLoop(events.AbstractEventLoop):\n\n    def __init__(self):\n        self._timer_cancelled_count = 0\n        self._closed = False\n        self._stopping = False\n        self._ready = collections.deque()\n        self._scheduled = []\n        self._default_executor = None\n        self._internal_fds = 0\n        # Identifier of the thread running the event loop, or None if the\n        # event loop is not running\n        self._thread_id = None\n        self._clock_resolution = time.get_clock_info('monotonic').resolution\n        self._exception_handler = None\n        self.set_debug(coroutines._is_debug_mode())\n        # In debug mode, if the execution of a callback or a step of a task\n        # exceed this duration in seconds, the slow callback/task is logged.\n        self.slow_callback_duration = 0.1\n        self._current_handle = None\n        self._task_factory = None\n        self._coroutine_origin_tracking_enabled = False\n        self._coroutine_origin_tracking_saved_depth = None\n\n        # A weak set of all asynchronous generators that are\n        # being iterated by the loop.\n        self._asyncgens = weakref.WeakSet()\n        # Set to True when `loop.shutdown_asyncgens` is called.\n        self._asyncgens_shutdown_called = False\n\n    def __repr__(self):\n        return (\n            f'<{self.__class__.__name__} running={self.is_running()} '\n            f'closed={self.is_closed()} debug={self.get_debug()}>'\n        )\n\n    def create_future(self):\n        \"\"\"Create a Future object attached to the loop.\"\"\"\n        return futures.Future(loop=self)\n\n    def create_task(self, coro, *, name=None):\n        \"\"\"Schedule a coroutine object.\n\n        Return a task object.\n        \"\"\"\n        self._check_closed()\n        if self._task_factory is None:\n            task = tasks.Task(coro, loop=self, name=name)\n            if task._source_traceback:\n                del task._source_traceback[-1]\n        else:\n            task = self._task_factory(self, coro)\n            tasks._set_task_name(task, name)\n\n        return task\n\n    def set_task_factory(self, factory):\n        \"\"\"Set a task factory that will be used by loop.create_task().\n\n        If factory is None the default task factory will be set.\n\n        If factory is a callable, it should have a signature matching\n        '(loop, coro)', where 'loop' will be a reference to the active\n        event loop, 'coro' will be a coroutine object.  The callable\n        must return a Future.\n        \"\"\"\n        if factory is not None and not callable(factory):\n            raise TypeError('task factory must be a callable or None')\n        self._task_factory = factory\n\n    def get_task_factory(self):\n        \"\"\"Return a task factory, or None if the default one is in use.\"\"\"\n        return self._task_factory\n\n    def _make_socket_transport(self, sock, protocol, waiter=None, *,\n                               extra=None, server=None):\n        \"\"\"Create socket transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_ssl_transport(\n            self, rawsock, protocol, sslcontext, waiter=None,\n            *, server_side=False, server_hostname=None,\n            extra=None, server=None,\n            ssl_handshake_timeout=None,\n            call_connection_made=True):\n        \"\"\"Create SSL transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_datagram_transport(self, sock, protocol,\n                                 address=None, waiter=None, extra=None):\n        \"\"\"Create datagram transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_read_pipe_transport(self, pipe, protocol, waiter=None,\n                                  extra=None):\n        \"\"\"Create read pipe transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_write_pipe_transport(self, pipe, protocol, waiter=None,\n                                   extra=None):\n        \"\"\"Create write pipe transport.\"\"\"\n        raise NotImplementedError\n\n    async def _make_subprocess_transport(self, protocol, args, shell,\n                                         stdin, stdout, stderr, bufsize,\n                                         extra=None, **kwargs):\n        \"\"\"Create subprocess transport.\"\"\"\n        raise NotImplementedError\n\n    def _write_to_self(self):\n        \"\"\"Write a byte to self-pipe, to wake up the event loop.\n\n        This may be called from a different thread.\n\n        The subclass is responsible for implementing the self-pipe.\n        \"\"\"\n        raise NotImplementedError\n\n    def _process_events(self, event_list):\n        \"\"\"Process selector events.\"\"\"\n        raise NotImplementedError\n\n    def _check_closed(self):\n        if self._closed:\n            raise RuntimeError('Event loop is closed')\n\n    def _asyncgen_finalizer_hook(self, agen):\n        self._asyncgens.discard(agen)\n        if not self.is_closed():\n            self.call_soon_threadsafe(self.create_task, agen.aclose())\n\n    def _asyncgen_firstiter_hook(self, agen):\n        if self._asyncgens_shutdown_called:\n            warnings.warn(\n                f\"asynchronous generator {agen!r} was scheduled after \"\n                f\"loop.shutdown_asyncgens() call\",\n                ResourceWarning, source=self)\n\n        self._asyncgens.add(agen)\n\n    async def shutdown_asyncgens(self):\n        \"\"\"Shutdown all active asynchronous generators.\"\"\"\n        self._asyncgens_shutdown_called = True\n\n        if not len(self._asyncgens):\n            # If Python version is <3.6 or we don't have any asynchronous\n            # generators alive.\n            return\n\n        closing_agens = list(self._asyncgens)\n        self._asyncgens.clear()\n\n        results = await tasks.gather(\n            *[ag.aclose() for ag in closing_agens],\n            return_exceptions=True,\n            loop=self)\n\n        for result, agen in zip(results, closing_agens):\n            if isinstance(result, Exception):\n                self.call_exception_handler({\n                    'message': f'an error occurred during closing of '\n                               f'asynchronous generator {agen!r}',\n                    'exception': result,\n                    'asyncgen': agen\n                })\n\n    def _check_running(self):\n        if self.is_running():\n            raise RuntimeError('This event loop is already running')\n        if events._get_running_loop() is not None:\n            raise RuntimeError(\n                'Cannot run the event loop while another loop is running')\n\n    def run_forever(self):\n        \"\"\"Run until stop() is called.\"\"\"\n        self._check_closed()\n        self._check_running()\n        self._set_coroutine_origin_tracking(self._debug)\n        self._thread_id = threading.get_ident()\n\n        old_agen_hooks = sys.get_asyncgen_hooks()\n        sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n                               finalizer=self._asyncgen_finalizer_hook)\n        try:\n            events._set_running_loop(self)\n            while True:\n                self._run_once()\n                if self._stopping:\n                    break\n        finally:\n            self._stopping = False\n            self._thread_id = None\n            events._set_running_loop(None)\n            self._set_coroutine_origin_tracking(False)\n            sys.set_asyncgen_hooks(*old_agen_hooks)\n\n    def run_until_complete(self, future):\n        \"\"\"Run until the Future is done.\n\n        If the argument is a coroutine, it is wrapped in a Task.\n\n        WARNING: It would be disastrous to call run_until_complete()\n        with the same coroutine twice -- it would wrap it in two\n        different Tasks and that can't be good.\n\n        Return the Future's result, or raise its exception.\n        \"\"\"\n        self._check_closed()\n        self._check_running()\n\n        new_task = not futures.isfuture(future)\n        future = tasks.ensure_future(future, loop=self)\n        if new_task:\n            # An exception is raised if the future didn't complete, so there\n            # is no need to log the \"destroy pending task\" message\n            future._log_destroy_pending = False\n\n        future.add_done_callback(_run_until_complete_cb)\n        try:\n            self.run_forever()\n        except:\n            if new_task and future.done() and not future.cancelled():\n                # The coroutine raised a BaseException. Consume the exception\n                # to not log a warning, the caller doesn't have access to the\n                # local task.\n                future.exception()\n            raise\n        finally:\n            future.remove_done_callback(_run_until_complete_cb)\n        if not future.done():\n            raise RuntimeError('Event loop stopped before Future completed.')\n\n        return future.result()\n\n    def stop(self):\n        \"\"\"Stop running the event loop.\n\n        Every callback already scheduled will still run.  This simply informs\n        run_forever to stop looping after a complete iteration.\n        \"\"\"\n        self._stopping = True\n\n    def close(self):\n        \"\"\"Close the event loop.\n\n        This clears the queues and shuts down the executor,\n        but does not wait for the executor to finish.\n\n        The event loop must not be running.\n        \"\"\"\n        if self.is_running():\n            raise RuntimeError(\"Cannot close a running event loop\")\n        if self._closed:\n            return\n        if self._debug:\n            logger.debug(\"Close %r\", self)\n        self._closed = True\n        self._ready.clear()\n        self._scheduled.clear()\n        executor = self._default_executor\n        if executor is not None:\n            self._default_executor = None\n            executor.shutdown(wait=False)\n\n    def is_closed(self):\n        \"\"\"Returns True if the event loop was closed.\"\"\"\n        return self._closed\n\n    def __del__(self, _warn=warnings.warn):\n        if not self.is_closed():\n            _warn(f\"unclosed event loop {self!r}\", ResourceWarning, source=self)\n            if not self.is_running():\n                self.close()\n\n    def is_running(self):\n        \"\"\"Returns True if the event loop is running.\"\"\"\n        return (self._thread_id is not None)\n\n    def time(self):\n        \"\"\"Return the time according to the event loop's clock.\n\n        This is a float expressed in seconds since an epoch, but the\n        epoch, precision, accuracy and drift are unspecified and may\n        differ per event loop.\n        \"\"\"\n        return time.monotonic()\n\n    def call_later(self, delay, callback, *args, context=None):\n        \"\"\"Arrange for a callback to be called at a given time.\n\n        Return a Handle: an opaque object with a cancel() method that\n        can be used to cancel the call.\n\n        The delay can be an int or float, expressed in seconds.  It is\n        always relative to the current time.\n\n        Each callback will be called exactly once.  If two callbacks\n        are scheduled for exactly the same time, it undefined which\n        will be called first.\n\n        Any positional arguments after the callback will be passed to\n        the callback when it is called.\n        \"\"\"\n        timer = self.call_at(self.time() + delay, callback, *args,\n                             context=context)\n        if timer._source_traceback:\n            del timer._source_traceback[-1]\n        return timer\n\n    def call_at(self, when, callback, *args, context=None):\n        \"\"\"Like call_later(), but uses an absolute time.\n\n        Absolute time corresponds to the event loop's time() method.\n        \"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_thread()\n            self._check_callback(callback, 'call_at')\n        timer = events.TimerHandle(when, callback, args, self, context)\n        if timer._source_traceback:\n            del timer._source_traceback[-1]\n        heapq.heappush(self._scheduled, timer)\n        timer._scheduled = True\n        return timer\n\n    def call_soon(self, callback, *args, context=None):\n        \"\"\"Arrange for a callback to be called as soon as possible.\n\n        This operates as a FIFO queue: callbacks are called in the\n        order in which they are registered.  Each callback will be\n        called exactly once.\n\n        Any positional arguments after the callback will be passed to\n        the callback when it is called.\n        \"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_thread()\n            self._check_callback(callback, 'call_soon')\n        handle = self._call_soon(callback, args, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        return handle\n\n    def _check_callback(self, callback, method):\n        if (coroutines.iscoroutine(callback) or\n                coroutines.iscoroutinefunction(callback)):\n            raise TypeError(\n                f\"coroutines cannot be used with {method}()\")\n        if not callable(callback):\n            raise TypeError(\n                f'a callable object was expected by {method}(), '\n                f'got {callback!r}')\n\n    def _call_soon(self, callback, args, context):\n        handle = events.Handle(callback, args, self, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        self._ready.append(handle)\n        return handle\n\n    def _check_thread(self):\n        \"\"\"Check that the current thread is the thread running the event loop.\n\n        Non-thread-safe methods of this class make this assumption and will\n        likely behave incorrectly when the assumption is violated.\n\n        Should only be called when (self._debug == True).  The caller is\n        responsible for checking this condition for performance reasons.\n        \"\"\"\n        if self._thread_id is None:\n            return\n        thread_id = threading.get_ident()\n        if thread_id != self._thread_id:\n            raise RuntimeError(\n                \"Non-thread-safe operation invoked on an event loop other \"\n                \"than the current one\")\n\n    def call_soon_threadsafe(self, callback, *args, context=None):\n        \"\"\"Like call_soon(), but thread-safe.\"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_callback(callback, 'call_soon_threadsafe')\n        handle = self._call_soon(callback, args, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        self._write_to_self()\n        return handle\n\n    def run_in_executor(self, executor, func, *args):\n        self._check_closed()\n        if self._debug:\n            self._check_callback(func, 'run_in_executor')\n        if executor is None:\n            executor = self._default_executor\n            if executor is None:\n                executor = concurrent.futures.ThreadPoolExecutor()\n                self._default_executor = executor\n        return futures.wrap_future(\n            executor.submit(func, *args), loop=self)\n\n    def set_default_executor(self, executor):\n        if not isinstance(executor, concurrent.futures.ThreadPoolExecutor):\n            warnings.warn(\n                'Using the default executor that is not an instance of '\n                'ThreadPoolExecutor is deprecated and will be prohibited '\n                'in Python 3.9',\n                DeprecationWarning, 2)\n        self._default_executor = executor\n\n    def _getaddrinfo_debug(self, host, port, family, type, proto, flags):\n        msg = [f\"{host}:{port!r}\"]\n        if family:\n            msg.append(f'family={family!r}')\n        if type:\n            msg.append(f'type={type!r}')\n        if proto:\n            msg.append(f'proto={proto!r}')\n        if flags:\n            msg.append(f'flags={flags!r}')\n        msg = ', '.join(msg)\n        logger.debug('Get address info %s', msg)\n\n        t0 = self.time()\n        addrinfo = socket.getaddrinfo(host, port, family, type, proto, flags)\n        dt = self.time() - t0\n\n        msg = f'Getting address info {msg} took {dt * 1e3:.3f}ms: {addrinfo!r}'\n        if dt >= self.slow_callback_duration:\n            logger.info(msg)\n        else:\n            logger.debug(msg)\n        return addrinfo\n\n    async def getaddrinfo(self, host, port, *,\n                          family=0, type=0, proto=0, flags=0):\n        if self._debug:\n            getaddr_func = self._getaddrinfo_debug\n        else:\n            getaddr_func = socket.getaddrinfo\n\n        return await self.run_in_executor(\n            None, getaddr_func, host, port, family, type, proto, flags)\n\n    async def getnameinfo(self, sockaddr, flags=0):\n        return await self.run_in_executor(\n            None, socket.getnameinfo, sockaddr, flags)\n\n    async def sock_sendfile(self, sock, file, offset=0, count=None,\n                            *, fallback=True):\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        self._check_sendfile_params(sock, file, offset, count)\n        try:\n            return await self._sock_sendfile_native(sock, file,\n                                                    offset, count)\n        except exceptions.SendfileNotAvailableError as exc:\n            if not fallback:\n                raise\n        return await self._sock_sendfile_fallback(sock, file,\n                                                  offset, count)\n\n    async def _sock_sendfile_native(self, sock, file, offset, count):\n        # NB: sendfile syscall is not supported for SSL sockets and\n        # non-mmap files even if sendfile is supported by OS\n        raise exceptions.SendfileNotAvailableError(\n            f\"syscall sendfile is not available for socket {sock!r} \"\n            \"and file {file!r} combination\")\n\n    async def _sock_sendfile_fallback(self, sock, file, offset, count):\n        if offset:\n            file.seek(offset)\n        blocksize = (\n            min(count, constants.SENDFILE_FALLBACK_READBUFFER_SIZE)\n            if count else constants.SENDFILE_FALLBACK_READBUFFER_SIZE\n        )\n        buf = bytearray(blocksize)\n        total_sent = 0\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        break\n                view = memoryview(buf)[:blocksize]\n                read = await self.run_in_executor(None, file.readinto, view)\n                if not read:\n                    break  # EOF\n                await self.sock_sendall(sock, view[:read])\n                total_sent += read\n            return total_sent\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n\n    def _check_sendfile_params(self, sock, file, offset, count):\n        if 'b' not in getattr(file, 'mode', 'b'):\n            raise ValueError(\"file should be opened in binary mode\")\n        if not sock.type == socket.SOCK_STREAM:\n            raise ValueError(\"only SOCK_STREAM type sockets are supported\")\n        if count is not None:\n            if not isinstance(count, int):\n                raise TypeError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n            if count <= 0:\n                raise ValueError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n        if not isinstance(offset, int):\n            raise TypeError(\n                \"offset must be a non-negative integer (got {!r})\".format(\n                    offset))\n        if offset < 0:\n            raise ValueError(\n                \"offset must be a non-negative integer (got {!r})\".format(\n                    offset))\n\n    async def _connect_sock(self, exceptions, addr_info, local_addr_infos=None):\n        \"\"\"Create, bind and connect one socket.\"\"\"\n        my_exceptions = []\n        exceptions.append(my_exceptions)\n        family, type_, proto, _, address = addr_info\n        sock = None\n        try:\n            sock = socket.socket(family=family, type=type_, proto=proto)\n            sock.setblocking(False)\n            if local_addr_infos is not None:\n                for _, _, _, _, laddr in local_addr_infos:\n                    try:\n                        sock.bind(laddr)\n                        break\n                    except OSError as exc:\n                        msg = (\n                            f'error while attempting to bind on '\n                            f'address {laddr!r}: '\n                            f'{exc.strerror.lower()}'\n                        )\n                        exc = OSError(exc.errno, msg)\n                        my_exceptions.append(exc)\n                else:  # all bind attempts failed\n                    raise my_exceptions.pop()\n            await self.sock_connect(sock, address)\n            return sock\n        except OSError as exc:\n            my_exceptions.append(exc)\n            if sock is not None:\n                sock.close()\n            raise\n        except:\n            if sock is not None:\n                sock.close()\n            raise\n\n    async def create_connection(\n            self, protocol_factory, host=None, port=None,\n            *, ssl=None, family=0,\n            proto=0, flags=0, sock=None,\n            local_addr=None, server_hostname=None,\n            ssl_handshake_timeout=None,\n            happy_eyeballs_delay=None, interleave=None):\n        \"\"\"Connect to a TCP server.\n\n        Create a streaming transport connection to a given Internet host and\n        port: socket family AF_INET or socket.AF_INET6 depending on host (or\n        family if specified), socket type SOCK_STREAM. protocol_factory must be\n        a callable returning a protocol instance.\n\n        This method is a coroutine which will try to establish the connection\n        in the background.  When successful, the coroutine returns a\n        (transport, protocol) pair.\n        \"\"\"\n        if server_hostname is not None and not ssl:\n            raise ValueError('server_hostname is only meaningful with ssl')\n\n        if server_hostname is None and ssl:\n            # Use host as default for server_hostname.  It is an error\n            # if host is empty or not set, e.g. when an\n            # already-connected socket was passed or when only a port\n            # is given.  To avoid this error, you can pass\n            # server_hostname='' -- this will bypass the hostname\n            # check.  (This also means that if host is a numeric\n            # IP/IPv6 address, we will attempt to verify that exact\n            # address; this will probably fail, but it is possible to\n            # create a certificate for a specific IP address, so we\n            # don't judge it here.)\n            if not host:\n                raise ValueError('You must set server_hostname '\n                                 'when using ssl without a host')\n            server_hostname = host\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if happy_eyeballs_delay is not None and interleave is None:\n            # If using happy eyeballs, default to interleave addresses by family\n            interleave = 1\n\n        if host is not None or port is not None:\n            if sock is not None:\n                raise ValueError(\n                    'host/port and sock can not be specified at the same time')\n\n            infos = await self._ensure_resolved(\n                (host, port), family=family,\n                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)\n            if not infos:\n                raise OSError('getaddrinfo() returned empty list')\n\n            if local_addr is not None:\n                laddr_infos = await self._ensure_resolved(\n                    local_addr, family=family,\n                    type=socket.SOCK_STREAM, proto=proto,\n                    flags=flags, loop=self)\n                if not laddr_infos:\n                    raise OSError('getaddrinfo() returned empty list')\n            else:\n                laddr_infos = None\n\n            if interleave:\n                infos = _interleave_addrinfos(infos, interleave)\n\n            exceptions = []\n            if happy_eyeballs_delay is None:\n                # not using happy eyeballs\n                for addrinfo in infos:\n                    try:\n                        sock = await self._connect_sock(\n                            exceptions, addrinfo, laddr_infos)\n                        break\n                    except OSError:\n                        continue\n            else:  # using happy eyeballs\n                sock, _, _ = await staggered.staggered_race(\n                    (functools.partial(self._connect_sock,\n                                       exceptions, addrinfo, laddr_infos)\n                     for addrinfo in infos),\n                    happy_eyeballs_delay, loop=self)\n\n            if sock is None:\n                exceptions = [exc for sub in exceptions for exc in sub]\n                if len(exceptions) == 1:\n                    raise exceptions[0]\n                else:\n                    # If they all have the same str(), raise one.\n                    model = str(exceptions[0])\n                    if all(str(exc) == model for exc in exceptions):\n                        raise exceptions[0]\n                    # Raise a combined exception so the user can see all\n                    # the various error messages.\n                    raise OSError('Multiple exceptions: {}'.format(\n                        ', '.join(str(exc) for exc in exceptions)))\n\n        else:\n            if sock is None:\n                raise ValueError(\n                    'host and port was not specified and no sock specified')\n            if sock.type != socket.SOCK_STREAM:\n                # We allow AF_INET, AF_INET6, AF_UNIX as long as they\n                # are SOCK_STREAM.\n                # We support passing AF_UNIX sockets even though we have\n                # a dedicated API for that: create_unix_connection.\n                # Disallowing AF_UNIX in this method, breaks backwards\n                # compatibility.\n                raise ValueError(\n                    f'A Stream Socket was expected, got {sock!r}')\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout)\n        if self._debug:\n            # Get the socket from the transport because SSL transport closes\n            # the old socket and creates a new SSL socket\n            sock = transport.get_extra_info('socket')\n            logger.debug(\"%r connected to %s:%r: (%r, %r)\",\n                         sock, host, port, transport, protocol)\n        return transport, protocol\n\n    async def _create_connection_transport(\n            self, sock, protocol_factory, ssl,\n            server_hostname, server_side=False,\n            ssl_handshake_timeout=None):\n\n        sock.setblocking(False)\n\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        if ssl:\n            sslcontext = None if isinstance(ssl, bool) else ssl\n            transport = self._make_ssl_transport(\n                sock, protocol, sslcontext, waiter,\n                server_side=server_side, server_hostname=server_hostname,\n                ssl_handshake_timeout=ssl_handshake_timeout)\n        else:\n            transport = self._make_socket_transport(sock, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        return transport, protocol\n\n    async def sendfile(self, transport, file, offset=0, count=None,\n                       *, fallback=True):\n        \"\"\"Send a file to transport.\n\n        Return the total number of bytes which were sent.\n\n        The method uses high-performance os.sendfile if available.\n\n        file must be a regular file object opened in binary mode.\n\n        offset tells from where to start reading the file. If specified,\n        count is the total number of bytes to transmit as opposed to\n        sending the file until EOF is reached. File position is updated on\n        return or also in case of error in which case file.tell()\n        can be used to figure out the number of bytes\n        which were sent.\n\n        fallback set to True makes asyncio to manually read and send\n        the file when the platform does not support the sendfile syscall\n        (e.g. Windows or SSL socket on Unix).\n\n        Raise SendfileNotAvailableError if the system does not support\n        sendfile syscall and fallback is False.\n        \"\"\"\n        if transport.is_closing():\n            raise RuntimeError(\"Transport is closing\")\n        mode = getattr(transport, '_sendfile_compatible',\n                       constants._SendfileMode.UNSUPPORTED)\n        if mode is constants._SendfileMode.UNSUPPORTED:\n            raise RuntimeError(\n                f\"sendfile is not supported for transport {transport!r}\")\n        if mode is constants._SendfileMode.TRY_NATIVE:\n            try:\n                return await self._sendfile_native(transport, file,\n                                                   offset, count)\n            except exceptions.SendfileNotAvailableError as exc:\n                if not fallback:\n                    raise\n\n        if not fallback:\n            raise RuntimeError(\n                f\"fallback is disabled and native sendfile is not \"\n                f\"supported for transport {transport!r}\")\n\n        return await self._sendfile_fallback(transport, file,\n                                             offset, count)\n\n    async def _sendfile_native(self, transp, file, offset, count):\n        raise exceptions.SendfileNotAvailableError(\n            \"sendfile syscall is not supported\")\n\n    async def _sendfile_fallback(self, transp, file, offset, count):\n        if offset:\n            file.seek(offset)\n        blocksize = min(count, 16384) if count else 16384\n        buf = bytearray(blocksize)\n        total_sent = 0\n        proto = _SendfileFallbackProtocol(transp)\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        return total_sent\n                view = memoryview(buf)[:blocksize]\n                read = await self.run_in_executor(None, file.readinto, view)\n                if not read:\n                    return total_sent  # EOF\n                await proto.drain()\n                transp.write(view[:read])\n                total_sent += read\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n            await proto.restore()\n\n    async def start_tls(self, transport, protocol, sslcontext, *,\n                        server_side=False,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None):\n        \"\"\"Upgrade transport to TLS.\n\n        Return a new transport that *protocol* should start using\n        immediately.\n        \"\"\"\n        if ssl is None:\n            raise RuntimeError('Python ssl module is not available')\n\n        if not isinstance(sslcontext, ssl.SSLContext):\n            raise TypeError(\n                f'sslcontext is expected to be an instance of ssl.SSLContext, '\n                f'got {sslcontext!r}')\n\n        if not getattr(transport, '_start_tls_compatible', False):\n            raise TypeError(\n                f'transport {transport!r} is not supported by start_tls()')\n\n        waiter = self.create_future()\n        ssl_protocol = sslproto.SSLProtocol(\n            self, protocol, sslcontext, waiter,\n            server_side, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            call_connection_made=False)\n\n        # Pause early so that \"ssl_protocol.data_received()\" doesn't\n        # have a chance to get called before \"ssl_protocol.connection_made()\".\n        transport.pause_reading()\n\n        transport.set_protocol(ssl_protocol)\n        conmade_cb = self.call_soon(ssl_protocol.connection_made, transport)\n        resume_cb = self.call_soon(transport.resume_reading)\n\n        try:\n            await waiter\n        except BaseException:\n            transport.close()\n            conmade_cb.cancel()\n            resume_cb.cancel()\n            raise\n\n        return ssl_protocol._app_transport\n\n    async def create_datagram_endpoint(self, protocol_factory,\n                                       local_addr=None, remote_addr=None, *,\n                                       family=0, proto=0, flags=0,\n                                       reuse_address=_unset, reuse_port=None,\n                                       allow_broadcast=None, sock=None):\n        \"\"\"Create datagram connection.\"\"\"\n        if sock is not None:\n            if sock.type != socket.SOCK_DGRAM:\n                raise ValueError(\n                    f'A UDP Socket was expected, got {sock!r}')\n            if (local_addr or remote_addr or\n                    family or proto or flags or\n                    reuse_port or allow_broadcast):\n                # show the problematic kwargs in exception msg\n                opts = dict(local_addr=local_addr, remote_addr=remote_addr,\n                            family=family, proto=proto, flags=flags,\n                            reuse_address=reuse_address, reuse_port=reuse_port,\n                            allow_broadcast=allow_broadcast)\n                problems = ', '.join(f'{k}={v}' for k, v in opts.items() if v)\n                raise ValueError(\n                    f'socket modifier keyword arguments can not be used '\n                    f'when sock is specified. ({problems})')\n            sock.setblocking(False)\n            r_addr = None\n        else:\n            if not (local_addr or remote_addr):\n                if family == 0:\n                    raise ValueError('unexpected address family')\n                addr_pairs_info = (((family, proto), (None, None)),)\n            elif hasattr(socket, 'AF_UNIX') and family == socket.AF_UNIX:\n                for addr in (local_addr, remote_addr):\n                    if addr is not None and not isinstance(addr, str):\n                        raise TypeError('string is expected')\n\n                if local_addr and local_addr[0] not in (0, '\\x00'):\n                    try:\n                        if stat.S_ISSOCK(os.stat(local_addr).st_mode):\n                            os.remove(local_addr)\n                    except FileNotFoundError:\n                        pass\n                    except OSError as err:\n                        # Directory may have permissions only to create socket.\n                        logger.error('Unable to check or remove stale UNIX '\n                                     'socket %r: %r',\n                                     local_addr, err)\n\n                addr_pairs_info = (((family, proto),\n                                    (local_addr, remote_addr)), )\n            else:\n                # join address by (family, protocol)\n                addr_infos = {}  # Using order preserving dict\n                for idx, addr in ((0, local_addr), (1, remote_addr)):\n                    if addr is not None:\n                        assert isinstance(addr, tuple) and len(addr) == 2, (\n                            '2-tuple is expected')\n\n                        infos = await self._ensure_resolved(\n                            addr, family=family, type=socket.SOCK_DGRAM,\n                            proto=proto, flags=flags, loop=self)\n                        if not infos:\n                            raise OSError('getaddrinfo() returned empty list')\n\n                        for fam, _, pro, _, address in infos:\n                            key = (fam, pro)\n                            if key not in addr_infos:\n                                addr_infos[key] = [None, None]\n                            addr_infos[key][idx] = address\n\n                # each addr has to have info for each (family, proto) pair\n                addr_pairs_info = [\n                    (key, addr_pair) for key, addr_pair in addr_infos.items()\n                    if not ((local_addr and addr_pair[0] is None) or\n                            (remote_addr and addr_pair[1] is None))]\n\n                if not addr_pairs_info:\n                    raise ValueError('can not get address information')\n\n            exceptions = []\n\n            # bpo-37228\n            if reuse_address is not _unset:\n                if reuse_address:\n                    raise ValueError(\"Passing `reuse_address=True` is no \"\n                                     \"longer supported, as the usage of \"\n                                     \"SO_REUSEPORT in UDP poses a significant \"\n                                     \"security concern.\")\n                else:\n                    warnings.warn(\"The *reuse_address* parameter has been \"\n                                  \"deprecated as of 3.5.10 and is scheduled \"\n                                  \"for removal in 3.11.\", DeprecationWarning,\n                                  stacklevel=2)\n\n            for ((family, proto),\n                 (local_address, remote_address)) in addr_pairs_info:\n                sock = None\n                r_addr = None\n                try:\n                    sock = socket.socket(\n                        family=family, type=socket.SOCK_DGRAM, proto=proto)\n                    if reuse_port:\n                        _set_reuseport(sock)\n                    if allow_broadcast:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n                    sock.setblocking(False)\n\n                    if local_addr:\n                        sock.bind(local_address)\n                    if remote_addr:\n                        if not allow_broadcast:\n                            await self.sock_connect(sock, remote_address)\n                        r_addr = remote_address\n                except OSError as exc:\n                    if sock is not None:\n                        sock.close()\n                    exceptions.append(exc)\n                except:\n                    if sock is not None:\n                        sock.close()\n                    raise\n                else:\n                    break\n            else:\n                raise exceptions[0]\n\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_datagram_transport(\n            sock, protocol, r_addr, waiter)\n        if self._debug:\n            if local_addr:\n                logger.info(\"Datagram endpoint local_addr=%r remote_addr=%r \"\n                            \"created: (%r, %r)\",\n                            local_addr, remote_addr, transport, protocol)\n            else:\n                logger.debug(\"Datagram endpoint remote_addr=%r created: \"\n                             \"(%r, %r)\",\n                             remote_addr, transport, protocol)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        return transport, protocol\n\n    async def _ensure_resolved(self, address, *,\n                               family=0, type=socket.SOCK_STREAM,\n                               proto=0, flags=0, loop):\n        host, port = address[:2]\n        info = _ipaddr_info(host, port, family, type, proto, *address[2:])\n        if info is not None:\n            # \"host\" is already a resolved IP.\n            return [info]\n        else:\n            return await loop.getaddrinfo(host, port, family=family, type=type,\n                                          proto=proto, flags=flags)\n\n    async def _create_server_getaddrinfo(self, host, port, family, flags):\n        infos = await self._ensure_resolved((host, port), family=family,\n                                            type=socket.SOCK_STREAM,\n                                            flags=flags, loop=self)\n        if not infos:\n            raise OSError(f'getaddrinfo({host!r}) returned empty list')\n        return infos\n\n    async def create_server(\n            self, protocol_factory, host=None, port=None,\n            *,\n            family=socket.AF_UNSPEC,\n            flags=socket.AI_PASSIVE,\n            sock=None,\n            backlog=100,\n            ssl=None,\n            reuse_address=None,\n            reuse_port=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        \"\"\"Create a TCP server.\n\n        The host parameter can be a string, in that case the TCP server is\n        bound to host and port.\n\n        The host parameter can also be a sequence of strings and in that case\n        the TCP server is bound to all hosts of the sequence. If a host\n        appears multiple times (possibly indirectly e.g. when hostnames\n        resolve to the same IP address), the server is only bound once to that\n        host.\n\n        Return a Server object which can be used to stop the service.\n\n        This method is a coroutine.\n        \"\"\"\n        if isinstance(ssl, bool):\n            raise TypeError('ssl argument must be an SSLContext or None')\n\n        if ssl_handshake_timeout is not None and ssl is None:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if host is not None or port is not None:\n            if sock is not None:\n                raise ValueError(\n                    'host/port and sock can not be specified at the same time')\n\n            if reuse_address is None:\n                reuse_address = os.name == 'posix' and sys.platform != 'cygwin'\n            sockets = []\n            if host == '':\n                hosts = [None]\n            elif (isinstance(host, str) or\n                  not isinstance(host, collections.abc.Iterable)):\n                hosts = [host]\n            else:\n                hosts = host\n\n            fs = [self._create_server_getaddrinfo(host, port, family=family,\n                                                  flags=flags)\n                  for host in hosts]\n            infos = await tasks.gather(*fs, loop=self)\n            infos = set(itertools.chain.from_iterable(infos))\n\n            completed = False\n            try:\n                for res in infos:\n                    af, socktype, proto, canonname, sa = res\n                    try:\n                        sock = socket.socket(af, socktype, proto)\n                    except socket.error:\n                        # Assume it's a bad family/type/protocol combination.\n                        if self._debug:\n                            logger.warning('create_server() failed to create '\n                                           'socket.socket(%r, %r, %r)',\n                                           af, socktype, proto, exc_info=True)\n                        continue\n                    sockets.append(sock)\n                    if reuse_address:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_REUSEADDR, True)\n                    if reuse_port:\n                        _set_reuseport(sock)\n                    # Disable IPv4/IPv6 dual stack support (enabled by\n                    # default on Linux) which makes a single socket\n                    # listen on both address families.\n                    if (_HAS_IPv6 and\n                            af == socket.AF_INET6 and\n                            hasattr(socket, 'IPPROTO_IPV6')):\n                        sock.setsockopt(socket.IPPROTO_IPV6,\n                                        socket.IPV6_V6ONLY,\n                                        True)\n                    try:\n                        sock.bind(sa)\n                    except OSError as err:\n                        raise OSError(err.errno, 'error while attempting '\n                                      'to bind on address %r: %s'\n                                      % (sa, err.strerror.lower())) from None\n                completed = True\n            finally:\n                if not completed:\n                    for sock in sockets:\n                        sock.close()\n        else:\n            if sock is None:\n                raise ValueError('Neither host/port nor sock were specified')\n            if sock.type != socket.SOCK_STREAM:\n                raise ValueError(f'A Stream Socket was expected, got {sock!r}')\n            sockets = [sock]\n\n        for sock in sockets:\n            sock.setblocking(False)\n\n        server = Server(self, sockets, protocol_factory,\n                        ssl, backlog, ssl_handshake_timeout)\n        if start_serving:\n            server._start_serving()\n            # Skip one loop iteration so that all 'loop.add_reader'\n            # go through.\n            await tasks.sleep(0, loop=self)\n\n        if self._debug:\n            logger.info(\"%r is serving\", server)\n        return server\n\n    async def connect_accepted_socket(\n            self, protocol_factory, sock,\n            *, ssl=None,\n            ssl_handshake_timeout=None):\n        \"\"\"Handle an accepted connection.\n\n        This is used by servers that accept connections outside of\n        asyncio but that use asyncio to handle connections.\n\n        This method is a coroutine.  When completed, the coroutine\n        returns a (transport, protocol) pair.\n        \"\"\"\n        if sock.type != socket.SOCK_STREAM:\n            raise ValueError(f'A Stream Socket was expected, got {sock!r}')\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, '', server_side=True,\n            ssl_handshake_timeout=ssl_handshake_timeout)\n        if self._debug:\n            # Get the socket from the transport because SSL transport closes\n            # the old socket and creates a new SSL socket\n            sock = transport.get_extra_info('socket')\n            logger.debug(\"%r handled: (%r, %r)\", sock, transport, protocol)\n        return transport, protocol\n\n    async def connect_read_pipe(self, protocol_factory, pipe):\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_read_pipe_transport(pipe, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        if self._debug:\n            logger.debug('Read pipe %r connected: (%r, %r)',\n                         pipe.fileno(), transport, protocol)\n        return transport, protocol\n\n    async def connect_write_pipe(self, protocol_factory, pipe):\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_write_pipe_transport(pipe, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        if self._debug:\n            logger.debug('Write pipe %r connected: (%r, %r)',\n                         pipe.fileno(), transport, protocol)\n        return transport, protocol\n\n    def _log_subprocess(self, msg, stdin, stdout, stderr):\n        info = [msg]\n        if stdin is not None:\n            info.append(f'stdin={_format_pipe(stdin)}')\n        if stdout is not None and stderr == subprocess.STDOUT:\n            info.append(f'stdout=stderr={_format_pipe(stdout)}')\n        else:\n            if stdout is not None:\n                info.append(f'stdout={_format_pipe(stdout)}')\n            if stderr is not None:\n                info.append(f'stderr={_format_pipe(stderr)}')\n        logger.debug(' '.join(info))\n\n    async def subprocess_shell(self, protocol_factory, cmd, *,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               universal_newlines=False,\n                               shell=True, bufsize=0,\n                               encoding=None, errors=None, text=None,\n                               **kwargs):\n        if not isinstance(cmd, (bytes, str)):\n            raise ValueError(\"cmd must be a string\")\n        if universal_newlines:\n            raise ValueError(\"universal_newlines must be False\")\n        if not shell:\n            raise ValueError(\"shell must be True\")\n        if bufsize != 0:\n            raise ValueError(\"bufsize must be 0\")\n        if text:\n            raise ValueError(\"text must be False\")\n        if encoding is not None:\n            raise ValueError(\"encoding must be None\")\n        if errors is not None:\n            raise ValueError(\"errors must be None\")\n\n        protocol = protocol_factory()\n        debug_log = None\n        if self._debug:\n            # don't log parameters: they may contain sensitive information\n            # (password) and may be too long\n            debug_log = 'run shell command %r' % cmd\n            self._log_subprocess(debug_log, stdin, stdout, stderr)\n        transport = await self._make_subprocess_transport(\n            protocol, cmd, True, stdin, stdout, stderr, bufsize, **kwargs)\n        if self._debug and debug_log is not None:\n            logger.info('%s: %r', debug_log, transport)\n        return transport, protocol\n\n    async def subprocess_exec(self, protocol_factory, program, *args,\n                              stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE, universal_newlines=False,\n                              shell=False, bufsize=0,\n                              encoding=None, errors=None, text=None,\n                              **kwargs):\n        if universal_newlines:\n            raise ValueError(\"universal_newlines must be False\")\n        if shell:\n            raise ValueError(\"shell must be False\")\n        if bufsize != 0:\n            raise ValueError(\"bufsize must be 0\")\n        if text:\n            raise ValueError(\"text must be False\")\n        if encoding is not None:\n            raise ValueError(\"encoding must be None\")\n        if errors is not None:\n            raise ValueError(\"errors must be None\")\n\n        popen_args = (program,) + args\n        protocol = protocol_factory()\n        debug_log = None\n        if self._debug:\n            # don't log parameters: they may contain sensitive information\n            # (password) and may be too long\n            debug_log = f'execute program {program!r}'\n            self._log_subprocess(debug_log, stdin, stdout, stderr)\n        transport = await self._make_subprocess_transport(\n            protocol, popen_args, False, stdin, stdout, stderr,\n            bufsize, **kwargs)\n        if self._debug and debug_log is not None:\n            logger.info('%s: %r', debug_log, transport)\n        return transport, protocol\n\n    def get_exception_handler(self):\n        \"\"\"Return an exception handler, or None if the default one is in use.\n        \"\"\"\n        return self._exception_handler\n\n    def set_exception_handler(self, handler):\n        \"\"\"Set handler as the new event loop exception handler.\n\n        If handler is None, the default exception handler will\n        be set.\n\n        If handler is a callable object, it should have a\n        signature matching '(loop, context)', where 'loop'\n        will be a reference to the active event loop, 'context'\n        will be a dict object (see `call_exception_handler()`\n        documentation for details about context).\n        \"\"\"\n        if handler is not None and not callable(handler):\n            raise TypeError(f'A callable object or None is expected, '\n                            f'got {handler!r}')\n        self._exception_handler = handler\n\n    def default_exception_handler(self, context):\n        \"\"\"Default exception handler.\n\n        This is called when an exception occurs and no exception\n        handler is set, and can be called by a custom exception\n        handler that wants to defer to the default behavior.\n\n        This default handler logs the error message and other\n        context-dependent information.  In debug mode, a truncated\n        stack trace is also appended showing where the given object\n        (e.g. a handle or future or task) was created, if any.\n\n        The context parameter has the same meaning as in\n        `call_exception_handler()`.\n        \"\"\"\n        message = context.get('message')\n        if not message:\n            message = 'Unhandled exception in event loop'\n\n        exception = context.get('exception')\n        if exception is not None:\n            exc_info = (type(exception), exception, exception.__traceback__)\n        else:\n            exc_info = False\n\n        if ('source_traceback' not in context and\n                self._current_handle is not None and\n                self._current_handle._source_traceback):\n            context['handle_traceback'] = \\\n                self._current_handle._source_traceback\n\n        log_lines = [message]\n        for key in sorted(context):\n            if key in {'message', 'exception'}:\n                continue\n            value = context[key]\n            if key == 'source_traceback':\n                tb = ''.join(traceback.format_list(value))\n                value = 'Object created at (most recent call last):\\n'\n                value += tb.rstrip()\n            elif key == 'handle_traceback':\n                tb = ''.join(traceback.format_list(value))\n                value = 'Handle created at (most recent call last):\\n'\n                value += tb.rstrip()\n            else:\n                value = repr(value)\n            log_lines.append(f'{key}: {value}')\n\n        logger.error('\\n'.join(log_lines), exc_info=exc_info)\n\n    def call_exception_handler(self, context):\n        \"\"\"Call the current event loop's exception handler.\n\n        The context argument is a dict containing the following keys:\n\n        - 'message': Error message;\n        - 'exception' (optional): Exception object;\n        - 'future' (optional): Future instance;\n        - 'task' (optional): Task instance;\n        - 'handle' (optional): Handle instance;\n        - 'protocol' (optional): Protocol instance;\n        - 'transport' (optional): Transport instance;\n        - 'socket' (optional): Socket instance;\n        - 'asyncgen' (optional): Asynchronous generator that caused\n                                 the exception.\n\n        New keys maybe introduced in the future.\n\n        Note: do not overload this method in an event loop subclass.\n        For custom exception handling, use the\n        `set_exception_handler()` method.\n        \"\"\"\n        if self._exception_handler is None:\n            try:\n                self.default_exception_handler(context)\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException:\n                # Second protection layer for unexpected errors\n                # in the default implementation, as well as for subclassed\n                # event loops with overloaded \"default_exception_handler\".\n                logger.error('Exception in default exception handler',\n                             exc_info=True)\n        else:\n            try:\n                self._exception_handler(self, context)\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                # Exception in the user set custom exception handler.\n                try:\n                    # Let's try default handler.\n                    self.default_exception_handler({\n                        'message': 'Unhandled error in exception handler',\n                        'exception': exc,\n                        'context': context,\n                    })\n                except (SystemExit, KeyboardInterrupt):\n                    raise\n                except BaseException:\n                    # Guard 'default_exception_handler' in case it is\n                    # overloaded.\n                    logger.error('Exception in default exception handler '\n                                 'while handling an unexpected error '\n                                 'in custom exception handler',\n                                 exc_info=True)\n\n    def _add_callback(self, handle):\n        \"\"\"Add a Handle to _scheduled (TimerHandle) or _ready.\"\"\"\n        assert isinstance(handle, events.Handle), 'A Handle is required here'\n        if handle._cancelled:\n            return\n        assert not isinstance(handle, events.TimerHandle)\n        self._ready.append(handle)\n\n    def _add_callback_signalsafe(self, handle):\n        \"\"\"Like _add_callback() but called from a signal handler.\"\"\"\n        self._add_callback(handle)\n        self._write_to_self()\n\n    def _timer_handle_cancelled(self, handle):\n        \"\"\"Notification that a TimerHandle has been cancelled.\"\"\"\n        if handle._scheduled:\n            self._timer_cancelled_count += 1\n\n    def _run_once(self):\n        \"\"\"Run one full iteration of the event loop.\n\n        This calls all currently ready callbacks, polls for I/O,\n        schedules the resulting callbacks, and finally schedules\n        'call_later' callbacks.\n        \"\"\"\n\n        sched_count = len(self._scheduled)\n        if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and\n            self._timer_cancelled_count / sched_count >\n                _MIN_CANCELLED_TIMER_HANDLES_FRACTION):\n            # Remove delayed calls that were cancelled if their number\n            # is too high\n            new_scheduled = []\n            for handle in self._scheduled:\n                if handle._cancelled:\n                    handle._scheduled = False\n                else:\n                    new_scheduled.append(handle)\n\n            heapq.heapify(new_scheduled)\n            self._scheduled = new_scheduled\n            self._timer_cancelled_count = 0\n        else:\n            # Remove delayed calls that were cancelled from head of queue.\n            while self._scheduled and self._scheduled[0]._cancelled:\n                self._timer_cancelled_count -= 1\n                handle = heapq.heappop(self._scheduled)\n                handle._scheduled = False\n\n        timeout = None\n        if self._ready or self._stopping:\n            timeout = 0\n        elif self._scheduled:\n            # Compute the desired timeout.\n            when = self._scheduled[0]._when\n            timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT)\n\n        event_list = self._selector.select(timeout)\n        self._process_events(event_list)\n\n        # Handle 'later' callbacks that are ready.\n        end_time = self.time() + self._clock_resolution\n        while self._scheduled:\n            handle = self._scheduled[0]\n            if handle._when >= end_time:\n                break\n            handle = heapq.heappop(self._scheduled)\n            handle._scheduled = False\n            self._ready.append(handle)\n\n        # This is the only place where callbacks are actually *called*.\n        # All other places just add them to ready.\n        # Note: We run all currently scheduled callbacks, but not any\n        # callbacks scheduled by callbacks run this time around --\n        # they will be run the next time (after another I/O poll).\n        # Use an idiom that is thread-safe without using locks.\n        ntodo = len(self._ready)\n        for i in range(ntodo):\n            handle = self._ready.popleft()\n            if handle._cancelled:\n                continue\n            if self._debug:\n                try:\n                    self._current_handle = handle\n                    t0 = self.time()\n                    handle._run()\n                    dt = self.time() - t0\n                    if dt >= self.slow_callback_duration:\n                        logger.warning('Executing %s took %.3f seconds',\n                                       _format_handle(handle), dt)\n                finally:\n                    self._current_handle = None\n            else:\n                handle._run()\n        handle = None  # Needed to break cycles when an exception occurs.\n\n    def _set_coroutine_origin_tracking(self, enabled):\n        if bool(enabled) == bool(self._coroutine_origin_tracking_enabled):\n            return\n\n        if enabled:\n            self._coroutine_origin_tracking_saved_depth = (\n                sys.get_coroutine_origin_tracking_depth())\n            sys.set_coroutine_origin_tracking_depth(\n                constants.DEBUG_STACK_DEPTH)\n        else:\n            sys.set_coroutine_origin_tracking_depth(\n                self._coroutine_origin_tracking_saved_depth)\n\n        self._coroutine_origin_tracking_enabled = enabled\n\n    def get_debug(self):\n        return self._debug\n\n    def set_debug(self, enabled):\n        self._debug = enabled\n\n        if self.is_running():\n            self.call_soon_threadsafe(self._set_coroutine_origin_tracking, enabled)\n", 1884], "/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py": ["# Access WeakSet through the weakref module.\n# This code is separated-out because it is needed\n# by abc.py to load everything else at startup.\n\nfrom _weakref import ref\n\n__all__ = ['WeakSet']\n\n\nclass _IterationGuard:\n    # This context manager registers itself in the current iterators of the\n    # weak container, such as to delay all removals until the context manager\n    # exits.\n    # This technique should be relatively thread-safe (since sets are).\n\n    def __init__(self, weakcontainer):\n        # Don't create cycles\n        self.weakcontainer = ref(weakcontainer)\n\n    def __enter__(self):\n        w = self.weakcontainer()\n        if w is not None:\n            w._iterating.add(self)\n        return self\n\n    def __exit__(self, e, t, b):\n        w = self.weakcontainer()\n        if w is not None:\n            s = w._iterating\n            s.remove(self)\n            if not s:\n                w._commit_removals()\n\n\nclass WeakSet:\n    def __init__(self, data=None):\n        self.data = set()\n        def _remove(item, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(item)\n                else:\n                    self.data.discard(item)\n        self._remove = _remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        if data is not None:\n            self.update(data)\n\n    def _commit_removals(self):\n        l = self._pending_removals\n        discard = self.data.discard\n        while l:\n            discard(l.pop())\n\n    def __iter__(self):\n        with _IterationGuard(self):\n            for itemref in self.data:\n                item = itemref()\n                if item is not None:\n                    # Caveat: the iterator will keep a strong reference to\n                    # `item` until it is resumed or closed.\n                    yield item\n\n    def __len__(self):\n        return len(self.data) - len(self._pending_removals)\n\n    def __contains__(self, item):\n        try:\n            wr = ref(item)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def __reduce__(self):\n        return (self.__class__, (list(self),),\n                getattr(self, '__dict__', None))\n\n    def add(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.add(ref(item, self._remove))\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        return self.__class__(self)\n\n    def pop(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            try:\n                itemref = self.data.pop()\n            except KeyError:\n                raise KeyError('pop from empty WeakSet') from None\n            item = itemref()\n            if item is not None:\n                return item\n\n    def remove(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.remove(ref(item))\n\n    def discard(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.discard(ref(item))\n\n    def update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        for element in other:\n            self.add(element)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def difference(self, other):\n        newset = self.copy()\n        newset.difference_update(other)\n        return newset\n    __sub__ = difference\n\n    def difference_update(self, other):\n        self.__isub__(other)\n    def __isub__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update(ref(item) for item in other)\n        return self\n\n    def intersection(self, other):\n        return self.__class__(item for item in other if item in self)\n    __and__ = intersection\n\n    def intersection_update(self, other):\n        self.__iand__(other)\n    def __iand__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update(ref(item) for item in other)\n        return self\n\n    def issubset(self, other):\n        return self.data.issubset(ref(item) for item in other)\n    __le__ = issubset\n\n    def __lt__(self, other):\n        return self.data < set(map(ref, other))\n\n    def issuperset(self, other):\n        return self.data.issuperset(ref(item) for item in other)\n    __ge__ = issuperset\n\n    def __gt__(self, other):\n        return self.data > set(map(ref, other))\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.data == set(map(ref, other))\n\n    def symmetric_difference(self, other):\n        newset = self.copy()\n        newset.symmetric_difference_update(other)\n        return newset\n    __xor__ = symmetric_difference\n\n    def symmetric_difference_update(self, other):\n        self.__ixor__(other)\n    def __ixor__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update(ref(item, self._remove) for item in other)\n        return self\n\n    def union(self, other):\n        return self.__class__(e for s in (self, other) for e in s)\n    __or__ = union\n\n    def isdisjoint(self, other):\n        return len(self.intersection(other)) == 0\n\n    def __repr__(self):\n        return repr(self.data)\n", 199], "/usr/lib/python3.8/selectors.py": ["\"\"\"Selectors module.\n\nThis module allows high-level and efficient I/O multiplexing, built upon the\n`select` module primitives.\n\"\"\"\n\n\nfrom abc import ABCMeta, abstractmethod\nfrom collections import namedtuple\nfrom collections.abc import Mapping\nimport math\nimport select\nimport sys\n\n\n# generic events, that must be mapped to implementation-specific ones\nEVENT_READ = (1 << 0)\nEVENT_WRITE = (1 << 1)\n\n\ndef _fileobj_to_fd(fileobj):\n    \"\"\"Return a file descriptor from a file object.\n\n    Parameters:\n    fileobj -- file object or file descriptor\n\n    Returns:\n    corresponding file descriptor\n\n    Raises:\n    ValueError if the object is invalid\n    \"\"\"\n    if isinstance(fileobj, int):\n        fd = fileobj\n    else:\n        try:\n            fd = int(fileobj.fileno())\n        except (AttributeError, TypeError, ValueError):\n            raise ValueError(\"Invalid file object: \"\n                             \"{!r}\".format(fileobj)) from None\n    if fd < 0:\n        raise ValueError(\"Invalid file descriptor: {}\".format(fd))\n    return fd\n\n\nSelectorKey = namedtuple('SelectorKey', ['fileobj', 'fd', 'events', 'data'])\n\nSelectorKey.__doc__ = \"\"\"SelectorKey(fileobj, fd, events, data)\n\n    Object used to associate a file object to its backing\n    file descriptor, selected event mask, and attached data.\n\"\"\"\nif sys.version_info >= (3, 5):\n    SelectorKey.fileobj.__doc__ = 'File object registered.'\n    SelectorKey.fd.__doc__ = 'Underlying file descriptor.'\n    SelectorKey.events.__doc__ = 'Events that must be waited for on this file object.'\n    SelectorKey.data.__doc__ = ('''Optional opaque data associated to this file object.\n    For example, this could be used to store a per-client session ID.''')\n\nclass _SelectorMapping(Mapping):\n    \"\"\"Mapping of file objects to selector keys.\"\"\"\n\n    def __init__(self, selector):\n        self._selector = selector\n\n    def __len__(self):\n        return len(self._selector._fd_to_key)\n\n    def __getitem__(self, fileobj):\n        try:\n            fd = self._selector._fileobj_lookup(fileobj)\n            return self._selector._fd_to_key[fd]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n\n    def __iter__(self):\n        return iter(self._selector._fd_to_key)\n\n\nclass BaseSelector(metaclass=ABCMeta):\n    \"\"\"Selector abstract base class.\n\n    A selector supports registering file objects to be monitored for specific\n    I/O events.\n\n    A file object is a file descriptor or any object with a `fileno()` method.\n    An arbitrary object can be attached to the file object, which can be used\n    for example to store context information, a callback, etc.\n\n    A selector can use various implementations (select(), poll(), epoll()...)\n    depending on the platform. The default `Selector` class uses the most\n    efficient implementation on the current platform.\n    \"\"\"\n\n    @abstractmethod\n    def register(self, fileobj, events, data=None):\n        \"\"\"Register a file object.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n        events  -- events to monitor (bitwise mask of EVENT_READ|EVENT_WRITE)\n        data    -- attached data\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        ValueError if events is invalid\n        KeyError if fileobj is already registered\n        OSError if fileobj is closed or otherwise is unacceptable to\n                the underlying system call (if a system call is made)\n\n        Note:\n        OSError may or may not be raised\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def unregister(self, fileobj):\n        \"\"\"Unregister a file object.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        KeyError if fileobj is not registered\n\n        Note:\n        If fileobj is registered but has since been closed this does\n        *not* raise OSError (even if the wrapped syscall does)\n        \"\"\"\n        raise NotImplementedError\n\n    def modify(self, fileobj, events, data=None):\n        \"\"\"Change a registered file object monitored events or attached data.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n        events  -- events to monitor (bitwise mask of EVENT_READ|EVENT_WRITE)\n        data    -- attached data\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        Anything that unregister() or register() raises\n        \"\"\"\n        self.unregister(fileobj)\n        return self.register(fileobj, events, data)\n\n    @abstractmethod\n    def select(self, timeout=None):\n        \"\"\"Perform the actual selection, until some monitored file objects are\n        ready or a timeout expires.\n\n        Parameters:\n        timeout -- if timeout > 0, this specifies the maximum wait time, in\n                   seconds\n                   if timeout <= 0, the select() call won't block, and will\n                   report the currently ready file objects\n                   if timeout is None, select() will block until a monitored\n                   file object becomes ready\n\n        Returns:\n        list of (key, events) for ready file objects\n        `events` is a bitwise mask of EVENT_READ|EVENT_WRITE\n        \"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Close the selector.\n\n        This must be called to make sure that any underlying resource is freed.\n        \"\"\"\n        pass\n\n    def get_key(self, fileobj):\n        \"\"\"Return the key associated to a registered file object.\n\n        Returns:\n        SelectorKey for this file object\n        \"\"\"\n        mapping = self.get_map()\n        if mapping is None:\n            raise RuntimeError('Selector is closed')\n        try:\n            return mapping[fileobj]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n\n    @abstractmethod\n    def get_map(self):\n        \"\"\"Return a mapping of file objects to selector keys.\"\"\"\n        raise NotImplementedError\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n\nclass _BaseSelectorImpl(BaseSelector):\n    \"\"\"Base selector implementation.\"\"\"\n\n    def __init__(self):\n        # this maps file descriptors to keys\n        self._fd_to_key = {}\n        # read-only mapping returned by get_map()\n        self._map = _SelectorMapping(self)\n\n    def _fileobj_lookup(self, fileobj):\n        \"\"\"Return a file descriptor from a file object.\n\n        This wraps _fileobj_to_fd() to do an exhaustive search in case\n        the object is invalid but we still have it in our map.  This\n        is used by unregister() so we can unregister an object that\n        was previously registered even if it is closed.  It is also\n        used by _SelectorMapping.\n        \"\"\"\n        try:\n            return _fileobj_to_fd(fileobj)\n        except ValueError:\n            # Do an exhaustive search.\n            for key in self._fd_to_key.values():\n                if key.fileobj is fileobj:\n                    return key.fd\n            # Raise ValueError after all.\n            raise\n\n    def register(self, fileobj, events, data=None):\n        if (not events) or (events & ~(EVENT_READ | EVENT_WRITE)):\n            raise ValueError(\"Invalid events: {!r}\".format(events))\n\n        key = SelectorKey(fileobj, self._fileobj_lookup(fileobj), events, data)\n\n        if key.fd in self._fd_to_key:\n            raise KeyError(\"{!r} (FD {}) is already registered\"\n                           .format(fileobj, key.fd))\n\n        self._fd_to_key[key.fd] = key\n        return key\n\n    def unregister(self, fileobj):\n        try:\n            key = self._fd_to_key.pop(self._fileobj_lookup(fileobj))\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n        return key\n\n    def modify(self, fileobj, events, data=None):\n        try:\n            key = self._fd_to_key[self._fileobj_lookup(fileobj)]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n        if events != key.events:\n            self.unregister(fileobj)\n            key = self.register(fileobj, events, data)\n        elif data != key.data:\n            # Use a shortcut to update the data.\n            key = key._replace(data=data)\n            self._fd_to_key[key.fd] = key\n        return key\n\n    def close(self):\n        self._fd_to_key.clear()\n        self._map = None\n\n    def get_map(self):\n        return self._map\n\n    def _key_from_fd(self, fd):\n        \"\"\"Return the key associated to a given file descriptor.\n\n        Parameters:\n        fd -- file descriptor\n\n        Returns:\n        corresponding key, or None if not found\n        \"\"\"\n        try:\n            return self._fd_to_key[fd]\n        except KeyError:\n            return None\n\n\nclass SelectSelector(_BaseSelectorImpl):\n    \"\"\"Select-based selector.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._readers = set()\n        self._writers = set()\n\n    def register(self, fileobj, events, data=None):\n        key = super().register(fileobj, events, data)\n        if events & EVENT_READ:\n            self._readers.add(key.fd)\n        if events & EVENT_WRITE:\n            self._writers.add(key.fd)\n        return key\n\n    def unregister(self, fileobj):\n        key = super().unregister(fileobj)\n        self._readers.discard(key.fd)\n        self._writers.discard(key.fd)\n        return key\n\n    if sys.platform == 'win32':\n        def _select(self, r, w, _, timeout=None):\n            r, w, x = select.select(r, w, w, timeout)\n            return r, w + x, []\n    else:\n        _select = select.select\n\n    def select(self, timeout=None):\n        timeout = None if timeout is None else max(timeout, 0)\n        ready = []\n        try:\n            r, w, _ = self._select(self._readers, self._writers, [], timeout)\n        except InterruptedError:\n            return ready\n        r = set(r)\n        w = set(w)\n        for fd in r | w:\n            events = 0\n            if fd in r:\n                events |= EVENT_READ\n            if fd in w:\n                events |= EVENT_WRITE\n\n            key = self._key_from_fd(fd)\n            if key:\n                ready.append((key, events & key.events))\n        return ready\n\n\nclass _PollLikeSelector(_BaseSelectorImpl):\n    \"\"\"Base class shared between poll, epoll and devpoll selectors.\"\"\"\n    _selector_cls = None\n    _EVENT_READ = None\n    _EVENT_WRITE = None\n\n    def __init__(self):\n        super().__init__()\n        self._selector = self._selector_cls()\n\n    def register(self, fileobj, events, data=None):\n        key = super().register(fileobj, events, data)\n        poller_events = 0\n        if events & EVENT_READ:\n            poller_events |= self._EVENT_READ\n        if events & EVENT_WRITE:\n            poller_events |= self._EVENT_WRITE\n        try:\n            self._selector.register(key.fd, poller_events)\n        except:\n            super().unregister(fileobj)\n            raise\n        return key\n\n    def unregister(self, fileobj):\n        key = super().unregister(fileobj)\n        try:\n            self._selector.unregister(key.fd)\n        except OSError:\n            # This can happen if the FD was closed since it\n            # was registered.\n            pass\n        return key\n\n    def modify(self, fileobj, events, data=None):\n        try:\n            key = self._fd_to_key[self._fileobj_lookup(fileobj)]\n        except KeyError:\n            raise KeyError(f\"{fileobj!r} is not registered\") from None\n\n        changed = False\n        if events != key.events:\n            selector_events = 0\n            if events & EVENT_READ:\n                selector_events |= self._EVENT_READ\n            if events & EVENT_WRITE:\n                selector_events |= self._EVENT_WRITE\n            try:\n                self._selector.modify(key.fd, selector_events)\n            except:\n                super().unregister(fileobj)\n                raise\n            changed = True\n        if data != key.data:\n            changed = True\n\n        if changed:\n            key = key._replace(events=events, data=data)\n            self._fd_to_key[key.fd] = key\n        return key\n\n    def select(self, timeout=None):\n        # This is shared between poll() and epoll().\n        # epoll() has a different signature and handling of timeout parameter.\n        if timeout is None:\n            timeout = None\n        elif timeout <= 0:\n            timeout = 0\n        else:\n            # poll() has a resolution of 1 millisecond, round away from\n            # zero to wait *at least* timeout seconds.\n            timeout = math.ceil(timeout * 1e3)\n        ready = []\n        try:\n            fd_event_list = self._selector.poll(timeout)\n        except InterruptedError:\n            return ready\n        for fd, event in fd_event_list:\n            events = 0\n            if event & ~self._EVENT_READ:\n                events |= EVENT_WRITE\n            if event & ~self._EVENT_WRITE:\n                events |= EVENT_READ\n\n            key = self._key_from_fd(fd)\n            if key:\n                ready.append((key, events & key.events))\n        return ready\n\n\nif hasattr(select, 'poll'):\n\n    class PollSelector(_PollLikeSelector):\n        \"\"\"Poll-based selector.\"\"\"\n        _selector_cls = select.poll\n        _EVENT_READ = select.POLLIN\n        _EVENT_WRITE = select.POLLOUT\n\n\nif hasattr(select, 'epoll'):\n\n    class EpollSelector(_PollLikeSelector):\n        \"\"\"Epoll-based selector.\"\"\"\n        _selector_cls = select.epoll\n        _EVENT_READ = select.EPOLLIN\n        _EVENT_WRITE = select.EPOLLOUT\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def select(self, timeout=None):\n            if timeout is None:\n                timeout = -1\n            elif timeout <= 0:\n                timeout = 0\n            else:\n                # epoll_wait() has a resolution of 1 millisecond, round away\n                # from zero to wait *at least* timeout seconds.\n                timeout = math.ceil(timeout * 1e3) * 1e-3\n\n            # epoll_wait() expects `maxevents` to be greater than zero;\n            # we want to make sure that `select()` can be called when no\n            # FD is registered.\n            max_ev = max(len(self._fd_to_key), 1)\n\n            ready = []\n            try:\n                fd_event_list = self._selector.poll(timeout, max_ev)\n            except InterruptedError:\n                return ready\n            for fd, event in fd_event_list:\n                events = 0\n                if event & ~select.EPOLLIN:\n                    events |= EVENT_WRITE\n                if event & ~select.EPOLLOUT:\n                    events |= EVENT_READ\n\n                key = self._key_from_fd(fd)\n                if key:\n                    ready.append((key, events & key.events))\n            return ready\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\nif hasattr(select, 'devpoll'):\n\n    class DevpollSelector(_PollLikeSelector):\n        \"\"\"Solaris /dev/poll selector.\"\"\"\n        _selector_cls = select.devpoll\n        _EVENT_READ = select.POLLIN\n        _EVENT_WRITE = select.POLLOUT\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\nif hasattr(select, 'kqueue'):\n\n    class KqueueSelector(_BaseSelectorImpl):\n        \"\"\"Kqueue-based selector.\"\"\"\n\n        def __init__(self):\n            super().__init__()\n            self._selector = select.kqueue()\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def register(self, fileobj, events, data=None):\n            key = super().register(fileobj, events, data)\n            try:\n                if events & EVENT_READ:\n                    kev = select.kevent(key.fd, select.KQ_FILTER_READ,\n                                        select.KQ_EV_ADD)\n                    self._selector.control([kev], 0, 0)\n                if events & EVENT_WRITE:\n                    kev = select.kevent(key.fd, select.KQ_FILTER_WRITE,\n                                        select.KQ_EV_ADD)\n                    self._selector.control([kev], 0, 0)\n            except:\n                super().unregister(fileobj)\n                raise\n            return key\n\n        def unregister(self, fileobj):\n            key = super().unregister(fileobj)\n            if key.events & EVENT_READ:\n                kev = select.kevent(key.fd, select.KQ_FILTER_READ,\n                                    select.KQ_EV_DELETE)\n                try:\n                    self._selector.control([kev], 0, 0)\n                except OSError:\n                    # This can happen if the FD was closed since it\n                    # was registered.\n                    pass\n            if key.events & EVENT_WRITE:\n                kev = select.kevent(key.fd, select.KQ_FILTER_WRITE,\n                                    select.KQ_EV_DELETE)\n                try:\n                    self._selector.control([kev], 0, 0)\n                except OSError:\n                    # See comment above.\n                    pass\n            return key\n\n        def select(self, timeout=None):\n            timeout = None if timeout is None else max(timeout, 0)\n            max_ev = len(self._fd_to_key)\n            ready = []\n            try:\n                kev_list = self._selector.control(None, max_ev, timeout)\n            except InterruptedError:\n                return ready\n            for kev in kev_list:\n                fd = kev.ident\n                flag = kev.filter\n                events = 0\n                if flag == select.KQ_FILTER_READ:\n                    events |= EVENT_READ\n                if flag == select.KQ_FILTER_WRITE:\n                    events |= EVENT_WRITE\n\n                key = self._key_from_fd(fd)\n                if key:\n                    ready.append((key, events & key.events))\n            return ready\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\n# Choose the best implementation, roughly:\n#    epoll|kqueue|devpoll > poll > select.\n# select() also can't accept a FD > FD_SETSIZE (usually around 1024)\nif 'KqueueSelector' in globals():\n    DefaultSelector = KqueueSelector\nelif 'EpollSelector' in globals():\n    DefaultSelector = EpollSelector\nelif 'DevpollSelector' in globals():\n    DefaultSelector = DevpollSelector\nelif 'PollSelector' in globals():\n    DefaultSelector = PollSelector\nelse:\n    DefaultSelector = SelectSelector\n", 592], "/usr/lib/python3.8/logging/__init__.py": ["# Copyright 2001-2017 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\"\"\"\nLogging package for Python. Based on PEP 282 and comments thereto in\ncomp.lang.python.\n\nCopyright (C) 2001-2017 Vinay Sajip. All Rights Reserved.\n\nTo use, simply 'import logging' and log away!\n\"\"\"\n\nimport sys, os, time, io, re, traceback, warnings, weakref, collections.abc\n\nfrom string import Template\nfrom string import Formatter as StrFormatter\n\n\n__all__ = ['BASIC_FORMAT', 'BufferingFormatter', 'CRITICAL', 'DEBUG', 'ERROR',\n           'FATAL', 'FileHandler', 'Filter', 'Formatter', 'Handler', 'INFO',\n           'LogRecord', 'Logger', 'LoggerAdapter', 'NOTSET', 'NullHandler',\n           'StreamHandler', 'WARN', 'WARNING', 'addLevelName', 'basicConfig',\n           'captureWarnings', 'critical', 'debug', 'disable', 'error',\n           'exception', 'fatal', 'getLevelName', 'getLogger', 'getLoggerClass',\n           'info', 'log', 'makeLogRecord', 'setLoggerClass', 'shutdown',\n           'warn', 'warning', 'getLogRecordFactory', 'setLogRecordFactory',\n           'lastResort', 'raiseExceptions']\n\nimport threading\n\n__author__  = \"Vinay Sajip <vinay_sajip@red-dove.com>\"\n__status__  = \"production\"\n# The following module attributes are no longer updated.\n__version__ = \"0.5.1.2\"\n__date__    = \"07 February 2010\"\n\n#---------------------------------------------------------------------------\n#   Miscellaneous module data\n#---------------------------------------------------------------------------\n\n#\n#_startTime is used as the base when calculating the relative time of events\n#\n_startTime = time.time()\n\n#\n#raiseExceptions is used to see if exceptions during handling should be\n#propagated\n#\nraiseExceptions = True\n\n#\n# If you don't want threading information in the log, set this to zero\n#\nlogThreads = True\n\n#\n# If you don't want multiprocessing information in the log, set this to zero\n#\nlogMultiprocessing = True\n\n#\n# If you don't want process information in the log, set this to zero\n#\nlogProcesses = True\n\n#---------------------------------------------------------------------------\n#   Level related stuff\n#---------------------------------------------------------------------------\n#\n# Default levels and level names, these can be replaced with any positive set\n# of values having corresponding names. There is a pseudo-level, NOTSET, which\n# is only really there as a lower limit for user-defined levels. Handlers and\n# loggers are initialized with NOTSET so that they will log all messages, even\n# at user-defined levels.\n#\n\nCRITICAL = 50\nFATAL = CRITICAL\nERROR = 40\nWARNING = 30\nWARN = WARNING\nINFO = 20\nDEBUG = 10\nNOTSET = 0\n\n_levelToName = {\n    CRITICAL: 'CRITICAL',\n    ERROR: 'ERROR',\n    WARNING: 'WARNING',\n    INFO: 'INFO',\n    DEBUG: 'DEBUG',\n    NOTSET: 'NOTSET',\n}\n_nameToLevel = {\n    'CRITICAL': CRITICAL,\n    'FATAL': FATAL,\n    'ERROR': ERROR,\n    'WARN': WARNING,\n    'WARNING': WARNING,\n    'INFO': INFO,\n    'DEBUG': DEBUG,\n    'NOTSET': NOTSET,\n}\n\ndef getLevelName(level):\n    \"\"\"\n    Return the textual representation of logging level 'level'.\n\n    If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,\n    INFO, DEBUG) then you get the corresponding string. If you have\n    associated levels with names using addLevelName then the name you have\n    associated with 'level' is returned.\n\n    If a numeric value corresponding to one of the defined levels is passed\n    in, the corresponding string representation is returned.\n\n    Otherwise, the string \"Level %s\" % level is returned.\n    \"\"\"\n    # See Issues #22386, #27937 and #29220 for why it's this way\n    result = _levelToName.get(level)\n    if result is not None:\n        return result\n    result = _nameToLevel.get(level)\n    if result is not None:\n        return result\n    return \"Level %s\" % level\n\ndef addLevelName(level, levelName):\n    \"\"\"\n    Associate 'levelName' with 'level'.\n\n    This is used when converting levels to text during message formatting.\n    \"\"\"\n    _acquireLock()\n    try:    #unlikely to cause an exception, but you never know...\n        _levelToName[level] = levelName\n        _nameToLevel[levelName] = level\n    finally:\n        _releaseLock()\n\nif hasattr(sys, '_getframe'):\n    currentframe = lambda: sys._getframe(3)\nelse: #pragma: no cover\n    def currentframe():\n        \"\"\"Return the frame object for the caller's stack frame.\"\"\"\n        try:\n            raise Exception\n        except Exception:\n            return sys.exc_info()[2].tb_frame.f_back\n\n#\n# _srcfile is used when walking the stack to check when we've got the first\n# caller stack frame, by skipping frames whose filename is that of this\n# module's source. It therefore should contain the filename of this module's\n# source file.\n#\n# Ordinarily we would use __file__ for this, but frozen modules don't always\n# have __file__ set, for some reason (see Issue #21736). Thus, we get the\n# filename from a handy code object from a function defined in this module.\n# (There's no particular reason for picking addLevelName.)\n#\n\n_srcfile = os.path.normcase(addLevelName.__code__.co_filename)\n\n# _srcfile is only used in conjunction with sys._getframe().\n# To provide compatibility with older versions of Python, set _srcfile\n# to None if _getframe() is not available; this value will prevent\n# findCaller() from being called. You can also do this if you want to avoid\n# the overhead of fetching caller information, even when _getframe() is\n# available.\n#if not hasattr(sys, '_getframe'):\n#    _srcfile = None\n\n\ndef _checkLevel(level):\n    if isinstance(level, int):\n        rv = level\n    elif str(level) == level:\n        if level not in _nameToLevel:\n            raise ValueError(\"Unknown level: %r\" % level)\n        rv = _nameToLevel[level]\n    else:\n        raise TypeError(\"Level not an integer or a valid string: %r\" % level)\n    return rv\n\n#---------------------------------------------------------------------------\n#   Thread-related stuff\n#---------------------------------------------------------------------------\n\n#\n#_lock is used to serialize access to shared data structures in this module.\n#This needs to be an RLock because fileConfig() creates and configures\n#Handlers, and so might arbitrary user threads. Since Handler code updates the\n#shared dictionary _handlers, it needs to acquire the lock. But if configuring,\n#the lock would already have been acquired - so we need an RLock.\n#The same argument applies to Loggers and Manager.loggerDict.\n#\n_lock = threading.RLock()\n\ndef _acquireLock():\n    \"\"\"\n    Acquire the module-level lock for serializing access to shared data.\n\n    This should be released with _releaseLock().\n    \"\"\"\n    if _lock:\n        _lock.acquire()\n\ndef _releaseLock():\n    \"\"\"\n    Release the module-level lock acquired by calling _acquireLock().\n    \"\"\"\n    if _lock:\n        _lock.release()\n\n\n# Prevent a held logging lock from blocking a child from logging.\n\nif not hasattr(os, 'register_at_fork'):  # Windows and friends.\n    def _register_at_fork_reinit_lock(instance):\n        pass  # no-op when os.register_at_fork does not exist.\nelse:\n    # A collection of instances with a createLock method (logging.Handler)\n    # to be called in the child after forking.  The weakref avoids us keeping\n    # discarded Handler instances alive.  A set is used to avoid accumulating\n    # duplicate registrations as createLock() is responsible for registering\n    # a new Handler instance with this set in the first place.\n    _at_fork_reinit_lock_weakset = weakref.WeakSet()\n\n    def _register_at_fork_reinit_lock(instance):\n        _acquireLock()\n        try:\n            _at_fork_reinit_lock_weakset.add(instance)\n        finally:\n            _releaseLock()\n\n    def _after_at_fork_child_reinit_locks():\n        # _acquireLock() was called in the parent before forking.\n        for handler in _at_fork_reinit_lock_weakset:\n            try:\n                handler.createLock()\n            except Exception as err:\n                # Similar to what PyErr_WriteUnraisable does.\n                print(\"Ignoring exception from logging atfork\", instance,\n                      \"._reinit_lock() method:\", err, file=sys.stderr)\n        _releaseLock()  # Acquired by os.register_at_fork(before=.\n\n\n    os.register_at_fork(before=_acquireLock,\n                        after_in_child=_after_at_fork_child_reinit_locks,\n                        after_in_parent=_releaseLock)\n\n\n#---------------------------------------------------------------------------\n#   The logging record\n#---------------------------------------------------------------------------\n\nclass LogRecord(object):\n    \"\"\"\n    A LogRecord instance represents an event being logged.\n\n    LogRecord instances are created every time something is logged. They\n    contain all the information pertinent to the event being logged. The\n    main information passed in is in msg and args, which are combined\n    using str(msg) % args to create the message field of the record. The\n    record also includes information such as when the record was created,\n    the source line where the logging call was made, and any exception\n    information to be logged.\n    \"\"\"\n    def __init__(self, name, level, pathname, lineno,\n                 msg, args, exc_info, func=None, sinfo=None, **kwargs):\n        \"\"\"\n        Initialize a logging record with interesting information.\n        \"\"\"\n        ct = time.time()\n        self.name = name\n        self.msg = msg\n        #\n        # The following statement allows passing of a dictionary as a sole\n        # argument, so that you can do something like\n        #  logging.debug(\"a %(a)d b %(b)s\", {'a':1, 'b':2})\n        # Suggested by Stefan Behnel.\n        # Note that without the test for args[0], we get a problem because\n        # during formatting, we test to see if the arg is present using\n        # 'if self.args:'. If the event being logged is e.g. 'Value is %d'\n        # and if the passed arg fails 'if self.args:' then no formatting\n        # is done. For example, logger.warning('Value is %d', 0) would log\n        # 'Value is %d' instead of 'Value is 0'.\n        # For the use case of passing a dictionary, this should not be a\n        # problem.\n        # Issue #21172: a request was made to relax the isinstance check\n        # to hasattr(args[0], '__getitem__'). However, the docs on string\n        # formatting still seem to suggest a mapping object is required.\n        # Thus, while not removing the isinstance check, it does now look\n        # for collections.abc.Mapping rather than, as before, dict.\n        if (args and len(args) == 1 and isinstance(args[0], collections.abc.Mapping)\n            and args[0]):\n            args = args[0]\n        self.args = args\n        self.levelname = getLevelName(level)\n        self.levelno = level\n        self.pathname = pathname\n        try:\n            self.filename = os.path.basename(pathname)\n            self.module = os.path.splitext(self.filename)[0]\n        except (TypeError, ValueError, AttributeError):\n            self.filename = pathname\n            self.module = \"Unknown module\"\n        self.exc_info = exc_info\n        self.exc_text = None      # used to cache the traceback text\n        self.stack_info = sinfo\n        self.lineno = lineno\n        self.funcName = func\n        self.created = ct\n        self.msecs = (ct - int(ct)) * 1000\n        self.relativeCreated = (self.created - _startTime) * 1000\n        if logThreads:\n            self.thread = threading.get_ident()\n            self.threadName = threading.current_thread().name\n        else: # pragma: no cover\n            self.thread = None\n            self.threadName = None\n        if not logMultiprocessing: # pragma: no cover\n            self.processName = None\n        else:\n            self.processName = 'MainProcess'\n            mp = sys.modules.get('multiprocessing')\n            if mp is not None:\n                # Errors may occur if multiprocessing has not finished loading\n                # yet - e.g. if a custom import hook causes third-party code\n                # to run when multiprocessing calls import. See issue 8200\n                # for an example\n                try:\n                    self.processName = mp.current_process().name\n                except Exception: #pragma: no cover\n                    pass\n        if logProcesses and hasattr(os, 'getpid'):\n            self.process = os.getpid()\n        else:\n            self.process = None\n\n    def __repr__(self):\n        return '<LogRecord: %s, %s, %s, %s, \"%s\">'%(self.name, self.levelno,\n            self.pathname, self.lineno, self.msg)\n\n    def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        msg = str(self.msg)\n        if self.args:\n            msg = msg % self.args\n        return msg\n\n#\n#   Determine which class to use when instantiating log records.\n#\n_logRecordFactory = LogRecord\n\ndef setLogRecordFactory(factory):\n    \"\"\"\n    Set the factory to be used when instantiating a log record.\n\n    :param factory: A callable which will be called to instantiate\n    a log record.\n    \"\"\"\n    global _logRecordFactory\n    _logRecordFactory = factory\n\ndef getLogRecordFactory():\n    \"\"\"\n    Return the factory to be used when instantiating a log record.\n    \"\"\"\n\n    return _logRecordFactory\n\ndef makeLogRecord(dict):\n    \"\"\"\n    Make a LogRecord whose attributes are defined by the specified dictionary,\n    This function is useful for converting a logging event received over\n    a socket connection (which is sent as a dictionary) into a LogRecord\n    instance.\n    \"\"\"\n    rv = _logRecordFactory(None, None, \"\", 0, \"\", (), None, None)\n    rv.__dict__.update(dict)\n    return rv\n\n\n#---------------------------------------------------------------------------\n#   Formatter classes and functions\n#---------------------------------------------------------------------------\n_str_formatter = StrFormatter()\ndel StrFormatter\n\n\nclass PercentStyle(object):\n\n    default_format = '%(message)s'\n    asctime_format = '%(asctime)s'\n    asctime_search = '%(asctime)'\n    validation_pattern = re.compile(r'%\\(\\w+\\)[#0+ -]*(\\*|\\d+)?(\\.(\\*|\\d+))?[diouxefgcrsa%]', re.I)\n\n    def __init__(self, fmt):\n        self._fmt = fmt or self.default_format\n\n    def usesTime(self):\n        return self._fmt.find(self.asctime_search) >= 0\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it matches the correct style\"\"\"\n        if not self.validation_pattern.search(self._fmt):\n            raise ValueError(\"Invalid format '%s' for '%s' style\" % (self._fmt, self.default_format[0]))\n\n    def _format(self, record):\n        return self._fmt % record.__dict__\n\n    def format(self, record):\n        try:\n            return self._format(record)\n        except KeyError as e:\n            raise ValueError('Formatting field not found in record: %s' % e)\n\n\nclass StrFormatStyle(PercentStyle):\n    default_format = '{message}'\n    asctime_format = '{asctime}'\n    asctime_search = '{asctime'\n\n    fmt_spec = re.compile(r'^(.?[<>=^])?[+ -]?#?0?(\\d+|{\\w+})?[,_]?(\\.(\\d+|{\\w+}))?[bcdefgnosx%]?$', re.I)\n    field_spec = re.compile(r'^(\\d+|\\w+)(\\.\\w+|\\[[^]]+\\])*$')\n\n    def _format(self, record):\n        return self._fmt.format(**record.__dict__)\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it is the correct string formatting style\"\"\"\n        fields = set()\n        try:\n            for _, fieldname, spec, conversion in _str_formatter.parse(self._fmt):\n                if fieldname:\n                    if not self.field_spec.match(fieldname):\n                        raise ValueError('invalid field name/expression: %r' % fieldname)\n                    fields.add(fieldname)\n                if conversion and conversion not in 'rsa':\n                    raise ValueError('invalid conversion: %r' % conversion)\n                if spec and not self.fmt_spec.match(spec):\n                    raise ValueError('bad specifier: %r' % spec)\n        except ValueError as e:\n            raise ValueError('invalid format: %s' % e)\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n\nclass StringTemplateStyle(PercentStyle):\n    default_format = '${message}'\n    asctime_format = '${asctime}'\n    asctime_search = '${asctime}'\n\n    def __init__(self, fmt):\n        self._fmt = fmt or self.default_format\n        self._tpl = Template(self._fmt)\n\n    def usesTime(self):\n        fmt = self._fmt\n        return fmt.find('$asctime') >= 0 or fmt.find(self.asctime_format) >= 0\n\n    def validate(self):\n        pattern = Template.pattern\n        fields = set()\n        for m in pattern.finditer(self._fmt):\n            d = m.groupdict()\n            if d['named']:\n                fields.add(d['named'])\n            elif d['braced']:\n                fields.add(d['braced'])\n            elif m.group(0) == '$':\n                raise ValueError('invalid format: bare \\'$\\' not allowed')\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n    def _format(self, record):\n        return self._tpl.substitute(**record.__dict__)\n\n\nBASIC_FORMAT = \"%(levelname)s:%(name)s:%(message)s\"\n\n_STYLES = {\n    '%': (PercentStyle, BASIC_FORMAT),\n    '{': (StrFormatStyle, '{levelname}:{name}:{message}'),\n    '$': (StringTemplateStyle, '${levelname}:${name}:${message}'),\n}\n\nclass Formatter(object):\n    \"\"\"\n    Formatter instances are used to convert a LogRecord to text.\n\n    Formatters need to know how a LogRecord is constructed. They are\n    responsible for converting a LogRecord to (usually) a string which can\n    be interpreted by either a human or an external system. The base Formatter\n    allows a formatting string to be specified. If none is supplied, the\n    the style-dependent default value, \"%(message)s\", \"{message}\", or\n    \"${message}\", is used.\n\n    The Formatter can be initialized with a format string which makes use of\n    knowledge of the LogRecord attributes - e.g. the default value mentioned\n    above makes use of the fact that the user's message and arguments are pre-\n    formatted into a LogRecord's message attribute. Currently, the useful\n    attributes in a LogRecord are described by:\n\n    %(name)s            Name of the logger (logging channel)\n    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,\n                        WARNING, ERROR, CRITICAL)\n    %(levelname)s       Text logging level for the message (\"DEBUG\", \"INFO\",\n                        \"WARNING\", \"ERROR\", \"CRITICAL\")\n    %(pathname)s        Full pathname of the source file where the logging\n                        call was issued (if available)\n    %(filename)s        Filename portion of pathname\n    %(module)s          Module (name portion of filename)\n    %(lineno)d          Source line number where the logging call was issued\n                        (if available)\n    %(funcName)s        Function name\n    %(created)f         Time when the LogRecord was created (time.time()\n                        return value)\n    %(asctime)s         Textual time when the LogRecord was created\n    %(msecs)d           Millisecond portion of the creation time\n    %(relativeCreated)d Time in milliseconds when the LogRecord was created,\n                        relative to the time the logging module was loaded\n                        (typically at application startup time)\n    %(thread)d          Thread ID (if available)\n    %(threadName)s      Thread name (if available)\n    %(process)d         Process ID (if available)\n    %(message)s         The result of record.getMessage(), computed just as\n                        the record is emitted\n    \"\"\"\n\n    converter = time.localtime\n\n    def __init__(self, fmt=None, datefmt=None, style='%', validate=True):\n        \"\"\"\n        Initialize the formatter with specified format strings.\n\n        Initialize the formatter either with the specified format string, or a\n        default as described above. Allow for specialized date formatting with\n        the optional datefmt argument. If datefmt is omitted, you get an\n        ISO8601-like (or RFC 3339-like) format.\n\n        Use a style parameter of '%', '{' or '$' to specify that you want to\n        use one of %-formatting, :meth:`str.format` (``{}``) formatting or\n        :class:`string.Template` formatting in your format string.\n\n        .. versionchanged:: 3.2\n           Added the ``style`` parameter.\n        \"\"\"\n        if style not in _STYLES:\n            raise ValueError('Style must be one of: %s' % ','.join(\n                             _STYLES.keys()))\n        self._style = _STYLES[style][0](fmt)\n        if validate:\n            self._style.validate()\n\n        self._fmt = self._style._fmt\n        self.datefmt = datefmt\n\n    default_time_format = '%Y-%m-%d %H:%M:%S'\n    default_msec_format = '%s,%03d'\n\n    def formatTime(self, record, datefmt=None):\n        \"\"\"\n        Return the creation time of the specified LogRecord as formatted text.\n\n        This method should be called from format() by a formatter which\n        wants to make use of a formatted time. This method can be overridden\n        in formatters to provide for any specific requirement, but the\n        basic behaviour is as follows: if datefmt (a string) is specified,\n        it is used with time.strftime() to format the creation time of the\n        record. Otherwise, an ISO8601-like (or RFC 3339-like) format is used.\n        The resulting string is returned. This function uses a user-configurable\n        function to convert the creation time to a tuple. By default,\n        time.localtime() is used; to change this for a particular formatter\n        instance, set the 'converter' attribute to a function with the same\n        signature as time.localtime() or time.gmtime(). To change it for all\n        formatters, for example if you want all logging times to be shown in GMT,\n        set the 'converter' attribute in the Formatter class.\n        \"\"\"\n        ct = self.converter(record.created)\n        if datefmt:\n            s = time.strftime(datefmt, ct)\n        else:\n            t = time.strftime(self.default_time_format, ct)\n            s = self.default_msec_format % (t, record.msecs)\n        return s\n\n    def formatException(self, ei):\n        \"\"\"\n        Format and return the specified exception information as a string.\n\n        This default implementation just uses\n        traceback.print_exception()\n        \"\"\"\n        sio = io.StringIO()\n        tb = ei[2]\n        # See issues #9427, #1553375. Commented out for now.\n        #if getattr(self, 'fullstack', False):\n        #    traceback.print_stack(tb.tb_frame.f_back, file=sio)\n        traceback.print_exception(ei[0], ei[1], tb, None, sio)\n        s = sio.getvalue()\n        sio.close()\n        if s[-1:] == \"\\n\":\n            s = s[:-1]\n        return s\n\n    def usesTime(self):\n        \"\"\"\n        Check if the format uses the creation time of the record.\n        \"\"\"\n        return self._style.usesTime()\n\n    def formatMessage(self, record):\n        return self._style.format(record)\n\n    def formatStack(self, stack_info):\n        \"\"\"\n        This method is provided as an extension point for specialized\n        formatting of stack information.\n\n        The input data is a string as returned from a call to\n        :func:`traceback.print_stack`, but with the last trailing newline\n        removed.\n\n        The base implementation just returns the value passed in.\n        \"\"\"\n        return stack_info\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record as text.\n\n        The record's attribute dictionary is used as the operand to a\n        string formatting operation which yields the returned string.\n        Before formatting the dictionary, a couple of preparatory steps\n        are carried out. The message attribute of the record is computed\n        using LogRecord.getMessage(). If the formatting string uses the\n        time (as determined by a call to usesTime(), formatTime() is\n        called to format the event time. If there is exception information,\n        it is formatted using formatException() and appended to the message.\n        \"\"\"\n        record.message = record.getMessage()\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        s = self.formatMessage(record)\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + record.exc_text\n        if record.stack_info:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + self.formatStack(record.stack_info)\n        return s\n\n#\n#   The default formatter to use when no other is specified\n#\n_defaultFormatter = Formatter()\n\nclass BufferingFormatter(object):\n    \"\"\"\n    A formatter suitable for formatting a number of records.\n    \"\"\"\n    def __init__(self, linefmt=None):\n        \"\"\"\n        Optionally specify a formatter which will be used to format each\n        individual record.\n        \"\"\"\n        if linefmt:\n            self.linefmt = linefmt\n        else:\n            self.linefmt = _defaultFormatter\n\n    def formatHeader(self, records):\n        \"\"\"\n        Return the header string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def formatFooter(self, records):\n        \"\"\"\n        Return the footer string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def format(self, records):\n        \"\"\"\n        Format the specified records and return the result as a string.\n        \"\"\"\n        rv = \"\"\n        if len(records) > 0:\n            rv = rv + self.formatHeader(records)\n            for record in records:\n                rv = rv + self.linefmt.format(record)\n            rv = rv + self.formatFooter(records)\n        return rv\n\n#---------------------------------------------------------------------------\n#   Filter classes and functions\n#---------------------------------------------------------------------------\n\nclass Filter(object):\n    \"\"\"\n    Filter instances are used to perform arbitrary filtering of LogRecords.\n\n    Loggers and Handlers can optionally use Filter instances to filter\n    records as desired. The base filter class only allows events which are\n    below a certain point in the logger hierarchy. For example, a filter\n    initialized with \"A.B\" will allow events logged by loggers \"A.B\",\n    \"A.B.C\", \"A.B.C.D\", \"A.B.D\" etc. but not \"A.BB\", \"B.A.B\" etc. If\n    initialized with the empty string, all events are passed.\n    \"\"\"\n    def __init__(self, name=''):\n        \"\"\"\n        Initialize a filter.\n\n        Initialize with the name of the logger which, together with its\n        children, will have its events allowed through the filter. If no\n        name is specified, allow every event.\n        \"\"\"\n        self.name = name\n        self.nlen = len(name)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if the specified record is to be logged.\n\n        Is the specified record to be logged? Returns 0 for no, nonzero for\n        yes. If deemed appropriate, the record may be modified in-place.\n        \"\"\"\n        if self.nlen == 0:\n            return True\n        elif self.name == record.name:\n            return True\n        elif record.name.find(self.name, 0, self.nlen) != 0:\n            return False\n        return (record.name[self.nlen] == \".\")\n\nclass Filterer(object):\n    \"\"\"\n    A base class for loggers and handlers which allows them to share\n    common code.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the list of filters to be an empty list.\n        \"\"\"\n        self.filters = []\n\n    def addFilter(self, filter):\n        \"\"\"\n        Add the specified filter to this handler.\n        \"\"\"\n        if not (filter in self.filters):\n            self.filters.append(filter)\n\n    def removeFilter(self, filter):\n        \"\"\"\n        Remove the specified filter from this handler.\n        \"\"\"\n        if filter in self.filters:\n            self.filters.remove(filter)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if a record is loggable by consulting all the filters.\n\n        The default is to allow the record to be logged; any filter can veto\n        this and the record is then dropped. Returns a zero value if a record\n        is to be dropped, else non-zero.\n\n        .. versionchanged:: 3.2\n\n           Allow filters to be just callables.\n        \"\"\"\n        rv = True\n        for f in self.filters:\n            if hasattr(f, 'filter'):\n                result = f.filter(record)\n            else:\n                result = f(record) # assume callable - will raise if not\n            if not result:\n                rv = False\n                break\n        return rv\n\n#---------------------------------------------------------------------------\n#   Handler classes and functions\n#---------------------------------------------------------------------------\n\n_handlers = weakref.WeakValueDictionary()  #map of handler names to handlers\n_handlerList = [] # added to allow handlers to be removed in reverse of order initialized\n\ndef _removeHandlerRef(wr):\n    \"\"\"\n    Remove a handler reference from the internal cleanup list.\n    \"\"\"\n    # This function can be called during module teardown, when globals are\n    # set to None. It can also be called from another thread. So we need to\n    # pre-emptively grab the necessary globals and check if they're None,\n    # to prevent race conditions and failures during interpreter shutdown.\n    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList\n    if acquire and release and handlers:\n        acquire()\n        try:\n            if wr in handlers:\n                handlers.remove(wr)\n        finally:\n            release()\n\ndef _addHandlerRef(handler):\n    \"\"\"\n    Add a handler to the internal cleanup list using a weak reference.\n    \"\"\"\n    _acquireLock()\n    try:\n        _handlerList.append(weakref.ref(handler, _removeHandlerRef))\n    finally:\n        _releaseLock()\n\nclass Handler(Filterer):\n    \"\"\"\n    Handler instances dispatch logging events to specific destinations.\n\n    The base handler class. Acts as a placeholder which defines the Handler\n    interface. Handlers can optionally use Formatter instances to format\n    records as desired. By default, no formatter is specified; in this case,\n    the 'raw' message as determined by record.message is logged.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initializes the instance - basically setting the formatter to None\n        and the filter list to empty.\n        \"\"\"\n        Filterer.__init__(self)\n        self._name = None\n        self.level = _checkLevel(level)\n        self.formatter = None\n        # Add the handler to the global _handlerList (for cleanup on shutdown)\n        _addHandlerRef(self)\n        self.createLock()\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, name):\n        _acquireLock()\n        try:\n            if self._name in _handlers:\n                del _handlers[self._name]\n            self._name = name\n            if name:\n                _handlers[name] = self\n        finally:\n            _releaseLock()\n\n    name = property(get_name, set_name)\n\n    def createLock(self):\n        \"\"\"\n        Acquire a thread lock for serializing access to the underlying I/O.\n        \"\"\"\n        self.lock = threading.RLock()\n        _register_at_fork_reinit_lock(self)\n\n    def acquire(self):\n        \"\"\"\n        Acquire the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.acquire()\n\n    def release(self):\n        \"\"\"\n        Release the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.release()\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this handler.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record.\n\n        If a formatter is set, use it. Otherwise, use the default formatter\n        for the module.\n        \"\"\"\n        if self.formatter:\n            fmt = self.formatter\n        else:\n            fmt = _defaultFormatter\n        return fmt.format(record)\n\n    def emit(self, record):\n        \"\"\"\n        Do whatever it takes to actually log the specified logging record.\n\n        This version is intended to be implemented by subclasses and so\n        raises a NotImplementedError.\n        \"\"\"\n        raise NotImplementedError('emit must be implemented '\n                                  'by Handler subclasses')\n\n    def handle(self, record):\n        \"\"\"\n        Conditionally emit the specified logging record.\n\n        Emission depends on filters which may have been added to the handler.\n        Wrap the actual emission of the record with acquisition/release of\n        the I/O thread lock. Returns whether the filter passed the record for\n        emission.\n        \"\"\"\n        rv = self.filter(record)\n        if rv:\n            self.acquire()\n            try:\n                self.emit(record)\n            finally:\n                self.release()\n        return rv\n\n    def setFormatter(self, fmt):\n        \"\"\"\n        Set the formatter for this handler.\n        \"\"\"\n        self.formatter = fmt\n\n    def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed.\n\n        This version does nothing and is intended to be implemented by\n        subclasses.\n        \"\"\"\n        pass\n\n    def close(self):\n        \"\"\"\n        Tidy up any resources used by the handler.\n\n        This version removes the handler from an internal map of handlers,\n        _handlers, which is used for handler lookup by name. Subclasses\n        should ensure that this gets called from overridden close()\n        methods.\n        \"\"\"\n        #get the module data lock, as we're updating a shared structure.\n        _acquireLock()\n        try:    #unlikely to raise an exception, but you never know...\n            if self._name and self._name in _handlers:\n                del _handlers[self._name]\n        finally:\n            _releaseLock()\n\n    def handleError(self, record):\n        \"\"\"\n        Handle errors which occur during an emit() call.\n\n        This method should be called from handlers when an exception is\n        encountered during an emit() call. If raiseExceptions is false,\n        exceptions get silently ignored. This is what is mostly wanted\n        for a logging system - most users will not care about errors in\n        the logging system, they are more interested in application errors.\n        You could, however, replace this with a custom handler if you wish.\n        The record which was being processed is passed in to this method.\n        \"\"\"\n        if raiseExceptions and sys.stderr:  # see issue 13807\n            t, v, tb = sys.exc_info()\n            try:\n                sys.stderr.write('--- Logging error ---\\n')\n                traceback.print_exception(t, v, tb, None, sys.stderr)\n                sys.stderr.write('Call stack:\\n')\n                # Walk the stack frame up until we're out of logging,\n                # so as to print the calling context.\n                frame = tb.tb_frame\n                while (frame and os.path.dirname(frame.f_code.co_filename) ==\n                       __path__[0]):\n                    frame = frame.f_back\n                if frame:\n                    traceback.print_stack(frame, file=sys.stderr)\n                else:\n                    # couldn't find the right stack frame, for some reason\n                    sys.stderr.write('Logged from file %s, line %s\\n' % (\n                                     record.filename, record.lineno))\n                # Issue 18671: output logging message and arguments\n                try:\n                    sys.stderr.write('Message: %r\\n'\n                                     'Arguments: %s\\n' % (record.msg,\n                                                          record.args))\n                except RecursionError:  # See issue 36272\n                    raise\n                except Exception:\n                    sys.stderr.write('Unable to print the message and arguments'\n                                     ' - possible formatting error.\\nUse the'\n                                     ' traceback above to help find the error.\\n'\n                                    )\n            except OSError: #pragma: no cover\n                pass    # see issue 5971\n            finally:\n                del t, v, tb\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s (%s)>' % (self.__class__.__name__, level)\n\nclass StreamHandler(Handler):\n    \"\"\"\n    A handler class which writes logging records, appropriately formatted,\n    to a stream. Note that this class does not close the stream, as\n    sys.stdout or sys.stderr may be used.\n    \"\"\"\n\n    terminator = '\\n'\n\n    def __init__(self, stream=None):\n        \"\"\"\n        Initialize the handler.\n\n        If stream is not specified, sys.stderr is used.\n        \"\"\"\n        Handler.__init__(self)\n        if stream is None:\n            stream = sys.stderr\n        self.stream = stream\n\n    def flush(self):\n        \"\"\"\n        Flushes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream and hasattr(self.stream, \"flush\"):\n                self.stream.flush()\n        finally:\n            self.release()\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If a formatter is specified, it is used to format the record.\n        The record is then written to the stream with a trailing newline.  If\n        exception information is present, it is formatted using\n        traceback.print_exception and appended to the stream.  If the stream\n        has an 'encoding' attribute, it is used to determine how to do the\n        output to the stream.\n        \"\"\"\n        try:\n            msg = self.format(record)\n            stream = self.stream\n            # issue 35046: merged two stream.writes into one.\n            stream.write(msg + self.terminator)\n            self.flush()\n        except RecursionError:  # See issue 36272\n            raise\n        except Exception:\n            self.handleError(record)\n\n    def setStream(self, stream):\n        \"\"\"\n        Sets the StreamHandler's stream to the specified value,\n        if it is different.\n\n        Returns the old stream, if the stream was changed, or None\n        if it wasn't.\n        \"\"\"\n        if stream is self.stream:\n            result = None\n        else:\n            result = self.stream\n            self.acquire()\n            try:\n                self.flush()\n                self.stream = stream\n            finally:\n                self.release()\n        return result\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        name = getattr(self.stream, 'name', '')\n        #  bpo-36015: name can be an int\n        name = str(name)\n        if name:\n            name += ' '\n        return '<%s %s(%s)>' % (self.__class__.__name__, name, level)\n\n\nclass FileHandler(StreamHandler):\n    \"\"\"\n    A handler class which writes formatted logging records to disk files.\n    \"\"\"\n    def __init__(self, filename, mode='a', encoding=None, delay=False):\n        \"\"\"\n        Open the specified file and use it as the stream for logging.\n        \"\"\"\n        # Issue #27493: add support for Path objects to be passed in\n        filename = os.fspath(filename)\n        #keep the absolute path, otherwise derived classes which use this\n        #may come a cropper when the current directory changes\n        self.baseFilename = os.path.abspath(filename)\n        self.mode = mode\n        self.encoding = encoding\n        self.delay = delay\n        if delay:\n            #We don't open the stream, but we still need to call the\n            #Handler constructor to set level, formatter, lock etc.\n            Handler.__init__(self)\n            self.stream = None\n        else:\n            StreamHandler.__init__(self, self._open())\n\n    def close(self):\n        \"\"\"\n        Closes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            try:\n                if self.stream:\n                    try:\n                        self.flush()\n                    finally:\n                        stream = self.stream\n                        self.stream = None\n                        if hasattr(stream, \"close\"):\n                            stream.close()\n            finally:\n                # Issue #19523: call unconditionally to\n                # prevent a handler leak when delay is set\n                StreamHandler.close(self)\n        finally:\n            self.release()\n\n    def _open(self):\n        \"\"\"\n        Open the current base file with the (original) mode and encoding.\n        Return the resulting stream.\n        \"\"\"\n        return open(self.baseFilename, self.mode, encoding=self.encoding)\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If the stream was not opened because 'delay' was specified in the\n        constructor, open it before calling the superclass's emit.\n        \"\"\"\n        if self.stream is None:\n            self.stream = self._open()\n        StreamHandler.emit(self, record)\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.baseFilename, level)\n\n\nclass _StderrHandler(StreamHandler):\n    \"\"\"\n    This class is like a StreamHandler using sys.stderr, but always uses\n    whatever sys.stderr is currently set to rather than the value of\n    sys.stderr at handler construction time.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initialize the handler.\n        \"\"\"\n        Handler.__init__(self, level)\n\n    @property\n    def stream(self):\n        return sys.stderr\n\n\n_defaultLastResort = _StderrHandler(WARNING)\nlastResort = _defaultLastResort\n\n#---------------------------------------------------------------------------\n#   Manager classes and functions\n#---------------------------------------------------------------------------\n\nclass PlaceHolder(object):\n    \"\"\"\n    PlaceHolder instances are used in the Manager logger hierarchy to take\n    the place of nodes for which no loggers have been defined. This class is\n    intended for internal use only and not as part of the public API.\n    \"\"\"\n    def __init__(self, alogger):\n        \"\"\"\n        Initialize with the specified logger being a child of this placeholder.\n        \"\"\"\n        self.loggerMap = { alogger : None }\n\n    def append(self, alogger):\n        \"\"\"\n        Add the specified logger as a child of this placeholder.\n        \"\"\"\n        if alogger not in self.loggerMap:\n            self.loggerMap[alogger] = None\n\n#\n#   Determine which class to use when instantiating loggers.\n#\n\ndef setLoggerClass(klass):\n    \"\"\"\n    Set the class to be used when instantiating a logger. The class should\n    define __init__() such that only a name argument is required, and the\n    __init__() should call Logger.__init__()\n    \"\"\"\n    if klass != Logger:\n        if not issubclass(klass, Logger):\n            raise TypeError(\"logger not derived from logging.Logger: \"\n                            + klass.__name__)\n    global _loggerClass\n    _loggerClass = klass\n\ndef getLoggerClass():\n    \"\"\"\n    Return the class to be used when instantiating a logger.\n    \"\"\"\n    return _loggerClass\n\nclass Manager(object):\n    \"\"\"\n    There is [under normal circumstances] just one Manager instance, which\n    holds the hierarchy of loggers.\n    \"\"\"\n    def __init__(self, rootnode):\n        \"\"\"\n        Initialize the manager with the root node of the logger hierarchy.\n        \"\"\"\n        self.root = rootnode\n        self.disable = 0\n        self.emittedNoHandlerWarning = False\n        self.loggerDict = {}\n        self.loggerClass = None\n        self.logRecordFactory = None\n\n    def getLogger(self, name):\n        \"\"\"\n        Get a logger with the specified name (channel name), creating it\n        if it doesn't yet exist. This name is a dot-separated hierarchical\n        name, such as \"a\", \"a.b\", \"a.b.c\" or similar.\n\n        If a PlaceHolder existed for the specified name [i.e. the logger\n        didn't exist but a child of it did], replace it with the created\n        logger and fix up the parent/child references which pointed to the\n        placeholder to now point to the logger.\n        \"\"\"\n        rv = None\n        if not isinstance(name, str):\n            raise TypeError('A logger name must be a string')\n        _acquireLock()\n        try:\n            if name in self.loggerDict:\n                rv = self.loggerDict[name]\n                if isinstance(rv, PlaceHolder):\n                    ph = rv\n                    rv = (self.loggerClass or _loggerClass)(name)\n                    rv.manager = self\n                    self.loggerDict[name] = rv\n                    self._fixupChildren(ph, rv)\n                    self._fixupParents(rv)\n            else:\n                rv = (self.loggerClass or _loggerClass)(name)\n                rv.manager = self\n                self.loggerDict[name] = rv\n                self._fixupParents(rv)\n        finally:\n            _releaseLock()\n        return rv\n\n    def setLoggerClass(self, klass):\n        \"\"\"\n        Set the class to be used when instantiating a logger with this Manager.\n        \"\"\"\n        if klass != Logger:\n            if not issubclass(klass, Logger):\n                raise TypeError(\"logger not derived from logging.Logger: \"\n                                + klass.__name__)\n        self.loggerClass = klass\n\n    def setLogRecordFactory(self, factory):\n        \"\"\"\n        Set the factory to be used when instantiating a log record with this\n        Manager.\n        \"\"\"\n        self.logRecordFactory = factory\n\n    def _fixupParents(self, alogger):\n        \"\"\"\n        Ensure that there are either loggers or placeholders all the way\n        from the specified logger to the root of the logger hierarchy.\n        \"\"\"\n        name = alogger.name\n        i = name.rfind(\".\")\n        rv = None\n        while (i > 0) and not rv:\n            substr = name[:i]\n            if substr not in self.loggerDict:\n                self.loggerDict[substr] = PlaceHolder(alogger)\n            else:\n                obj = self.loggerDict[substr]\n                if isinstance(obj, Logger):\n                    rv = obj\n                else:\n                    assert isinstance(obj, PlaceHolder)\n                    obj.append(alogger)\n            i = name.rfind(\".\", 0, i - 1)\n        if not rv:\n            rv = self.root\n        alogger.parent = rv\n\n    def _fixupChildren(self, ph, alogger):\n        \"\"\"\n        Ensure that children of the placeholder ph are connected to the\n        specified logger.\n        \"\"\"\n        name = alogger.name\n        namelen = len(name)\n        for c in ph.loggerMap.keys():\n            #The if means ... if not c.parent.name.startswith(nm)\n            if c.parent.name[:namelen] != name:\n                alogger.parent = c.parent\n                c.parent = alogger\n\n    def _clear_cache(self):\n        \"\"\"\n        Clear the cache for all loggers in loggerDict\n        Called when level changes are made\n        \"\"\"\n\n        _acquireLock()\n        for logger in self.loggerDict.values():\n            if isinstance(logger, Logger):\n                logger._cache.clear()\n        self.root._cache.clear()\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n#   Logger classes and functions\n#---------------------------------------------------------------------------\n\nclass Logger(Filterer):\n    \"\"\"\n    Instances of the Logger class represent a single logging channel. A\n    \"logging channel\" indicates an area of an application. Exactly how an\n    \"area\" is defined is up to the application developer. Since an\n    application can have any number of areas, logging channels are identified\n    by a unique string. Application areas can be nested (e.g. an area\n    of \"input processing\" might include sub-areas \"read CSV files\", \"read\n    XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n    channel names are organized into a namespace hierarchy where levels are\n    separated by periods, much like the Java or Python package namespace. So\n    in the instance given above, channel names might be \"input\" for the upper\n    level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n    There is no arbitrary limit to the depth of nesting.\n    \"\"\"\n    def __init__(self, name, level=NOTSET):\n        \"\"\"\n        Initialize the logger with a name and an optional level.\n        \"\"\"\n        Filterer.__init__(self)\n        self.name = name\n        self.level = _checkLevel(level)\n        self.parent = None\n        self.propagate = True\n        self.handlers = []\n        self.disabled = False\n        self._cache = {}\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this logger.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n        self.manager._clear_cache()\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'DEBUG'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(DEBUG):\n            self._log(DEBUG, msg, args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(INFO):\n            self._log(INFO, msg, args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'WARNING'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(WARNING):\n            self._log(WARNING, msg, args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'ERROR'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.error(\"Houston, we have a %s\", \"major problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(ERROR):\n            self._log(ERROR, msg, args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Convenience method for logging an ERROR with exception information.\n        \"\"\"\n        self.error(msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'CRITICAL'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.critical(\"Houston, we have a %s\", \"major disaster\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(CRITICAL):\n            self._log(CRITICAL, msg, args, **kwargs)\n\n    fatal = critical\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with the integer severity 'level'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.log(level, \"We have a %s\", \"mysterious problem\", exc_info=1)\n        \"\"\"\n        if not isinstance(level, int):\n            if raiseExceptions:\n                raise TypeError(\"level must be an integer\")\n            else:\n                return\n        if self.isEnabledFor(level):\n            self._log(level, msg, args, **kwargs)\n\n    def findCaller(self, stack_info=False, stacklevel=1):\n        \"\"\"\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        #On some versions of IronPython, currentframe() returns None if\n        #IronPython isn't run with -X:Frames.\n        if f is not None:\n            f = f.f_back\n        orig_f = f\n        while f and stacklevel > 1:\n            f = f.f_back\n            stacklevel -= 1\n        if not f:\n            f = orig_f\n        rv = \"(unknown file)\", 0, \"(unknown function)\", None\n        while hasattr(f, \"f_code\"):\n            co = f.f_code\n            filename = os.path.normcase(co.co_filename)\n            if filename == _srcfile:\n                f = f.f_back\n                continue\n            sinfo = None\n            if stack_info:\n                sio = io.StringIO()\n                sio.write('Stack (most recent call last):\\n')\n                traceback.print_stack(f, file=sio)\n                sinfo = sio.getvalue()\n                if sinfo[-1] == '\\n':\n                    sinfo = sinfo[:-1]\n                sio.close()\n            rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)\n            break\n        return rv\n\n    def makeRecord(self, name, level, fn, lno, msg, args, exc_info,\n                   func=None, extra=None, sinfo=None):\n        \"\"\"\n        A factory method which can be overridden in subclasses to create\n        specialized LogRecords.\n        \"\"\"\n        rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n                             sinfo)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv\n\n    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False,\n             stacklevel=1):\n        \"\"\"\n        Low-level logging routine which creates a LogRecord and then calls\n        all the handlers of this logger to handle the record.\n        \"\"\"\n        sinfo = None\n        if _srcfile:\n            #IronPython doesn't track Python frames, so findCaller raises an\n            #exception on some versions of IronPython. We trap it here so that\n            #IronPython can use logging.\n            try:\n                fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n            except ValueError: # pragma: no cover\n                fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        else: # pragma: no cover\n            fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        if exc_info:\n            if isinstance(exc_info, BaseException):\n                exc_info = (type(exc_info), exc_info, exc_info.__traceback__)\n            elif not isinstance(exc_info, tuple):\n                exc_info = sys.exc_info()\n        record = self.makeRecord(self.name, level, fn, lno, msg, args,\n                                 exc_info, func, extra, sinfo)\n        self.handle(record)\n\n    def handle(self, record):\n        \"\"\"\n        Call the handlers for the specified record.\n\n        This method is used for unpickled records received from a socket, as\n        well as those created locally. Logger-level filtering is applied.\n        \"\"\"\n        if (not self.disabled) and self.filter(record):\n            self.callHandlers(record)\n\n    def addHandler(self, hdlr):\n        \"\"\"\n        Add the specified handler to this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if not (hdlr in self.handlers):\n                self.handlers.append(hdlr)\n        finally:\n            _releaseLock()\n\n    def removeHandler(self, hdlr):\n        \"\"\"\n        Remove the specified handler from this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if hdlr in self.handlers:\n                self.handlers.remove(hdlr)\n        finally:\n            _releaseLock()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if this logger has any handlers configured.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. Return True if a handler was found, else False.\n        Stop searching up the hierarchy whenever a logger with the \"propagate\"\n        attribute set to zero is found - that will be the last logger which\n        is checked for the existence of handlers.\n        \"\"\"\n        c = self\n        rv = False\n        while c:\n            if c.handlers:\n                rv = True\n                break\n            if not c.propagate:\n                break\n            else:\n                c = c.parent\n        return rv\n\n    def callHandlers(self, record):\n        \"\"\"\n        Pass a record to all relevant handlers.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. If no handler was found, output a one-off error\n        message to sys.stderr. Stop searching up the hierarchy whenever a\n        logger with the \"propagate\" attribute set to zero is found - that\n        will be the last logger whose handlers are called.\n        \"\"\"\n        c = self\n        found = 0\n        while c:\n            for hdlr in c.handlers:\n                found = found + 1\n                if record.levelno >= hdlr.level:\n                    hdlr.handle(record)\n            if not c.propagate:\n                c = None    #break out\n            else:\n                c = c.parent\n        if (found == 0):\n            if lastResort:\n                if record.levelno >= lastResort.level:\n                    lastResort.handle(record)\n            elif raiseExceptions and not self.manager.emittedNoHandlerWarning:\n                sys.stderr.write(\"No handlers could be found for logger\"\n                                 \" \\\"%s\\\"\\n\" % self.name)\n                self.manager.emittedNoHandlerWarning = True\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for this logger.\n\n        Loop through this logger and its parents in the logger hierarchy,\n        looking for a non-zero logging level. Return the first one found.\n        \"\"\"\n        logger = self\n        while logger:\n            if logger.level:\n                return logger.level\n            logger = logger.parent\n        return NOTSET\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        if self.disabled:\n            return False\n\n        try:\n            return self._cache[level]\n        except KeyError:\n            _acquireLock()\n            try:\n                if self.manager.disable >= level:\n                    is_enabled = self._cache[level] = False\n                else:\n                    is_enabled = self._cache[level] = (\n                        level >= self.getEffectiveLevel()\n                    )\n            finally:\n                _releaseLock()\n            return is_enabled\n\n    def getChild(self, suffix):\n        \"\"\"\n        Get a logger which is a descendant to this one.\n\n        This is a convenience method, such that\n\n        logging.getLogger('abc').getChild('def.ghi')\n\n        is the same as\n\n        logging.getLogger('abc.def.ghi')\n\n        It's useful, for example, when the parent logger is named using\n        __name__ rather than a literal string.\n        \"\"\"\n        if self.root is not self:\n            suffix = '.'.join((self.name, suffix))\n        return self.manager.getLogger(suffix)\n\n    def __repr__(self):\n        level = getLevelName(self.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.name, level)\n\n    def __reduce__(self):\n        # In general, only the root logger will not be accessible via its name.\n        # However, the root logger's class has its own __reduce__ method.\n        if getLogger(self.name) is not self:\n            import pickle\n            raise pickle.PicklingError('logger cannot be pickled')\n        return getLogger, (self.name,)\n\n\nclass RootLogger(Logger):\n    \"\"\"\n    A root logger is not that different to any other logger, except that\n    it must have a logging level and there is only one instance of it in\n    the hierarchy.\n    \"\"\"\n    def __init__(self, level):\n        \"\"\"\n        Initialize the logger with the name \"root\".\n        \"\"\"\n        Logger.__init__(self, \"root\", level)\n\n    def __reduce__(self):\n        return getLogger, ()\n\n_loggerClass = Logger\n\nclass LoggerAdapter(object):\n    \"\"\"\n    An adapter for loggers which makes it easier to specify contextual\n    information in logging output.\n    \"\"\"\n\n    def __init__(self, logger, extra):\n        \"\"\"\n        Initialize the adapter with a logger and a dict-like object which\n        provides contextual information. This constructor signature allows\n        easy stacking of LoggerAdapters, if so desired.\n\n        You can effectively pass keyword arguments as shown in the\n        following example:\n\n        adapter = LoggerAdapter(someLogger, dict(p1=v1, p2=\"v2\"))\n        \"\"\"\n        self.logger = logger\n        self.extra = extra\n\n    def process(self, msg, kwargs):\n        \"\"\"\n        Process the logging message and keyword arguments passed in to\n        a logging call to insert contextual information. You can either\n        manipulate the message itself, the keyword args or both. Return\n        the message and kwargs modified (or not) to suit your needs.\n\n        Normally, you'll only need to override this one method in a\n        LoggerAdapter subclass for your specific needs.\n        \"\"\"\n        kwargs[\"extra\"] = self.extra\n        return msg, kwargs\n\n    #\n    # Boilerplate convenience methods\n    #\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a debug call to the underlying logger.\n        \"\"\"\n        self.log(DEBUG, msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an info call to the underlying logger.\n        \"\"\"\n        self.log(INFO, msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a warning call to the underlying logger.\n        \"\"\"\n        self.log(WARNING, msg, *args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an error call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Delegate an exception call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a critical call to the underlying logger.\n        \"\"\"\n        self.log(CRITICAL, msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a log call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        if self.isEnabledFor(level):\n            msg, kwargs = self.process(msg, kwargs)\n            self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        return self.logger.isEnabledFor(level)\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the specified level on the underlying logger.\n        \"\"\"\n        self.logger.setLevel(level)\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for the underlying logger.\n        \"\"\"\n        return self.logger.getEffectiveLevel()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if the underlying logger has any handlers.\n        \"\"\"\n        return self.logger.hasHandlers()\n\n    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False):\n        \"\"\"\n        Low-level log implementation, proxied to allow nested logger adapters.\n        \"\"\"\n        return self.logger._log(\n            level,\n            msg,\n            args,\n            exc_info=exc_info,\n            extra=extra,\n            stack_info=stack_info,\n        )\n\n    @property\n    def manager(self):\n        return self.logger.manager\n\n    @manager.setter\n    def manager(self, value):\n        self.logger.manager = value\n\n    @property\n    def name(self):\n        return self.logger.name\n\n    def __repr__(self):\n        logger = self.logger\n        level = getLevelName(logger.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, logger.name, level)\n\nroot = RootLogger(WARNING)\nLogger.root = root\nLogger.manager = Manager(Logger.root)\n\n#---------------------------------------------------------------------------\n# Configuration classes and functions\n#---------------------------------------------------------------------------\n\ndef basicConfig(**kwargs):\n    \"\"\"\n    Do basic configuration for the logging system.\n\n    This function does nothing if the root logger already has handlers\n    configured, unless the keyword argument *force* is set to ``True``.\n    It is a convenience method intended for use by simple scripts\n    to do one-shot configuration of the logging package.\n\n    The default behaviour is to create a StreamHandler which writes to\n    sys.stderr, set a formatter using the BASIC_FORMAT format string, and\n    add the handler to the root logger.\n\n    A number of optional keyword arguments may be specified, which can alter\n    the default behaviour.\n\n    filename  Specifies that a FileHandler be created, using the specified\n              filename, rather than a StreamHandler.\n    filemode  Specifies the mode to open the file, if filename is specified\n              (if filemode is unspecified, it defaults to 'a').\n    format    Use the specified format string for the handler.\n    datefmt   Use the specified date/time format.\n    style     If a format string is specified, use this to specify the\n              type of format string (possible values '%', '{', '$', for\n              %-formatting, :meth:`str.format` and :class:`string.Template`\n              - defaults to '%').\n    level     Set the root logger level to the specified level.\n    stream    Use the specified stream to initialize the StreamHandler. Note\n              that this argument is incompatible with 'filename' - if both\n              are present, 'stream' is ignored.\n    handlers  If specified, this should be an iterable of already created\n              handlers, which will be added to the root handler. Any handler\n              in the list which does not have a formatter assigned will be\n              assigned the formatter created in this function.\n    force     If this keyword  is specified as true, any existing handlers\n              attached to the root logger are removed and closed, before\n              carrying out the configuration as specified by the other\n              arguments.\n    Note that you could specify a stream created using open(filename, mode)\n    rather than passing the filename and mode in. However, it should be\n    remembered that StreamHandler does not close its stream (since it may be\n    using sys.stdout or sys.stderr), whereas FileHandler closes its stream\n    when the handler is closed.\n\n    .. versionchanged:: 3.8\n       Added the ``force`` parameter.\n\n    .. versionchanged:: 3.2\n       Added the ``style`` parameter.\n\n    .. versionchanged:: 3.3\n       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for\n       incompatible arguments (e.g. ``handlers`` specified together with\n       ``filename``/``filemode``, or ``filename``/``filemode`` specified\n       together with ``stream``, or ``handlers`` specified together with\n       ``stream``.\n    \"\"\"\n    # Add thread safety in case someone mistakenly calls\n    # basicConfig() from multiple threads\n    _acquireLock()\n    try:\n        force = kwargs.pop('force', False)\n        if force:\n            for h in root.handlers[:]:\n                root.removeHandler(h)\n                h.close()\n        if len(root.handlers) == 0:\n            handlers = kwargs.pop(\"handlers\", None)\n            if handlers is None:\n                if \"stream\" in kwargs and \"filename\" in kwargs:\n                    raise ValueError(\"'stream' and 'filename' should not be \"\n                                     \"specified together\")\n            else:\n                if \"stream\" in kwargs or \"filename\" in kwargs:\n                    raise ValueError(\"'stream' or 'filename' should not be \"\n                                     \"specified together with 'handlers'\")\n            if handlers is None:\n                filename = kwargs.pop(\"filename\", None)\n                mode = kwargs.pop(\"filemode\", 'a')\n                if filename:\n                    h = FileHandler(filename, mode)\n                else:\n                    stream = kwargs.pop(\"stream\", None)\n                    h = StreamHandler(stream)\n                handlers = [h]\n            dfs = kwargs.pop(\"datefmt\", None)\n            style = kwargs.pop(\"style\", '%')\n            if style not in _STYLES:\n                raise ValueError('Style must be one of: %s' % ','.join(\n                                 _STYLES.keys()))\n            fs = kwargs.pop(\"format\", _STYLES[style][1])\n            fmt = Formatter(fs, dfs, style)\n            for h in handlers:\n                if h.formatter is None:\n                    h.setFormatter(fmt)\n                root.addHandler(h)\n            level = kwargs.pop(\"level\", None)\n            if level is not None:\n                root.setLevel(level)\n            if kwargs:\n                keys = ', '.join(kwargs.keys())\n                raise ValueError('Unrecognised argument(s): %s' % keys)\n    finally:\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n# Utility functions at module level.\n# Basically delegate everything to the root logger.\n#---------------------------------------------------------------------------\n\ndef getLogger(name=None):\n    \"\"\"\n    Return a logger with the specified name, creating it if necessary.\n\n    If no name is specified, return the root logger.\n    \"\"\"\n    if name:\n        return Logger.manager.getLogger(name)\n    else:\n        return root\n\ndef critical(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'CRITICAL' on the root logger. If the logger\n    has no handlers, call basicConfig() to add a console handler with a\n    pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.critical(msg, *args, **kwargs)\n\nfatal = critical\n\ndef error(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.error(msg, *args, **kwargs)\n\ndef exception(msg, *args, exc_info=True, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger, with exception\n    information. If the logger has no handlers, basicConfig() is called to add\n    a console handler with a pre-defined format.\n    \"\"\"\n    error(msg, *args, exc_info=exc_info, **kwargs)\n\ndef warning(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'WARNING' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.warning(msg, *args, **kwargs)\n\ndef warn(msg, *args, **kwargs):\n    warnings.warn(\"The 'warn' function is deprecated, \"\n        \"use 'warning' instead\", DeprecationWarning, 2)\n    warning(msg, *args, **kwargs)\n\ndef info(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'INFO' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.info(msg, *args, **kwargs)\n\ndef debug(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'DEBUG' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.debug(msg, *args, **kwargs)\n\ndef log(level, msg, *args, **kwargs):\n    \"\"\"\n    Log 'msg % args' with the integer severity 'level' on the root logger. If\n    the logger has no handlers, call basicConfig() to add a console handler\n    with a pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.log(level, msg, *args, **kwargs)\n\ndef disable(level=CRITICAL):\n    \"\"\"\n    Disable all logging calls of severity 'level' and below.\n    \"\"\"\n    root.manager.disable = level\n    root.manager._clear_cache()\n\ndef shutdown(handlerList=_handlerList):\n    \"\"\"\n    Perform any cleanup actions in the logging system (e.g. flushing\n    buffers).\n\n    Should be called at application exit.\n    \"\"\"\n    for wr in reversed(handlerList[:]):\n        #errors might occur, for example, if files are locked\n        #we just ignore them if raiseExceptions is not set\n        try:\n            h = wr()\n            if h:\n                try:\n                    h.acquire()\n                    h.flush()\n                    h.close()\n                except (OSError, ValueError):\n                    # Ignore errors which might be caused\n                    # because handlers have been closed but\n                    # references to them are still around at\n                    # application exit.\n                    pass\n                finally:\n                    h.release()\n        except: # ignore everything, as we're shutting down\n            if raiseExceptions:\n                raise\n            #else, swallow\n\n#Let's try and shutdown automatically on application exit...\nimport atexit\natexit.register(shutdown)\n\n# Null handler\n\nclass NullHandler(Handler):\n    \"\"\"\n    This handler does nothing. It's intended to be used to avoid the\n    \"No handlers could be found for logger XXX\" one-off warning. This is\n    important for library code, which may contain code to log events. If a user\n    of the library does not configure logging, the one-off warning might be\n    produced; to avoid this, the library developer simply needs to instantiate\n    a NullHandler and add it to the top-level logger of the library module or\n    package.\n    \"\"\"\n    def handle(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def emit(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def createLock(self):\n        self.lock = None\n\n# Warnings integration\n\n_warnings_showwarning = None\n\ndef _showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"\n    Implementation of showwarnings which redirects to logging, which will first\n    check to see if the file parameter is None. If a file is specified, it will\n    delegate to the original warnings implementation of showwarning. Otherwise,\n    it will call warnings.formatwarning and will log the resulting string to a\n    warnings logger named \"py.warnings\" with level logging.WARNING.\n    \"\"\"\n    if file is not None:\n        if _warnings_showwarning is not None:\n            _warnings_showwarning(message, category, filename, lineno, file, line)\n    else:\n        s = warnings.formatwarning(message, category, filename, lineno, line)\n        logger = getLogger(\"py.warnings\")\n        if not logger.handlers:\n            logger.addHandler(NullHandler())\n        logger.warning(\"%s\", s)\n\ndef captureWarnings(capture):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n    global _warnings_showwarning\n    if capture:\n        if _warnings_showwarning is None:\n            _warnings_showwarning = warnings.showwarning\n            warnings.showwarning = _showwarning\n    else:\n        if _warnings_showwarning is not None:\n            warnings.showwarning = _warnings_showwarning\n            _warnings_showwarning = None\n", 2190], "/usr/lib/python3.8/socket.py": ["# Wrapper module for _socket, providing some additional facilities\n# implemented in Python.\n\n\"\"\"\\\nThis module provides socket operations and some related functions.\nOn Unix, it supports IP (Internet Protocol) and Unix domain sockets.\nOn other systems, it only supports IP. Functions specific for a\nsocket are available as methods of the socket object.\n\nFunctions:\n\nsocket() -- create a new socket object\nsocketpair() -- create a pair of new socket objects [*]\nfromfd() -- create a socket object from an open file descriptor [*]\nfromshare() -- create a socket object from data received from socket.share() [*]\ngethostname() -- return the current hostname\ngethostbyname() -- map a hostname to its IP number\ngethostbyaddr() -- map an IP number or hostname to DNS info\ngetservbyname() -- map a service name and a protocol name to a port number\ngetprotobyname() -- map a protocol name (e.g. 'tcp') to a number\nntohs(), ntohl() -- convert 16, 32 bit int from network to host byte order\nhtons(), htonl() -- convert 16, 32 bit int from host to network byte order\ninet_aton() -- convert IP addr string (123.45.67.89) to 32-bit packed format\ninet_ntoa() -- convert 32-bit packed format IP to string (123.45.67.89)\nsocket.getdefaulttimeout() -- get the default timeout value\nsocket.setdefaulttimeout() -- set the default timeout value\ncreate_connection() -- connects to an address, with an optional timeout and\n                       optional source address.\n\n [*] not available on all platforms!\n\nSpecial objects:\n\nSocketType -- type object for socket objects\nerror -- exception raised for I/O errors\nhas_ipv6 -- boolean value indicating if IPv6 is supported\n\nIntEnum constants:\n\nAF_INET, AF_UNIX -- socket domains (first argument to socket() call)\nSOCK_STREAM, SOCK_DGRAM, SOCK_RAW -- socket types (second argument)\n\nInteger constants:\n\nMany other constants may be defined; these may be used in calls to\nthe setsockopt() and getsockopt() methods.\n\"\"\"\n\nimport _socket\nfrom _socket import *\n\nimport os, sys, io, selectors\nfrom enum import IntEnum, IntFlag\n\ntry:\n    import errno\nexcept ImportError:\n    errno = None\nEBADF = getattr(errno, 'EBADF', 9)\nEAGAIN = getattr(errno, 'EAGAIN', 11)\nEWOULDBLOCK = getattr(errno, 'EWOULDBLOCK', 11)\n\n__all__ = [\"fromfd\", \"getfqdn\", \"create_connection\", \"create_server\",\n           \"has_dualstack_ipv6\", \"AddressFamily\", \"SocketKind\"]\n__all__.extend(os._get_exports_list(_socket))\n\n# Set up the socket.AF_* socket.SOCK_* constants as members of IntEnums for\n# nicer string representations.\n# Note that _socket only knows about the integer values. The public interface\n# in this module understands the enums and translates them back from integers\n# where needed (e.g. .family property of a socket object).\n\nIntEnum._convert_(\n        'AddressFamily',\n        __name__,\n        lambda C: C.isupper() and C.startswith('AF_'))\n\nIntEnum._convert_(\n        'SocketKind',\n        __name__,\n        lambda C: C.isupper() and C.startswith('SOCK_'))\n\nIntFlag._convert_(\n        'MsgFlag',\n        __name__,\n        lambda C: C.isupper() and C.startswith('MSG_'))\n\nIntFlag._convert_(\n        'AddressInfo',\n        __name__,\n        lambda C: C.isupper() and C.startswith('AI_'))\n\n_LOCALHOST    = '127.0.0.1'\n_LOCALHOST_V6 = '::1'\n\n\ndef _intenum_converter(value, enum_klass):\n    \"\"\"Convert a numeric family value to an IntEnum member.\n\n    If it's not a known member, return the numeric value itself.\n    \"\"\"\n    try:\n        return enum_klass(value)\n    except ValueError:\n        return value\n\n_realsocket = socket\n\n# WSA error codes\nif sys.platform.lower().startswith(\"win\"):\n    errorTab = {}\n    errorTab[6] = \"Specified event object handle is invalid.\"\n    errorTab[8] = \"Insufficient memory available.\"\n    errorTab[87] = \"One or more parameters are invalid.\"\n    errorTab[995] = \"Overlapped operation aborted.\"\n    errorTab[996] = \"Overlapped I/O event object not in signaled state.\"\n    errorTab[997] = \"Overlapped operation will complete later.\"\n    errorTab[10004] = \"The operation was interrupted.\"\n    errorTab[10009] = \"A bad file handle was passed.\"\n    errorTab[10013] = \"Permission denied.\"\n    errorTab[10014] = \"A fault occurred on the network??\"  # WSAEFAULT\n    errorTab[10022] = \"An invalid operation was attempted.\"\n    errorTab[10024] = \"Too many open files.\"\n    errorTab[10035] = \"The socket operation would block\"\n    errorTab[10036] = \"A blocking operation is already in progress.\"\n    errorTab[10037] = \"Operation already in progress.\"\n    errorTab[10038] = \"Socket operation on nonsocket.\"\n    errorTab[10039] = \"Destination address required.\"\n    errorTab[10040] = \"Message too long.\"\n    errorTab[10041] = \"Protocol wrong type for socket.\"\n    errorTab[10042] = \"Bad protocol option.\"\n    errorTab[10043] = \"Protocol not supported.\"\n    errorTab[10044] = \"Socket type not supported.\"\n    errorTab[10045] = \"Operation not supported.\"\n    errorTab[10046] = \"Protocol family not supported.\"\n    errorTab[10047] = \"Address family not supported by protocol family.\"\n    errorTab[10048] = \"The network address is in use.\"\n    errorTab[10049] = \"Cannot assign requested address.\"\n    errorTab[10050] = \"Network is down.\"\n    errorTab[10051] = \"Network is unreachable.\"\n    errorTab[10052] = \"Network dropped connection on reset.\"\n    errorTab[10053] = \"Software caused connection abort.\"\n    errorTab[10054] = \"The connection has been reset.\"\n    errorTab[10055] = \"No buffer space available.\"\n    errorTab[10056] = \"Socket is already connected.\"\n    errorTab[10057] = \"Socket is not connected.\"\n    errorTab[10058] = \"The network has been shut down.\"\n    errorTab[10059] = \"Too many references.\"\n    errorTab[10060] = \"The operation timed out.\"\n    errorTab[10061] = \"Connection refused.\"\n    errorTab[10062] = \"Cannot translate name.\"\n    errorTab[10063] = \"The name is too long.\"\n    errorTab[10064] = \"The host is down.\"\n    errorTab[10065] = \"The host is unreachable.\"\n    errorTab[10066] = \"Directory not empty.\"\n    errorTab[10067] = \"Too many processes.\"\n    errorTab[10068] = \"User quota exceeded.\"\n    errorTab[10069] = \"Disk quota exceeded.\"\n    errorTab[10070] = \"Stale file handle reference.\"\n    errorTab[10071] = \"Item is remote.\"\n    errorTab[10091] = \"Network subsystem is unavailable.\"\n    errorTab[10092] = \"Winsock.dll version out of range.\"\n    errorTab[10093] = \"Successful WSAStartup not yet performed.\"\n    errorTab[10101] = \"Graceful shutdown in progress.\"\n    errorTab[10102] = \"No more results from WSALookupServiceNext.\"\n    errorTab[10103] = \"Call has been canceled.\"\n    errorTab[10104] = \"Procedure call table is invalid.\"\n    errorTab[10105] = \"Service provider is invalid.\"\n    errorTab[10106] = \"Service provider failed to initialize.\"\n    errorTab[10107] = \"System call failure.\"\n    errorTab[10108] = \"Service not found.\"\n    errorTab[10109] = \"Class type not found.\"\n    errorTab[10110] = \"No more results from WSALookupServiceNext.\"\n    errorTab[10111] = \"Call was canceled.\"\n    errorTab[10112] = \"Database query was refused.\"\n    errorTab[11001] = \"Host not found.\"\n    errorTab[11002] = \"Nonauthoritative host not found.\"\n    errorTab[11003] = \"This is a nonrecoverable error.\"\n    errorTab[11004] = \"Valid name, no data record requested type.\"\n    errorTab[11005] = \"QoS receivers.\"\n    errorTab[11006] = \"QoS senders.\"\n    errorTab[11007] = \"No QoS senders.\"\n    errorTab[11008] = \"QoS no receivers.\"\n    errorTab[11009] = \"QoS request confirmed.\"\n    errorTab[11010] = \"QoS admission error.\"\n    errorTab[11011] = \"QoS policy failure.\"\n    errorTab[11012] = \"QoS bad style.\"\n    errorTab[11013] = \"QoS bad object.\"\n    errorTab[11014] = \"QoS traffic control error.\"\n    errorTab[11015] = \"QoS generic error.\"\n    errorTab[11016] = \"QoS service type error.\"\n    errorTab[11017] = \"QoS flowspec error.\"\n    errorTab[11018] = \"Invalid QoS provider buffer.\"\n    errorTab[11019] = \"Invalid QoS filter style.\"\n    errorTab[11020] = \"Invalid QoS filter style.\"\n    errorTab[11021] = \"Incorrect QoS filter count.\"\n    errorTab[11022] = \"Invalid QoS object length.\"\n    errorTab[11023] = \"Incorrect QoS flow count.\"\n    errorTab[11024] = \"Unrecognized QoS object.\"\n    errorTab[11025] = \"Invalid QoS policy object.\"\n    errorTab[11026] = \"Invalid QoS flow descriptor.\"\n    errorTab[11027] = \"Invalid QoS provider-specific flowspec.\"\n    errorTab[11028] = \"Invalid QoS provider-specific filterspec.\"\n    errorTab[11029] = \"Invalid QoS shape discard mode object.\"\n    errorTab[11030] = \"Invalid QoS shaping rate object.\"\n    errorTab[11031] = \"Reserved policy QoS element type.\"\n    __all__.append(\"errorTab\")\n\n\nclass _GiveupOnSendfile(Exception): pass\n\n\nclass socket(_socket.socket):\n\n    \"\"\"A subclass of _socket.socket adding the makefile() method.\"\"\"\n\n    __slots__ = [\"__weakref__\", \"_io_refs\", \"_closed\"]\n\n    def __init__(self, family=-1, type=-1, proto=-1, fileno=None):\n        # For user code address family and type values are IntEnum members, but\n        # for the underlying _socket.socket they're just integers. The\n        # constructor of _socket.socket converts the given argument to an\n        # integer automatically.\n        if fileno is None:\n            if family == -1:\n                family = AF_INET\n            if type == -1:\n                type = SOCK_STREAM\n            if proto == -1:\n                proto = 0\n        _socket.socket.__init__(self, family, type, proto, fileno)\n        self._io_refs = 0\n        self._closed = False\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        if not self._closed:\n            self.close()\n\n    def __repr__(self):\n        \"\"\"Wrap __repr__() to reveal the real class name and socket\n        address(es).\n        \"\"\"\n        closed = getattr(self, '_closed', False)\n        s = \"<%s.%s%s fd=%i, family=%s, type=%s, proto=%i\" \\\n            % (self.__class__.__module__,\n               self.__class__.__qualname__,\n               \" [closed]\" if closed else \"\",\n               self.fileno(),\n               self.family,\n               self.type,\n               self.proto)\n        if not closed:\n            try:\n                laddr = self.getsockname()\n                if laddr:\n                    s += \", laddr=%s\" % str(laddr)\n            except error:\n                pass\n            try:\n                raddr = self.getpeername()\n                if raddr:\n                    s += \", raddr=%s\" % str(raddr)\n            except error:\n                pass\n        s += '>'\n        return s\n\n    def __getstate__(self):\n        raise TypeError(f\"cannot pickle {self.__class__.__name__!r} object\")\n\n    def dup(self):\n        \"\"\"dup() -> socket object\n\n        Duplicate the socket. Return a new socket object connected to the same\n        system resource. The new socket is non-inheritable.\n        \"\"\"\n        fd = dup(self.fileno())\n        sock = self.__class__(self.family, self.type, self.proto, fileno=fd)\n        sock.settimeout(self.gettimeout())\n        return sock\n\n    def accept(self):\n        \"\"\"accept() -> (socket object, address info)\n\n        Wait for an incoming connection.  Return a new socket\n        representing the connection, and the address of the client.\n        For IP sockets, the address info is a pair (hostaddr, port).\n        \"\"\"\n        fd, addr = self._accept()\n        sock = socket(self.family, self.type, self.proto, fileno=fd)\n        # Issue #7995: if no default timeout is set and the listening\n        # socket had a (non-zero) timeout, force the new socket in blocking\n        # mode to override platform-specific socket flags inheritance.\n        if getdefaulttimeout() is None and self.gettimeout():\n            sock.setblocking(True)\n        return sock, addr\n\n    def makefile(self, mode=\"r\", buffering=None, *,\n                 encoding=None, errors=None, newline=None):\n        \"\"\"makefile(...) -> an I/O stream connected to the socket\n\n        The arguments are as for io.open() after the filename, except the only\n        supported mode values are 'r' (default), 'w' and 'b'.\n        \"\"\"\n        # XXX refactor to share code?\n        if not set(mode) <= {\"r\", \"w\", \"b\"}:\n            raise ValueError(\"invalid mode %r (only r, w, b allowed)\" % (mode,))\n        writing = \"w\" in mode\n        reading = \"r\" in mode or not writing\n        assert reading or writing\n        binary = \"b\" in mode\n        rawmode = \"\"\n        if reading:\n            rawmode += \"r\"\n        if writing:\n            rawmode += \"w\"\n        raw = SocketIO(self, rawmode)\n        self._io_refs += 1\n        if buffering is None:\n            buffering = -1\n        if buffering < 0:\n            buffering = io.DEFAULT_BUFFER_SIZE\n        if buffering == 0:\n            if not binary:\n                raise ValueError(\"unbuffered streams must be binary\")\n            return raw\n        if reading and writing:\n            buffer = io.BufferedRWPair(raw, raw, buffering)\n        elif reading:\n            buffer = io.BufferedReader(raw, buffering)\n        else:\n            assert writing\n            buffer = io.BufferedWriter(raw, buffering)\n        if binary:\n            return buffer\n        text = io.TextIOWrapper(buffer, encoding, errors, newline)\n        text.mode = mode\n        return text\n\n    if hasattr(os, 'sendfile'):\n\n        def _sendfile_use_sendfile(self, file, offset=0, count=None):\n            self._check_sendfile_params(file, offset, count)\n            sockno = self.fileno()\n            try:\n                fileno = file.fileno()\n            except (AttributeError, io.UnsupportedOperation) as err:\n                raise _GiveupOnSendfile(err)  # not a regular file\n            try:\n                fsize = os.fstat(fileno).st_size\n            except OSError as err:\n                raise _GiveupOnSendfile(err)  # not a regular file\n            if not fsize:\n                return 0  # empty file\n            # Truncate to 1GiB to avoid OverflowError, see bpo-38319.\n            blocksize = min(count or fsize, 2 ** 30)\n            timeout = self.gettimeout()\n            if timeout == 0:\n                raise ValueError(\"non-blocking sockets are not supported\")\n            # poll/select have the advantage of not requiring any\n            # extra file descriptor, contrarily to epoll/kqueue\n            # (also, they require a single syscall).\n            if hasattr(selectors, 'PollSelector'):\n                selector = selectors.PollSelector()\n            else:\n                selector = selectors.SelectSelector()\n            selector.register(sockno, selectors.EVENT_WRITE)\n\n            total_sent = 0\n            # localize variable access to minimize overhead\n            selector_select = selector.select\n            os_sendfile = os.sendfile\n            try:\n                while True:\n                    if timeout and not selector_select(timeout):\n                        raise _socket.timeout('timed out')\n                    if count:\n                        blocksize = count - total_sent\n                        if blocksize <= 0:\n                            break\n                    try:\n                        sent = os_sendfile(sockno, fileno, offset, blocksize)\n                    except BlockingIOError:\n                        if not timeout:\n                            # Block until the socket is ready to send some\n                            # data; avoids hogging CPU resources.\n                            selector_select()\n                        continue\n                    except OSError as err:\n                        if total_sent == 0:\n                            # We can get here for different reasons, the main\n                            # one being 'file' is not a regular mmap(2)-like\n                            # file, in which case we'll fall back on using\n                            # plain send().\n                            raise _GiveupOnSendfile(err)\n                        raise err from None\n                    else:\n                        if sent == 0:\n                            break  # EOF\n                        offset += sent\n                        total_sent += sent\n                return total_sent\n            finally:\n                if total_sent > 0 and hasattr(file, 'seek'):\n                    file.seek(offset)\n    else:\n        def _sendfile_use_sendfile(self, file, offset=0, count=None):\n            raise _GiveupOnSendfile(\n                \"os.sendfile() not available on this platform\")\n\n    def _sendfile_use_send(self, file, offset=0, count=None):\n        self._check_sendfile_params(file, offset, count)\n        if self.gettimeout() == 0:\n            raise ValueError(\"non-blocking sockets are not supported\")\n        if offset:\n            file.seek(offset)\n        blocksize = min(count, 8192) if count else 8192\n        total_sent = 0\n        # localize variable access to minimize overhead\n        file_read = file.read\n        sock_send = self.send\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        break\n                data = memoryview(file_read(blocksize))\n                if not data:\n                    break  # EOF\n                while True:\n                    try:\n                        sent = sock_send(data)\n                    except BlockingIOError:\n                        continue\n                    else:\n                        total_sent += sent\n                        if sent < len(data):\n                            data = data[sent:]\n                        else:\n                            break\n            return total_sent\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n\n    def _check_sendfile_params(self, file, offset, count):\n        if 'b' not in getattr(file, 'mode', 'b'):\n            raise ValueError(\"file should be opened in binary mode\")\n        if not self.type & SOCK_STREAM:\n            raise ValueError(\"only SOCK_STREAM type sockets are supported\")\n        if count is not None:\n            if not isinstance(count, int):\n                raise TypeError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n            if count <= 0:\n                raise ValueError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n\n    def sendfile(self, file, offset=0, count=None):\n        \"\"\"sendfile(file[, offset[, count]]) -> sent\n\n        Send a file until EOF is reached by using high-performance\n        os.sendfile() and return the total number of bytes which\n        were sent.\n        *file* must be a regular file object opened in binary mode.\n        If os.sendfile() is not available (e.g. Windows) or file is\n        not a regular file socket.send() will be used instead.\n        *offset* tells from where to start reading the file.\n        If specified, *count* is the total number of bytes to transmit\n        as opposed to sending the file until EOF is reached.\n        File position is updated on return or also in case of error in\n        which case file.tell() can be used to figure out the number of\n        bytes which were sent.\n        The socket must be of SOCK_STREAM type.\n        Non-blocking sockets are not supported.\n        \"\"\"\n        try:\n            return self._sendfile_use_sendfile(file, offset, count)\n        except _GiveupOnSendfile:\n            return self._sendfile_use_send(file, offset, count)\n\n    def _decref_socketios(self):\n        if self._io_refs > 0:\n            self._io_refs -= 1\n        if self._closed:\n            self.close()\n\n    def _real_close(self, _ss=_socket.socket):\n        # This function should not reference any globals. See issue #808164.\n        _ss.close(self)\n\n    def close(self):\n        # This function should not reference any globals. See issue #808164.\n        self._closed = True\n        if self._io_refs <= 0:\n            self._real_close()\n\n    def detach(self):\n        \"\"\"detach() -> file descriptor\n\n        Close the socket object without closing the underlying file descriptor.\n        The object cannot be used after this call, but the file descriptor\n        can be reused for other purposes.  The file descriptor is returned.\n        \"\"\"\n        self._closed = True\n        return super().detach()\n\n    @property\n    def family(self):\n        \"\"\"Read-only access to the address family for this socket.\n        \"\"\"\n        return _intenum_converter(super().family, AddressFamily)\n\n    @property\n    def type(self):\n        \"\"\"Read-only access to the socket type.\n        \"\"\"\n        return _intenum_converter(super().type, SocketKind)\n\n    if os.name == 'nt':\n        def get_inheritable(self):\n            return os.get_handle_inheritable(self.fileno())\n        def set_inheritable(self, inheritable):\n            os.set_handle_inheritable(self.fileno(), inheritable)\n    else:\n        def get_inheritable(self):\n            return os.get_inheritable(self.fileno())\n        def set_inheritable(self, inheritable):\n            os.set_inheritable(self.fileno(), inheritable)\n    get_inheritable.__doc__ = \"Get the inheritable flag of the socket\"\n    set_inheritable.__doc__ = \"Set the inheritable flag of the socket\"\n\ndef fromfd(fd, family, type, proto=0):\n    \"\"\" fromfd(fd, family, type[, proto]) -> socket object\n\n    Create a socket object from a duplicate of the given file\n    descriptor.  The remaining arguments are the same as for socket().\n    \"\"\"\n    nfd = dup(fd)\n    return socket(family, type, proto, nfd)\n\nif hasattr(_socket.socket, \"share\"):\n    def fromshare(info):\n        \"\"\" fromshare(info) -> socket object\n\n        Create a socket object from the bytes object returned by\n        socket.share(pid).\n        \"\"\"\n        return socket(0, 0, 0, info)\n    __all__.append(\"fromshare\")\n\nif hasattr(_socket, \"socketpair\"):\n\n    def socketpair(family=None, type=SOCK_STREAM, proto=0):\n        \"\"\"socketpair([family[, type[, proto]]]) -> (socket object, socket object)\n\n        Create a pair of socket objects from the sockets returned by the platform\n        socketpair() function.\n        The arguments are the same as for socket() except the default family is\n        AF_UNIX if defined on the platform; otherwise, the default is AF_INET.\n        \"\"\"\n        if family is None:\n            try:\n                family = AF_UNIX\n            except NameError:\n                family = AF_INET\n        a, b = _socket.socketpair(family, type, proto)\n        a = socket(family, type, proto, a.detach())\n        b = socket(family, type, proto, b.detach())\n        return a, b\n\nelse:\n\n    # Origin: https://gist.github.com/4325783, by Geert Jansen.  Public domain.\n    def socketpair(family=AF_INET, type=SOCK_STREAM, proto=0):\n        if family == AF_INET:\n            host = _LOCALHOST\n        elif family == AF_INET6:\n            host = _LOCALHOST_V6\n        else:\n            raise ValueError(\"Only AF_INET and AF_INET6 socket address families \"\n                             \"are supported\")\n        if type != SOCK_STREAM:\n            raise ValueError(\"Only SOCK_STREAM socket type is supported\")\n        if proto != 0:\n            raise ValueError(\"Only protocol zero is supported\")\n\n        # We create a connected TCP socket. Note the trick with\n        # setblocking(False) that prevents us from having to create a thread.\n        lsock = socket(family, type, proto)\n        try:\n            lsock.bind((host, 0))\n            lsock.listen()\n            # On IPv6, ignore flow_info and scope_id\n            addr, port = lsock.getsockname()[:2]\n            csock = socket(family, type, proto)\n            try:\n                csock.setblocking(False)\n                try:\n                    csock.connect((addr, port))\n                except (BlockingIOError, InterruptedError):\n                    pass\n                csock.setblocking(True)\n                ssock, _ = lsock.accept()\n            except:\n                csock.close()\n                raise\n        finally:\n            lsock.close()\n        return (ssock, csock)\n    __all__.append(\"socketpair\")\n\nsocketpair.__doc__ = \"\"\"socketpair([family[, type[, proto]]]) -> (socket object, socket object)\nCreate a pair of socket objects from the sockets returned by the platform\nsocketpair() function.\nThe arguments are the same as for socket() except the default family is AF_UNIX\nif defined on the platform; otherwise, the default is AF_INET.\n\"\"\"\n\n_blocking_errnos = { EAGAIN, EWOULDBLOCK }\n\nclass SocketIO(io.RawIOBase):\n\n    \"\"\"Raw I/O implementation for stream sockets.\n\n    This class supports the makefile() method on sockets.  It provides\n    the raw I/O interface on top of a socket object.\n    \"\"\"\n\n    # One might wonder why not let FileIO do the job instead.  There are two\n    # main reasons why FileIO is not adapted:\n    # - it wouldn't work under Windows (where you can't used read() and\n    #   write() on a socket handle)\n    # - it wouldn't work with socket timeouts (FileIO would ignore the\n    #   timeout and consider the socket non-blocking)\n\n    # XXX More docs\n\n    def __init__(self, sock, mode):\n        if mode not in (\"r\", \"w\", \"rw\", \"rb\", \"wb\", \"rwb\"):\n            raise ValueError(\"invalid mode: %r\" % mode)\n        io.RawIOBase.__init__(self)\n        self._sock = sock\n        if \"b\" not in mode:\n            mode += \"b\"\n        self._mode = mode\n        self._reading = \"r\" in mode\n        self._writing = \"w\" in mode\n        self._timeout_occurred = False\n\n    def readinto(self, b):\n        \"\"\"Read up to len(b) bytes into the writable buffer *b* and return\n        the number of bytes read.  If the socket is non-blocking and no bytes\n        are available, None is returned.\n\n        If *b* is non-empty, a 0 return value indicates that the connection\n        was shutdown at the other end.\n        \"\"\"\n        self._checkClosed()\n        self._checkReadable()\n        if self._timeout_occurred:\n            raise OSError(\"cannot read from timed out object\")\n        while True:\n            try:\n                return self._sock.recv_into(b)\n            except timeout:\n                self._timeout_occurred = True\n                raise\n            except error as e:\n                if e.args[0] in _blocking_errnos:\n                    return None\n                raise\n\n    def write(self, b):\n        \"\"\"Write the given bytes or bytearray object *b* to the socket\n        and return the number of bytes written.  This can be less than\n        len(b) if not all data could be written.  If the socket is\n        non-blocking and no bytes could be written None is returned.\n        \"\"\"\n        self._checkClosed()\n        self._checkWritable()\n        try:\n            return self._sock.send(b)\n        except error as e:\n            # XXX what about EINTR?\n            if e.args[0] in _blocking_errnos:\n                return None\n            raise\n\n    def readable(self):\n        \"\"\"True if the SocketIO is open for reading.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed socket.\")\n        return self._reading\n\n    def writable(self):\n        \"\"\"True if the SocketIO is open for writing.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed socket.\")\n        return self._writing\n\n    def seekable(self):\n        \"\"\"True if the SocketIO is open for seeking.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed socket.\")\n        return super().seekable()\n\n    def fileno(self):\n        \"\"\"Return the file descriptor of the underlying socket.\n        \"\"\"\n        self._checkClosed()\n        return self._sock.fileno()\n\n    @property\n    def name(self):\n        if not self.closed:\n            return self.fileno()\n        else:\n            return -1\n\n    @property\n    def mode(self):\n        return self._mode\n\n    def close(self):\n        \"\"\"Close the SocketIO object.  This doesn't close the underlying\n        socket, except if all references to it have disappeared.\n        \"\"\"\n        if self.closed:\n            return\n        io.RawIOBase.close(self)\n        self._sock._decref_socketios()\n        self._sock = None\n\n\ndef getfqdn(name=''):\n    \"\"\"Get fully qualified domain name from name.\n\n    An empty argument is interpreted as meaning the local host.\n\n    First the hostname returned by gethostbyaddr() is checked, then\n    possibly existing aliases. In case no FQDN is available, hostname\n    from gethostname() is returned.\n    \"\"\"\n    name = name.strip()\n    if not name or name == '0.0.0.0':\n        name = gethostname()\n    try:\n        hostname, aliases, ipaddrs = gethostbyaddr(name)\n    except error:\n        pass\n    else:\n        aliases.insert(0, hostname)\n        for name in aliases:\n            if '.' in name:\n                break\n        else:\n            name = hostname\n    return name\n\n\n_GLOBAL_DEFAULT_TIMEOUT = object()\n\ndef create_connection(address, timeout=_GLOBAL_DEFAULT_TIMEOUT,\n                      source_address=None):\n    \"\"\"Connect to *address* and return the socket object.\n\n    Convenience function.  Connect to *address* (a 2-tuple ``(host,\n    port)``) and return the socket object.  Passing the optional\n    *timeout* parameter will set the timeout on the socket instance\n    before attempting to connect.  If no *timeout* is supplied, the\n    global default timeout setting returned by :func:`getdefaulttimeout`\n    is used.  If *source_address* is set it must be a tuple of (host, port)\n    for the socket to bind as a source address before making the connection.\n    A host of '' or port 0 tells the OS to use the default.\n    \"\"\"\n\n    host, port = address\n    err = None\n    for res in getaddrinfo(host, port, 0, SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket(af, socktype, proto)\n            if timeout is not _GLOBAL_DEFAULT_TIMEOUT:\n                sock.settimeout(timeout)\n            if source_address:\n                sock.bind(source_address)\n            sock.connect(sa)\n            # Break explicitly a reference cycle\n            err = None\n            return sock\n\n        except error as _:\n            err = _\n            if sock is not None:\n                sock.close()\n\n    if err is not None:\n        try:\n            raise err\n        finally:\n            # Break explicitly a reference cycle\n            err = None\n    else:\n        raise error(\"getaddrinfo returns an empty list\")\n\n\ndef has_dualstack_ipv6():\n    \"\"\"Return True if the platform supports creating a SOCK_STREAM socket\n    which can handle both AF_INET and AF_INET6 (IPv4 / IPv6) connections.\n    \"\"\"\n    if not has_ipv6 \\\n            or not hasattr(_socket, 'IPPROTO_IPV6') \\\n            or not hasattr(_socket, 'IPV6_V6ONLY'):\n        return False\n    try:\n        with socket(AF_INET6, SOCK_STREAM) as sock:\n            sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 0)\n            return True\n    except error:\n        return False\n\n\ndef create_server(address, *, family=AF_INET, backlog=None, reuse_port=False,\n                  dualstack_ipv6=False):\n    \"\"\"Convenience function which creates a SOCK_STREAM type socket\n    bound to *address* (a 2-tuple (host, port)) and return the socket\n    object.\n\n    *family* should be either AF_INET or AF_INET6.\n    *backlog* is the queue size passed to socket.listen().\n    *reuse_port* dictates whether to use the SO_REUSEPORT socket option.\n    *dualstack_ipv6*: if true and the platform supports it, it will\n    create an AF_INET6 socket able to accept both IPv4 or IPv6\n    connections. When false it will explicitly disable this option on\n    platforms that enable it by default (e.g. Linux).\n\n    >>> with create_server(('', 8000)) as server:\n    ...     while True:\n    ...         conn, addr = server.accept()\n    ...         # handle new connection\n    \"\"\"\n    if reuse_port and not hasattr(_socket, \"SO_REUSEPORT\"):\n        raise ValueError(\"SO_REUSEPORT not supported on this platform\")\n    if dualstack_ipv6:\n        if not has_dualstack_ipv6():\n            raise ValueError(\"dualstack_ipv6 not supported on this platform\")\n        if family != AF_INET6:\n            raise ValueError(\"dualstack_ipv6 requires AF_INET6 family\")\n    sock = socket(family, SOCK_STREAM)\n    try:\n        # Note about Windows. We don't set SO_REUSEADDR because:\n        # 1) It's unnecessary: bind() will succeed even in case of a\n        # previous closed socket on the same address and still in\n        # TIME_WAIT state.\n        # 2) If set, another socket is free to bind() on the same\n        # address, effectively preventing this one from accepting\n        # connections. Also, it may set the process in a state where\n        # it'll no longer respond to any signals or graceful kills.\n        # See: msdn2.microsoft.com/en-us/library/ms740621(VS.85).aspx\n        if os.name not in ('nt', 'cygwin') and \\\n                hasattr(_socket, 'SO_REUSEADDR'):\n            try:\n                sock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1)\n            except error:\n                # Fail later on bind(), for platforms which may not\n                # support this option.\n                pass\n        if reuse_port:\n            sock.setsockopt(SOL_SOCKET, SO_REUSEPORT, 1)\n        if has_ipv6 and family == AF_INET6:\n            if dualstack_ipv6:\n                sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 0)\n            elif hasattr(_socket, \"IPV6_V6ONLY\") and \\\n                    hasattr(_socket, \"IPPROTO_IPV6\"):\n                sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 1)\n        try:\n            sock.bind(address)\n        except error as err:\n            msg = '%s (while attempting to bind on address %r)' % \\\n                (err.strerror, address)\n            raise error(err.errno, msg) from None\n        if backlog is None:\n            sock.listen()\n        else:\n            sock.listen(backlog)\n        return sock\n    except error:\n        sock.close()\n        raise\n\n\ndef getaddrinfo(host, port, family=0, type=0, proto=0, flags=0):\n    \"\"\"Resolve host and port into list of address info entries.\n\n    Translate the host/port argument into a sequence of 5-tuples that contain\n    all the necessary arguments for creating a socket connected to that service.\n    host is a domain name, a string representation of an IPv4/v6 address or\n    None. port is a string service name such as 'http', a numeric port number or\n    None. By passing None as the value of host and port, you can pass NULL to\n    the underlying C API.\n\n    The family, type and proto arguments can be optionally specified in order to\n    narrow the list of addresses returned. Passing zero as a value for each of\n    these arguments selects the full range of results.\n    \"\"\"\n    # We override this function since we want to translate the numeric family\n    # and socket type values to enum constants.\n    addrlist = []\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n        af, socktype, proto, canonname, sa = res\n        addrlist.append((_intenum_converter(af, AddressFamily),\n                         _intenum_converter(socktype, SocketKind),\n                         proto, canonname, sa))\n    return addrlist\n", 923], "/usr/lib/python3.8/asyncio/selector_events.py": ["\"\"\"Event loop using a selector and related classes.\n\nA selector is a \"notify-when-ready\" multiplexer.  For a subclass which\nalso includes support for signal handling, see the unix_events sub-module.\n\"\"\"\n\n__all__ = 'BaseSelectorEventLoop',\n\nimport collections\nimport errno\nimport functools\nimport selectors\nimport socket\nimport warnings\nimport weakref\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nfrom . import base_events\nfrom . import constants\nfrom . import events\nfrom . import futures\nfrom . import protocols\nfrom . import sslproto\nfrom . import transports\nfrom . import trsock\nfrom .log import logger\n\n\ndef _test_selector_event(selector, fd, event):\n    # Test if the selector is monitoring 'event' events\n    # for the file descriptor 'fd'.\n    try:\n        key = selector.get_key(fd)\n    except KeyError:\n        return False\n    else:\n        return bool(key.events & event)\n\n\ndef _check_ssl_socket(sock):\n    if ssl is not None and isinstance(sock, ssl.SSLSocket):\n        raise TypeError(\"Socket cannot be of type SSLSocket\")\n\n\nclass BaseSelectorEventLoop(base_events.BaseEventLoop):\n    \"\"\"Selector event loop.\n\n    See events.EventLoop for API specification.\n    \"\"\"\n\n    def __init__(self, selector=None):\n        super().__init__()\n\n        if selector is None:\n            selector = selectors.DefaultSelector()\n        logger.debug('Using selector: %s', selector.__class__.__name__)\n        self._selector = selector\n        self._make_self_pipe()\n        self._transports = weakref.WeakValueDictionary()\n\n    def _make_socket_transport(self, sock, protocol, waiter=None, *,\n                               extra=None, server=None):\n        return _SelectorSocketTransport(self, sock, protocol, waiter,\n                                        extra, server)\n\n    def _make_ssl_transport(\n            self, rawsock, protocol, sslcontext, waiter=None,\n            *, server_side=False, server_hostname=None,\n            extra=None, server=None,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT):\n        ssl_protocol = sslproto.SSLProtocol(\n                self, protocol, sslcontext, waiter,\n                server_side, server_hostname,\n                ssl_handshake_timeout=ssl_handshake_timeout)\n        _SelectorSocketTransport(self, rawsock, ssl_protocol,\n                                 extra=extra, server=server)\n        return ssl_protocol._app_transport\n\n    def _make_datagram_transport(self, sock, protocol,\n                                 address=None, waiter=None, extra=None):\n        return _SelectorDatagramTransport(self, sock, protocol,\n                                          address, waiter, extra)\n\n    def close(self):\n        if self.is_running():\n            raise RuntimeError(\"Cannot close a running event loop\")\n        if self.is_closed():\n            return\n        self._close_self_pipe()\n        super().close()\n        if self._selector is not None:\n            self._selector.close()\n            self._selector = None\n\n    def _close_self_pipe(self):\n        self._remove_reader(self._ssock.fileno())\n        self._ssock.close()\n        self._ssock = None\n        self._csock.close()\n        self._csock = None\n        self._internal_fds -= 1\n\n    def _make_self_pipe(self):\n        # A self-socket, really. :-)\n        self._ssock, self._csock = socket.socketpair()\n        self._ssock.setblocking(False)\n        self._csock.setblocking(False)\n        self._internal_fds += 1\n        self._add_reader(self._ssock.fileno(), self._read_from_self)\n\n    def _process_self_data(self, data):\n        pass\n\n    def _read_from_self(self):\n        while True:\n            try:\n                data = self._ssock.recv(4096)\n                if not data:\n                    break\n                self._process_self_data(data)\n            except InterruptedError:\n                continue\n            except BlockingIOError:\n                break\n\n    def _write_to_self(self):\n        # This may be called from a different thread, possibly after\n        # _close_self_pipe() has been called or even while it is\n        # running.  Guard for self._csock being None or closed.  When\n        # a socket is closed, send() raises OSError (with errno set to\n        # EBADF, but let's not rely on the exact error code).\n        csock = self._csock\n        if csock is None:\n            return\n\n        try:\n            csock.send(b'\\0')\n        except OSError:\n            if self._debug:\n                logger.debug(\"Fail to write a null byte into the \"\n                             \"self-pipe socket\",\n                             exc_info=True)\n\n    def _start_serving(self, protocol_factory, sock,\n                       sslcontext=None, server=None, backlog=100,\n                       ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT):\n        self._add_reader(sock.fileno(), self._accept_connection,\n                         protocol_factory, sock, sslcontext, server, backlog,\n                         ssl_handshake_timeout)\n\n    def _accept_connection(\n            self, protocol_factory, sock,\n            sslcontext=None, server=None, backlog=100,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT):\n        # This method is only called once for each event loop tick where the\n        # listening socket has triggered an EVENT_READ. There may be multiple\n        # connections waiting for an .accept() so it is called in a loop.\n        # See https://bugs.python.org/issue27906 for more details.\n        for _ in range(backlog):\n            try:\n                conn, addr = sock.accept()\n                if self._debug:\n                    logger.debug(\"%r got a new connection from %r: %r\",\n                                 server, addr, conn)\n                conn.setblocking(False)\n            except (BlockingIOError, InterruptedError, ConnectionAbortedError):\n                # Early exit because the socket accept buffer is empty.\n                return None\n            except OSError as exc:\n                # There's nowhere to send the error, so just log it.\n                if exc.errno in (errno.EMFILE, errno.ENFILE,\n                                 errno.ENOBUFS, errno.ENOMEM):\n                    # Some platforms (e.g. Linux keep reporting the FD as\n                    # ready, so we remove the read handler temporarily.\n                    # We'll try again in a while.\n                    self.call_exception_handler({\n                        'message': 'socket.accept() out of system resource',\n                        'exception': exc,\n                        'socket': trsock.TransportSocket(sock),\n                    })\n                    self._remove_reader(sock.fileno())\n                    self.call_later(constants.ACCEPT_RETRY_DELAY,\n                                    self._start_serving,\n                                    protocol_factory, sock, sslcontext, server,\n                                    backlog, ssl_handshake_timeout)\n                else:\n                    raise  # The event loop will catch, log and ignore it.\n            else:\n                extra = {'peername': addr}\n                accept = self._accept_connection2(\n                    protocol_factory, conn, extra, sslcontext, server,\n                    ssl_handshake_timeout)\n                self.create_task(accept)\n\n    async def _accept_connection2(\n            self, protocol_factory, conn, extra,\n            sslcontext=None, server=None,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT):\n        protocol = None\n        transport = None\n        try:\n            protocol = protocol_factory()\n            waiter = self.create_future()\n            if sslcontext:\n                transport = self._make_ssl_transport(\n                    conn, protocol, sslcontext, waiter=waiter,\n                    server_side=True, extra=extra, server=server,\n                    ssl_handshake_timeout=ssl_handshake_timeout)\n            else:\n                transport = self._make_socket_transport(\n                    conn, protocol, waiter=waiter, extra=extra,\n                    server=server)\n\n            try:\n                await waiter\n            except BaseException:\n                transport.close()\n                raise\n                # It's now up to the protocol to handle the connection.\n\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            if self._debug:\n                context = {\n                    'message':\n                        'Error on transport creation for incoming connection',\n                    'exception': exc,\n                }\n                if protocol is not None:\n                    context['protocol'] = protocol\n                if transport is not None:\n                    context['transport'] = transport\n                self.call_exception_handler(context)\n\n    def _ensure_fd_no_transport(self, fd):\n        fileno = fd\n        if not isinstance(fileno, int):\n            try:\n                fileno = int(fileno.fileno())\n            except (AttributeError, TypeError, ValueError):\n                # This code matches selectors._fileobj_to_fd function.\n                raise ValueError(f\"Invalid file object: {fd!r}\") from None\n        try:\n            transport = self._transports[fileno]\n        except KeyError:\n            pass\n        else:\n            if not transport.is_closing():\n                raise RuntimeError(\n                    f'File descriptor {fd!r} is used by transport '\n                    f'{transport!r}')\n\n    def _add_reader(self, fd, callback, *args):\n        self._check_closed()\n        handle = events.Handle(callback, args, self, None)\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            self._selector.register(fd, selectors.EVENT_READ,\n                                    (handle, None))\n        else:\n            mask, (reader, writer) = key.events, key.data\n            self._selector.modify(fd, mask | selectors.EVENT_READ,\n                                  (handle, writer))\n            if reader is not None:\n                reader.cancel()\n\n    def _remove_reader(self, fd):\n        if self.is_closed():\n            return False\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            return False\n        else:\n            mask, (reader, writer) = key.events, key.data\n            mask &= ~selectors.EVENT_READ\n            if not mask:\n                self._selector.unregister(fd)\n            else:\n                self._selector.modify(fd, mask, (None, writer))\n\n            if reader is not None:\n                reader.cancel()\n                return True\n            else:\n                return False\n\n    def _add_writer(self, fd, callback, *args):\n        self._check_closed()\n        handle = events.Handle(callback, args, self, None)\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            self._selector.register(fd, selectors.EVENT_WRITE,\n                                    (None, handle))\n        else:\n            mask, (reader, writer) = key.events, key.data\n            self._selector.modify(fd, mask | selectors.EVENT_WRITE,\n                                  (reader, handle))\n            if writer is not None:\n                writer.cancel()\n\n    def _remove_writer(self, fd):\n        \"\"\"Remove a writer callback.\"\"\"\n        if self.is_closed():\n            return False\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            return False\n        else:\n            mask, (reader, writer) = key.events, key.data\n            # Remove both writer and connector.\n            mask &= ~selectors.EVENT_WRITE\n            if not mask:\n                self._selector.unregister(fd)\n            else:\n                self._selector.modify(fd, mask, (reader, None))\n\n            if writer is not None:\n                writer.cancel()\n                return True\n            else:\n                return False\n\n    def add_reader(self, fd, callback, *args):\n        \"\"\"Add a reader callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._add_reader(fd, callback, *args)\n\n    def remove_reader(self, fd):\n        \"\"\"Remove a reader callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._remove_reader(fd)\n\n    def add_writer(self, fd, callback, *args):\n        \"\"\"Add a writer callback..\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._add_writer(fd, callback, *args)\n\n    def remove_writer(self, fd):\n        \"\"\"Remove a writer callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._remove_writer(fd)\n\n    async def sock_recv(self, sock, n):\n        \"\"\"Receive data from the socket.\n\n        The return value is a bytes object representing the data received.\n        The maximum amount of data to be received at once is specified by\n        nbytes.\n        \"\"\"\n        _check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.recv(n)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self.add_reader(fd, self._sock_recv, fut, sock, n)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd))\n        return await fut\n\n    def _sock_read_done(self, fd, fut):\n        self.remove_reader(fd)\n\n    def _sock_recv(self, fut, sock, n):\n        # _sock_recv() can add itself as an I/O callback if the operation can't\n        # be done immediately. Don't use it directly, call sock_recv().\n        if fut.done():\n            return\n        try:\n            data = sock.recv(n)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(data)\n\n    async def sock_recv_into(self, sock, buf):\n        \"\"\"Receive data from the socket.\n\n        The received data is written into *buf* (a writable buffer).\n        The return value is the number of bytes written.\n        \"\"\"\n        _check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self.add_reader(fd, self._sock_recv_into, fut, sock, buf)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd))\n        return await fut\n\n    def _sock_recv_into(self, fut, sock, buf):\n        # _sock_recv_into() can add itself as an I/O callback if the operation\n        # can't be done immediately. Don't use it directly, call\n        # sock_recv_into().\n        if fut.done():\n            return\n        try:\n            nbytes = sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(nbytes)\n\n    async def sock_sendall(self, sock, data):\n        \"\"\"Send data to the socket.\n\n        The socket must be connected to a remote socket. This method continues\n        to send data from data until either all data has been sent or an\n        error occurs. None is returned on success. On error, an exception is\n        raised, and there is no way to determine how much data, if any, was\n        successfully processed by the receiving end of the connection.\n        \"\"\"\n        _check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            n = sock.send(data)\n        except (BlockingIOError, InterruptedError):\n            n = 0\n\n        if n == len(data):\n            # all data sent\n            return\n\n        fut = self.create_future()\n        fd = sock.fileno()\n        fut.add_done_callback(\n            functools.partial(self._sock_write_done, fd))\n        # use a trick with a list in closure to store a mutable state\n        self.add_writer(fd, self._sock_sendall, fut, sock,\n                        memoryview(data), [n])\n        return await fut\n\n    def _sock_sendall(self, fut, sock, view, pos):\n        if fut.done():\n            # Future cancellation can be scheduled on previous loop iteration\n            return\n        start = pos[0]\n        try:\n            n = sock.send(view[start:])\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n            return\n\n        start += n\n\n        if start == len(view):\n            fut.set_result(None)\n        else:\n            pos[0] = start\n\n    async def sock_connect(self, sock, address):\n        \"\"\"Connect to a remote socket at address.\n\n        This method is a coroutine.\n        \"\"\"\n        _check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n\n        if not hasattr(socket, 'AF_UNIX') or sock.family != socket.AF_UNIX:\n            resolved = await self._ensure_resolved(\n                address, family=sock.family, proto=sock.proto, loop=self)\n            _, _, _, _, address = resolved[0]\n\n        fut = self.create_future()\n        self._sock_connect(fut, sock, address)\n        return await fut\n\n    def _sock_connect(self, fut, sock, address):\n        fd = sock.fileno()\n        try:\n            sock.connect(address)\n        except (BlockingIOError, InterruptedError):\n            # Issue #23618: When the C function connect() fails with EINTR, the\n            # connection runs in background. We have to wait until the socket\n            # becomes writable to be notified when the connection succeed or\n            # fails.\n            fut.add_done_callback(\n                functools.partial(self._sock_write_done, fd))\n            self.add_writer(fd, self._sock_connect_cb, fut, sock, address)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(None)\n\n    def _sock_write_done(self, fd, fut):\n        self.remove_writer(fd)\n\n    def _sock_connect_cb(self, fut, sock, address):\n        if fut.done():\n            return\n\n        try:\n            err = sock.getsockopt(socket.SOL_SOCKET, socket.SO_ERROR)\n            if err != 0:\n                # Jump to any except clause below.\n                raise OSError(err, f'Connect call failed {address}')\n        except (BlockingIOError, InterruptedError):\n            # socket is still registered, the callback will be retried later\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(None)\n\n    async def sock_accept(self, sock):\n        \"\"\"Accept a connection.\n\n        The socket must be bound to an address and listening for connections.\n        The return value is a pair (conn, address) where conn is a new socket\n        object usable to send and receive data on the connection, and address\n        is the address bound to the socket on the other end of the connection.\n        \"\"\"\n        _check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        fut = self.create_future()\n        self._sock_accept(fut, False, sock)\n        return await fut\n\n    def _sock_accept(self, fut, registered, sock):\n        fd = sock.fileno()\n        if registered:\n            self.remove_reader(fd)\n        if fut.done():\n            return\n        try:\n            conn, address = sock.accept()\n            conn.setblocking(False)\n        except (BlockingIOError, InterruptedError):\n            self.add_reader(fd, self._sock_accept, fut, True, sock)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result((conn, address))\n\n    async def _sendfile_native(self, transp, file, offset, count):\n        del self._transports[transp._sock_fd]\n        resume_reading = transp.is_reading()\n        transp.pause_reading()\n        await transp._make_empty_waiter()\n        try:\n            return await self.sock_sendfile(transp._sock, file, offset, count,\n                                            fallback=False)\n        finally:\n            transp._reset_empty_waiter()\n            if resume_reading:\n                transp.resume_reading()\n            self._transports[transp._sock_fd] = transp\n\n    def _process_events(self, event_list):\n        for key, mask in event_list:\n            fileobj, (reader, writer) = key.fileobj, key.data\n            if mask & selectors.EVENT_READ and reader is not None:\n                if reader._cancelled:\n                    self._remove_reader(fileobj)\n                else:\n                    self._add_callback(reader)\n            if mask & selectors.EVENT_WRITE and writer is not None:\n                if writer._cancelled:\n                    self._remove_writer(fileobj)\n                else:\n                    self._add_callback(writer)\n\n    def _stop_serving(self, sock):\n        self._remove_reader(sock.fileno())\n        sock.close()\n\n\nclass _SelectorTransport(transports._FlowControlMixin,\n                         transports.Transport):\n\n    max_size = 256 * 1024  # Buffer size passed to recv().\n\n    _buffer_factory = bytearray  # Constructs initial value for self._buffer.\n\n    # Attribute used in the destructor: it must be set even if the constructor\n    # is not called (see _SelectorSslTransport which may start by raising an\n    # exception)\n    _sock = None\n\n    def __init__(self, loop, sock, protocol, extra=None, server=None):\n        super().__init__(extra, loop)\n        self._extra['socket'] = trsock.TransportSocket(sock)\n        try:\n            self._extra['sockname'] = sock.getsockname()\n        except OSError:\n            self._extra['sockname'] = None\n        if 'peername' not in self._extra:\n            try:\n                self._extra['peername'] = sock.getpeername()\n            except socket.error:\n                self._extra['peername'] = None\n        self._sock = sock\n        self._sock_fd = sock.fileno()\n\n        self._protocol_connected = False\n        self.set_protocol(protocol)\n\n        self._server = server\n        self._buffer = self._buffer_factory()\n        self._conn_lost = 0  # Set when call to connection_lost scheduled.\n        self._closing = False  # Set when close() called.\n        if self._server is not None:\n            self._server._attach()\n        loop._transports[self._sock_fd] = self\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._sock is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._sock_fd}')\n        # test if the transport was closed\n        if self._loop is not None and not self._loop.is_closed():\n            polling = _test_selector_event(self._loop._selector,\n                                           self._sock_fd, selectors.EVENT_READ)\n            if polling:\n                info.append('read=polling')\n            else:\n                info.append('read=idle')\n\n            polling = _test_selector_event(self._loop._selector,\n                                           self._sock_fd,\n                                           selectors.EVENT_WRITE)\n            if polling:\n                state = 'polling'\n            else:\n                state = 'idle'\n\n            bufsize = self.get_write_buffer_size()\n            info.append(f'write=<{state}, bufsize={bufsize}>')\n        return '<{}>'.format(' '.join(info))\n\n    def abort(self):\n        self._force_close(None)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n        self._protocol_connected = True\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def close(self):\n        if self._closing:\n            return\n        self._closing = True\n        self._loop._remove_reader(self._sock_fd)\n        if not self._buffer:\n            self._conn_lost += 1\n            self._loop._remove_writer(self._sock_fd)\n            self._loop.call_soon(self._call_connection_lost, None)\n\n    def __del__(self, _warn=warnings.warn):\n        if self._sock is not None:\n            _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n            self._sock.close()\n\n    def _fatal_error(self, exc, message='Fatal error on transport'):\n        # Should be called from exception handler only.\n        if isinstance(exc, OSError):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._force_close(exc)\n\n    def _force_close(self, exc):\n        if self._conn_lost:\n            return\n        if self._buffer:\n            self._buffer.clear()\n            self._loop._remove_writer(self._sock_fd)\n        if not self._closing:\n            self._closing = True\n            self._loop._remove_reader(self._sock_fd)\n        self._conn_lost += 1\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            if self._protocol_connected:\n                self._protocol.connection_lost(exc)\n        finally:\n            self._sock.close()\n            self._sock = None\n            self._protocol = None\n            self._loop = None\n            server = self._server\n            if server is not None:\n                server._detach()\n                self._server = None\n\n    def get_write_buffer_size(self):\n        return len(self._buffer)\n\n    def _add_reader(self, fd, callback, *args):\n        if self._closing:\n            return\n\n        self._loop._add_reader(fd, callback, *args)\n\n\nclass _SelectorSocketTransport(_SelectorTransport):\n\n    _start_tls_compatible = True\n    _sendfile_compatible = constants._SendfileMode.TRY_NATIVE\n\n    def __init__(self, loop, sock, protocol, waiter=None,\n                 extra=None, server=None):\n\n        self._read_ready_cb = None\n        super().__init__(loop, sock, protocol, extra, server)\n        self._eof = False\n        self._paused = False\n        self._empty_waiter = None\n\n        # Disable the Nagle algorithm -- small writes will be\n        # sent without waiting for the TCP ACK.  This generally\n        # decreases the latency (in some cases significantly.)\n        base_events._set_nodelay(self._sock)\n\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._add_reader,\n                             self._sock_fd, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def set_protocol(self, protocol):\n        if isinstance(protocol, protocols.BufferedProtocol):\n            self._read_ready_cb = self._read_ready__get_buffer\n        else:\n            self._read_ready_cb = self._read_ready__data_received\n\n        super().set_protocol(protocol)\n\n    def is_reading(self):\n        return not self._paused and not self._closing\n\n    def pause_reading(self):\n        if self._closing or self._paused:\n            return\n        self._paused = True\n        self._loop._remove_reader(self._sock_fd)\n        if self._loop.get_debug():\n            logger.debug(\"%r pauses reading\", self)\n\n    def resume_reading(self):\n        if self._closing or not self._paused:\n            return\n        self._paused = False\n        self._add_reader(self._sock_fd, self._read_ready)\n        if self._loop.get_debug():\n            logger.debug(\"%r resumes reading\", self)\n\n    def _read_ready(self):\n        self._read_ready_cb()\n\n    def _read_ready__get_buffer(self):\n        if self._conn_lost:\n            return\n\n        try:\n            buf = self._protocol.get_buffer(-1)\n            if not len(buf):\n                raise RuntimeError('get_buffer() returned an empty buffer')\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.get_buffer() call failed.')\n            return\n\n        try:\n            nbytes = self._sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on socket transport')\n            return\n\n        if not nbytes:\n            self._read_ready__on_eof()\n            return\n\n        try:\n            self._protocol.buffer_updated(nbytes)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.buffer_updated() call failed.')\n\n    def _read_ready__data_received(self):\n        if self._conn_lost:\n            return\n        try:\n            data = self._sock.recv(self.max_size)\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on socket transport')\n            return\n\n        if not data:\n            self._read_ready__on_eof()\n            return\n\n        try:\n            self._protocol.data_received(data)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.data_received() call failed.')\n\n    def _read_ready__on_eof(self):\n        if self._loop.get_debug():\n            logger.debug(\"%r received EOF\", self)\n\n        try:\n            keep_open = self._protocol.eof_received()\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.eof_received() call failed.')\n            return\n\n        if keep_open:\n            # We're keeping the connection open so the\n            # protocol can write more, but we still can't\n            # receive more, so remove the reader callback.\n            self._loop._remove_reader(self._sock_fd)\n        else:\n            self.close()\n\n    def write(self, data):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(f'data argument must be a bytes-like object, '\n                            f'not {type(data).__name__!r}')\n        if self._eof:\n            raise RuntimeError('Cannot call write() after write_eof()')\n        if self._empty_waiter is not None:\n            raise RuntimeError('unable to write; sendfile is in progress')\n        if not data:\n            return\n\n        if self._conn_lost:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('socket.send() raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Optimization: try to send now.\n            try:\n                n = self._sock.send(data)\n            except (BlockingIOError, InterruptedError):\n                pass\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(exc, 'Fatal write error on socket transport')\n                return\n            else:\n                data = data[n:]\n                if not data:\n                    return\n            # Not all was written; register write handler.\n            self._loop._add_writer(self._sock_fd, self._write_ready)\n\n        # Add it to the buffer.\n        self._buffer.extend(data)\n        self._maybe_pause_protocol()\n\n    def _write_ready(self):\n        assert self._buffer, 'Data should not be empty'\n\n        if self._conn_lost:\n            return\n        try:\n            n = self._sock.send(self._buffer)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._loop._remove_writer(self._sock_fd)\n            self._buffer.clear()\n            self._fatal_error(exc, 'Fatal write error on socket transport')\n            if self._empty_waiter is not None:\n                self._empty_waiter.set_exception(exc)\n        else:\n            if n:\n                del self._buffer[:n]\n            self._maybe_resume_protocol()  # May append to buffer.\n            if not self._buffer:\n                self._loop._remove_writer(self._sock_fd)\n                if self._empty_waiter is not None:\n                    self._empty_waiter.set_result(None)\n                if self._closing:\n                    self._call_connection_lost(None)\n                elif self._eof:\n                    self._sock.shutdown(socket.SHUT_WR)\n\n    def write_eof(self):\n        if self._closing or self._eof:\n            return\n        self._eof = True\n        if not self._buffer:\n            self._sock.shutdown(socket.SHUT_WR)\n\n    def can_write_eof(self):\n        return True\n\n    def _call_connection_lost(self, exc):\n        super()._call_connection_lost(exc)\n        if self._empty_waiter is not None:\n            self._empty_waiter.set_exception(\n                ConnectionError(\"Connection is closed by peer\"))\n\n    def _make_empty_waiter(self):\n        if self._empty_waiter is not None:\n            raise RuntimeError(\"Empty waiter is already set\")\n        self._empty_waiter = self._loop.create_future()\n        if not self._buffer:\n            self._empty_waiter.set_result(None)\n        return self._empty_waiter\n\n    def _reset_empty_waiter(self):\n        self._empty_waiter = None\n\n\nclass _SelectorDatagramTransport(_SelectorTransport):\n\n    _buffer_factory = collections.deque\n\n    def __init__(self, loop, sock, protocol, address=None,\n                 waiter=None, extra=None):\n        super().__init__(loop, sock, protocol, extra)\n        self._address = address\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._add_reader,\n                             self._sock_fd, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def get_write_buffer_size(self):\n        return sum(len(data) for data, _ in self._buffer)\n\n    def _read_ready(self):\n        if self._conn_lost:\n            return\n        try:\n            data, addr = self._sock.recvfrom(self.max_size)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except OSError as exc:\n            self._protocol.error_received(exc)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on datagram transport')\n        else:\n            self._protocol.datagram_received(data, addr)\n\n    def sendto(self, data, addr=None):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(f'data argument must be a bytes-like object, '\n                            f'not {type(data).__name__!r}')\n        if not data:\n            return\n\n        if self._address:\n            if addr not in (None, self._address):\n                raise ValueError(\n                    f'Invalid address: must be None or {self._address}')\n            addr = self._address\n\n        if self._conn_lost and self._address:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('socket.send() raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Attempt to send it right away first.\n            try:\n                if self._extra['peername']:\n                    self._sock.send(data)\n                else:\n                    self._sock.sendto(data, addr)\n                return\n            except (BlockingIOError, InterruptedError):\n                self._loop._add_writer(self._sock_fd, self._sendto_ready)\n            except OSError as exc:\n                self._protocol.error_received(exc)\n                return\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(\n                    exc, 'Fatal write error on datagram transport')\n                return\n\n        # Ensure that what we buffer is immutable.\n        self._buffer.append((bytes(data), addr))\n        self._maybe_pause_protocol()\n\n    def _sendto_ready(self):\n        while self._buffer:\n            data, addr = self._buffer.popleft()\n            try:\n                if self._extra['peername']:\n                    self._sock.send(data)\n                else:\n                    self._sock.sendto(data, addr)\n            except (BlockingIOError, InterruptedError):\n                self._buffer.appendleft((data, addr))  # Try again later.\n                break\n            except OSError as exc:\n                self._protocol.error_received(exc)\n                return\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(\n                    exc, 'Fatal write error on datagram transport')\n                return\n\n        self._maybe_resume_protocol()  # May append to buffer.\n        if not self._buffer:\n            self._loop._remove_writer(self._sock_fd)\n            if self._closing:\n                self._call_connection_lost(None)\n", 1091], "/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/weakref.py": ["\"\"\"Weak reference support for Python.\n\nThis module is an implementation of PEP 205:\n\nhttp://www.python.org/dev/peps/pep-0205/\n\"\"\"\n\n# Naming convention: Variables named \"wr\" are weak reference objects;\n# they are called this instead of \"ref\" to avoid name collisions with\n# the module-global ref() function imported from _weakref.\n\nfrom _weakref import (\n     getweakrefcount,\n     getweakrefs,\n     ref,\n     proxy,\n     CallableProxyType,\n     ProxyType,\n     ReferenceType,\n     _remove_dead_weakref)\n\nfrom _weakrefset import WeakSet, _IterationGuard\n\nimport _collections_abc  # Import after _weakref to avoid circular import.\nimport sys\nimport itertools\n\nProxyTypes = (ProxyType, CallableProxyType)\n\n__all__ = [\"ref\", \"proxy\", \"getweakrefcount\", \"getweakrefs\",\n           \"WeakKeyDictionary\", \"ReferenceType\", \"ProxyType\",\n           \"CallableProxyType\", \"ProxyTypes\", \"WeakValueDictionary\",\n           \"WeakSet\", \"WeakMethod\", \"finalize\"]\n\n\nclass WeakMethod(ref):\n    \"\"\"\n    A custom `weakref.ref` subclass which simulates a weak reference to\n    a bound method, working around the lifetime problem of bound methods.\n    \"\"\"\n\n    __slots__ = \"_func_ref\", \"_meth_type\", \"_alive\", \"__weakref__\"\n\n    def __new__(cls, meth, callback=None):\n        try:\n            obj = meth.__self__\n            func = meth.__func__\n        except AttributeError:\n            raise TypeError(\"argument should be a bound method, not {}\"\n                            .format(type(meth))) from None\n        def _cb(arg):\n            # The self-weakref trick is needed to avoid creating a reference\n            # cycle.\n            self = self_wr()\n            if self._alive:\n                self._alive = False\n                if callback is not None:\n                    callback(self)\n        self = ref.__new__(cls, obj, _cb)\n        self._func_ref = ref(func, _cb)\n        self._meth_type = type(meth)\n        self._alive = True\n        self_wr = ref(self)\n        return self\n\n    def __call__(self):\n        obj = super().__call__()\n        func = self._func_ref()\n        if obj is None or func is None:\n            return None\n        return self._meth_type(func, obj)\n\n    def __eq__(self, other):\n        if isinstance(other, WeakMethod):\n            if not self._alive or not other._alive:\n                return self is other\n            return ref.__eq__(self, other) and self._func_ref == other._func_ref\n        return False\n\n    def __ne__(self, other):\n        if isinstance(other, WeakMethod):\n            if not self._alive or not other._alive:\n                return self is not other\n            return ref.__ne__(self, other) or self._func_ref != other._func_ref\n        return True\n\n    __hash__ = ref.__hash__\n\n\nclass WeakValueDictionary(_collections_abc.MutableMapping):\n    \"\"\"Mapping class that references values weakly.\n\n    Entries in the dictionary will be discarded when no strong\n    reference to the value exists anymore\n    \"\"\"\n    # We inherit the constructor without worrying about the input\n    # dictionary; since it uses our .update() method, we get the right\n    # checks (if the other dictionary is a WeakValueDictionary,\n    # objects are unwrapped on the way out, and we always wrap on the\n    # way in).\n\n    def __init__(self, other=(), /, **kw):\n        def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(wr.key)\n                else:\n                    # Atomic removal is necessary since this function\n                    # can be called asynchronously by the GC\n                    _atomic_removal(self.data, wr.key)\n        self._remove = remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        self.data = {}\n        self.update(other, **kw)\n\n    def _commit_removals(self):\n        l = self._pending_removals\n        d = self.data\n        # We shouldn't encounter any KeyError, because this method should\n        # always be called *before* mutating the dict.\n        while l:\n            key = l.pop()\n            _remove_dead_weakref(d, key)\n\n    def __getitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        o = self.data[key]()\n        if o is None:\n            raise KeyError(key)\n        else:\n            return o\n\n    def __delitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        del self.data[key]\n\n    def __len__(self):\n        if self._pending_removals:\n            self._commit_removals()\n        return len(self.data)\n\n    def __contains__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data[key]()\n        except KeyError:\n            return False\n        return o is not None\n\n    def __repr__(self):\n        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n\n    def __setitem__(self, key, value):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data[key] = KeyedRef(value, self._remove, key)\n\n    def copy(self):\n        if self._pending_removals:\n            self._commit_removals()\n        new = WeakValueDictionary()\n        with _IterationGuard(self):\n            for key, wr in self.data.items():\n                o = wr()\n                if o is not None:\n                    new[key] = o\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        if self._pending_removals:\n            self._commit_removals()\n        new = self.__class__()\n        with _IterationGuard(self):\n            for key, wr in self.data.items():\n                o = wr()\n                if o is not None:\n                    new[deepcopy(key, memo)] = o\n        return new\n\n    def get(self, key, default=None):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            wr = self.data[key]\n        except KeyError:\n            return default\n        else:\n            o = wr()\n            if o is None:\n                # This should only happen\n                return default\n            else:\n                return o\n\n    def items(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for k, wr in self.data.items():\n                v = wr()\n                if v is not None:\n                    yield k, v\n\n    def keys(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for k, wr in self.data.items():\n                if wr() is not None:\n                    yield k\n\n    __iter__ = keys\n\n    def itervaluerefs(self):\n        \"\"\"Return an iterator that yields the weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            yield from self.data.values()\n\n    def values(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for wr in self.data.values():\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    def popitem(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            key, wr = self.data.popitem()\n            o = wr()\n            if o is not None:\n                return key, o\n\n    def pop(self, key, *args):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data.pop(key)()\n        except KeyError:\n            o = None\n        if o is None:\n            if args:\n                return args[0]\n            else:\n                raise KeyError(key)\n        else:\n            return o\n\n    def setdefault(self, key, default=None):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            o = None\n        if o is None:\n            if self._pending_removals:\n                self._commit_removals()\n            self.data[key] = KeyedRef(default, self._remove, key)\n            return default\n        else:\n            return o\n\n    def update(self, other=None, /, **kwargs):\n        if self._pending_removals:\n            self._commit_removals()\n        d = self.data\n        if other is not None:\n            if not hasattr(other, \"items\"):\n                other = dict(other)\n            for key, o in other.items():\n                d[key] = KeyedRef(o, self._remove, key)\n        for key, o in kwargs.items():\n            d[key] = KeyedRef(o, self._remove, key)\n\n    def valuerefs(self):\n        \"\"\"Return a list of weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        if self._pending_removals:\n            self._commit_removals()\n        return list(self.data.values())\n\n\nclass KeyedRef(ref):\n    \"\"\"Specialized reference that includes a key corresponding to the value.\n\n    This is used in the WeakValueDictionary to avoid having to create\n    a function object for each key stored in the mapping.  A shared\n    callback object can use the 'key' attribute of a KeyedRef instead\n    of getting a reference to the key from an enclosing scope.\n\n    \"\"\"\n\n    __slots__ = \"key\",\n\n    def __new__(type, ob, callback, key):\n        self = ref.__new__(type, ob, callback)\n        self.key = key\n        return self\n\n    def __init__(self, ob, callback, key):\n        super().__init__(ob, callback)\n\n\nclass WeakKeyDictionary(_collections_abc.MutableMapping):\n    \"\"\" Mapping class that references keys weakly.\n\n    Entries in the dictionary will be discarded when there is no\n    longer a strong reference to the key. This can be used to\n    associate additional data with an object owned by other parts of\n    an application without adding attributes to those objects. This\n    can be especially useful with objects that override attribute\n    accesses.\n    \"\"\"\n\n    def __init__(self, dict=None):\n        self.data = {}\n        def remove(k, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(k)\n                else:\n                    del self.data[k]\n        self._remove = remove\n        # A list of dead weakrefs (keys to be removed)\n        self._pending_removals = []\n        self._iterating = set()\n        self._dirty_len = False\n        if dict is not None:\n            self.update(dict)\n\n    def _commit_removals(self):\n        # NOTE: We don't need to call this method before mutating the dict,\n        # because a dead weakref never compares equal to a live weakref,\n        # even if they happened to refer to equal objects.\n        # However, it means keys may already have been removed.\n        l = self._pending_removals\n        d = self.data\n        while l:\n            try:\n                del d[l.pop()]\n            except KeyError:\n                pass\n\n    def _scrub_removals(self):\n        d = self.data\n        self._pending_removals = [k for k in self._pending_removals if k in d]\n        self._dirty_len = False\n\n    def __delitem__(self, key):\n        self._dirty_len = True\n        del self.data[ref(key)]\n\n    def __getitem__(self, key):\n        return self.data[ref(key)]\n\n    def __len__(self):\n        if self._dirty_len and self._pending_removals:\n            # self._pending_removals may still contain keys which were\n            # explicitly removed, we have to scrub them (see issue #21173).\n            self._scrub_removals()\n        return len(self.data) - len(self._pending_removals)\n\n    def __repr__(self):\n        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n\n    def __setitem__(self, key, value):\n        self.data[ref(key, self._remove)] = value\n\n    def copy(self):\n        new = WeakKeyDictionary()\n        with _IterationGuard(self):\n            for key, value in self.data.items():\n                o = key()\n                if o is not None:\n                    new[o] = value\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        new = self.__class__()\n        with _IterationGuard(self):\n            for key, value in self.data.items():\n                o = key()\n                if o is not None:\n                    new[o] = deepcopy(value, memo)\n        return new\n\n    def get(self, key, default=None):\n        return self.data.get(ref(key),default)\n\n    def __contains__(self, key):\n        try:\n            wr = ref(key)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def items(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.items():\n                key = wr()\n                if key is not None:\n                    yield key, value\n\n    def keys(self):\n        with _IterationGuard(self):\n            for wr in self.data:\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    __iter__ = keys\n\n    def values(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.items():\n                if wr() is not None:\n                    yield value\n\n    def keyrefs(self):\n        \"\"\"Return a list of weak references to the keys.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the keys around longer than needed.\n\n        \"\"\"\n        return list(self.data)\n\n    def popitem(self):\n        self._dirty_len = True\n        while True:\n            key, value = self.data.popitem()\n            o = key()\n            if o is not None:\n                return o, value\n\n    def pop(self, key, *args):\n        self._dirty_len = True\n        return self.data.pop(ref(key), *args)\n\n    def setdefault(self, key, default=None):\n        return self.data.setdefault(ref(key, self._remove),default)\n\n    def update(self, dict=None, /, **kwargs):\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, value in dict.items():\n                d[ref(key, self._remove)] = value\n        if len(kwargs):\n            self.update(kwargs)\n\n\nclass finalize:\n    \"\"\"Class for finalization of weakrefable objects\n\n    finalize(obj, func, *args, **kwargs) returns a callable finalizer\n    object which will be called when obj is garbage collected. The\n    first time the finalizer is called it evaluates func(*arg, **kwargs)\n    and returns the result. After this the finalizer is dead, and\n    calling it just returns None.\n\n    When the program exits any remaining finalizers for which the\n    atexit attribute is true will be run in reverse order of creation.\n    By default atexit is true.\n    \"\"\"\n\n    # Finalizer objects don't have any state of their own.  They are\n    # just used as keys to lookup _Info objects in the registry.  This\n    # ensures that they cannot be part of a ref-cycle.\n\n    __slots__ = ()\n    _registry = {}\n    _shutdown = False\n    _index_iter = itertools.count()\n    _dirty = False\n    _registered_with_atexit = False\n\n    class _Info:\n        __slots__ = (\"weakref\", \"func\", \"args\", \"kwargs\", \"atexit\", \"index\")\n\n    def __init__(*args, **kwargs):\n        if len(args) >= 3:\n            self, obj, func, *args = args\n        elif not args:\n            raise TypeError(\"descriptor '__init__' of 'finalize' object \"\n                            \"needs an argument\")\n        else:\n            if 'func' not in kwargs:\n                raise TypeError('finalize expected at least 2 positional '\n                                'arguments, got %d' % (len(args)-1))\n            func = kwargs.pop('func')\n            if len(args) >= 2:\n                self, obj, *args = args\n                import warnings\n                warnings.warn(\"Passing 'func' as keyword argument is deprecated\",\n                              DeprecationWarning, stacklevel=2)\n            else:\n                if 'obj' not in kwargs:\n                    raise TypeError('finalize expected at least 2 positional '\n                                    'arguments, got %d' % (len(args)-1))\n                obj = kwargs.pop('obj')\n                self, *args = args\n                import warnings\n                warnings.warn(\"Passing 'obj' as keyword argument is deprecated\",\n                              DeprecationWarning, stacklevel=2)\n        args = tuple(args)\n\n        if not self._registered_with_atexit:\n            # We may register the exit function more than once because\n            # of a thread race, but that is harmless\n            import atexit\n            atexit.register(self._exitfunc)\n            finalize._registered_with_atexit = True\n        info = self._Info()\n        info.weakref = ref(obj, self)\n        info.func = func\n        info.args = args\n        info.kwargs = kwargs or None\n        info.atexit = True\n        info.index = next(self._index_iter)\n        self._registry[self] = info\n        finalize._dirty = True\n    __init__.__text_signature__ = '($self, obj, func, /, *args, **kwargs)'\n\n    def __call__(self, _=None):\n        \"\"\"If alive then mark as dead and return func(*args, **kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.pop(self, None)\n        if info and not self._shutdown:\n            return info.func(*info.args, **(info.kwargs or {}))\n\n    def detach(self):\n        \"\"\"If alive then mark as dead and return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None and self._registry.pop(self, None):\n            return (obj, info.func, info.args, info.kwargs or {})\n\n    def peek(self):\n        \"\"\"If alive then return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None:\n            return (obj, info.func, info.args, info.kwargs or {})\n\n    @property\n    def alive(self):\n        \"\"\"Whether finalizer is alive\"\"\"\n        return self in self._registry\n\n    @property\n    def atexit(self):\n        \"\"\"Whether finalizer should be called at exit\"\"\"\n        info = self._registry.get(self)\n        return bool(info) and info.atexit\n\n    @atexit.setter\n    def atexit(self, value):\n        info = self._registry.get(self)\n        if info:\n            info.atexit = bool(value)\n\n    def __repr__(self):\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is None:\n            return '<%s object at %#x; dead>' % (type(self).__name__, id(self))\n        else:\n            return '<%s object at %#x; for %r at %#x>' % \\\n                (type(self).__name__, id(self), type(obj).__name__, id(obj))\n\n    @classmethod\n    def _select_for_exit(cls):\n        # Return live finalizers marked for exit, oldest first\n        L = [(f,i) for (f,i) in cls._registry.items() if i.atexit]\n        L.sort(key=lambda item:item[1].index)\n        return [f for (f,i) in L]\n\n    @classmethod\n    def _exitfunc(cls):\n        # At shutdown invoke finalizers for which atexit is true.\n        # This is called once all other non-daemonic threads have been\n        # joined.\n        reenable_gc = False\n        try:\n            if cls._registry:\n                import gc\n                if gc.isenabled():\n                    reenable_gc = True\n                    gc.disable()\n                pending = None\n                while True:\n                    if pending is None or finalize._dirty:\n                        pending = cls._select_for_exit()\n                        finalize._dirty = False\n                    if not pending:\n                        break\n                    f = pending.pop()\n                    try:\n                        # gc is disabled, so (assuming no daemonic\n                        # threads) the following is the only line in\n                        # this function which might trigger creation\n                        # of a new finalizer\n                        f()\n                    except Exception:\n                        sys.excepthook(*sys.exc_info())\n                    assert f not in cls._registry\n        finally:\n            # prevent any more finalizers from executing during shutdown\n            finalize._shutdown = True\n            if reenable_gc:\n                gc.enable()\n", 650], "/usr/lib/python3.8/asyncio/base_futures.py": ["__all__ = ()\n\nimport reprlib\n\nfrom . import format_helpers\n\n# States for Future.\n_PENDING = 'PENDING'\n_CANCELLED = 'CANCELLED'\n_FINISHED = 'FINISHED'\n\n\ndef isfuture(obj):\n    \"\"\"Check for a Future.\n\n    This returns True when obj is a Future instance or is advertising\n    itself as duck-type compatible by setting _asyncio_future_blocking.\n    See comment in Future for more details.\n    \"\"\"\n    return (hasattr(obj.__class__, '_asyncio_future_blocking') and\n            obj._asyncio_future_blocking is not None)\n\n\ndef _format_callbacks(cb):\n    \"\"\"helper function for Future.__repr__\"\"\"\n    size = len(cb)\n    if not size:\n        cb = ''\n\n    def format_cb(callback):\n        return format_helpers._format_callback_source(callback, ())\n\n    if size == 1:\n        cb = format_cb(cb[0][0])\n    elif size == 2:\n        cb = '{}, {}'.format(format_cb(cb[0][0]), format_cb(cb[1][0]))\n    elif size > 2:\n        cb = '{}, <{} more>, {}'.format(format_cb(cb[0][0]),\n                                        size - 2,\n                                        format_cb(cb[-1][0]))\n    return f'cb=[{cb}]'\n\n\ndef _future_repr_info(future):\n    # (Future) -> str\n    \"\"\"helper function for Future.__repr__\"\"\"\n    info = [future._state.lower()]\n    if future._state == _FINISHED:\n        if future._exception is not None:\n            info.append(f'exception={future._exception!r}')\n        else:\n            # use reprlib to limit the length of the output, especially\n            # for very long strings\n            result = reprlib.repr(future._result)\n            info.append(f'result={result}')\n    if future._callbacks:\n        info.append(_format_callbacks(future._callbacks))\n    if future._source_traceback:\n        frame = future._source_traceback[-1]\n        info.append(f'created at {frame[0]}:{frame[1]}')\n    return info\n", 61], "/usr/lib/python3.8/asyncio/tasks.py": ["\"\"\"Support for tasks, coroutines and the scheduler.\"\"\"\n\n__all__ = (\n    'Task', 'create_task',\n    'FIRST_COMPLETED', 'FIRST_EXCEPTION', 'ALL_COMPLETED',\n    'wait', 'wait_for', 'as_completed', 'sleep',\n    'gather', 'shield', 'ensure_future', 'run_coroutine_threadsafe',\n    'current_task', 'all_tasks',\n    '_register_task', '_unregister_task', '_enter_task', '_leave_task',\n)\n\nimport concurrent.futures\nimport contextvars\nimport functools\nimport inspect\nimport itertools\nimport types\nimport warnings\nimport weakref\n\nfrom . import base_tasks\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom .coroutines import _is_coroutine\n\n# Helper to generate new task names\n# This uses itertools.count() instead of a \"+= 1\" operation because the latter\n# is not thread safe. See bpo-11866 for a longer explanation.\n_task_name_counter = itertools.count(1).__next__\n\n\ndef current_task(loop=None):\n    \"\"\"Return a currently executed task.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    return _current_tasks.get(loop)\n\n\ndef all_tasks(loop=None):\n    \"\"\"Return a set of all tasks for the loop.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    # Looping over a WeakSet (_all_tasks) isn't safe as it can be updated from another\n    # thread while we do so. Therefore we cast it to list prior to filtering. The list\n    # cast itself requires iteration, so we repeat it several times ignoring\n    # RuntimeErrors (which are not very likely to occur). See issues 34970 and 36607 for\n    # details.\n    i = 0\n    while True:\n        try:\n            tasks = list(_all_tasks)\n        except RuntimeError:\n            i += 1\n            if i >= 1000:\n                raise\n        else:\n            break\n    return {t for t in tasks\n            if futures._get_loop(t) is loop and not t.done()}\n\n\ndef _all_tasks_compat(loop=None):\n    # Different from \"all_task()\" by returning *all* Tasks, including\n    # the completed ones.  Used to implement deprecated \"Tasks.all_task()\"\n    # method.\n    if loop is None:\n        loop = events.get_event_loop()\n    # Looping over a WeakSet (_all_tasks) isn't safe as it can be updated from another\n    # thread while we do so. Therefore we cast it to list prior to filtering. The list\n    # cast itself requires iteration, so we repeat it several times ignoring\n    # RuntimeErrors (which are not very likely to occur). See issues 34970 and 36607 for\n    # details.\n    i = 0\n    while True:\n        try:\n            tasks = list(_all_tasks)\n        except RuntimeError:\n            i += 1\n            if i >= 1000:\n                raise\n        else:\n            break\n    return {t for t in tasks if futures._get_loop(t) is loop}\n\n\ndef _set_task_name(task, name):\n    if name is not None:\n        try:\n            set_name = task.set_name\n        except AttributeError:\n            pass\n        else:\n            set_name(name)\n\n\nclass Task(futures._PyFuture):  # Inherit Python Task implementation\n                                # from a Python Future implementation.\n\n    \"\"\"A coroutine wrapped in a Future.\"\"\"\n\n    # An important invariant maintained while a Task not done:\n    #\n    # - Either _fut_waiter is None, and _step() is scheduled;\n    # - or _fut_waiter is some Future, and _step() is *not* scheduled.\n    #\n    # The only transition from the latter to the former is through\n    # _wakeup().  When _fut_waiter is not None, one of its callbacks\n    # must be _wakeup().\n\n    # If False, don't log a message if the task is destroyed whereas its\n    # status is still pending\n    _log_destroy_pending = True\n\n    @classmethod\n    def current_task(cls, loop=None):\n        \"\"\"Return the currently running task in an event loop or None.\n\n        By default the current task for the current event loop is returned.\n\n        None is returned when called not in the context of a Task.\n        \"\"\"\n        warnings.warn(\"Task.current_task() is deprecated since Python 3.7, \"\n                      \"use asyncio.current_task() instead\",\n                      DeprecationWarning,\n                      stacklevel=2)\n        if loop is None:\n            loop = events.get_event_loop()\n        return current_task(loop)\n\n    @classmethod\n    def all_tasks(cls, loop=None):\n        \"\"\"Return a set of all tasks for an event loop.\n\n        By default all tasks for the current event loop are returned.\n        \"\"\"\n        warnings.warn(\"Task.all_tasks() is deprecated since Python 3.7, \"\n                      \"use asyncio.all_tasks() instead\",\n                      DeprecationWarning,\n                      stacklevel=2)\n        return _all_tasks_compat(loop)\n\n    def __init__(self, coro, *, loop=None, name=None):\n        super().__init__(loop=loop)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        if not coroutines.iscoroutine(coro):\n            # raise after Future.__init__(), attrs are required for __del__\n            # prevent logging for pending task in __del__\n            self._log_destroy_pending = False\n            raise TypeError(f\"a coroutine was expected, got {coro!r}\")\n\n        if name is None:\n            self._name = f'Task-{_task_name_counter()}'\n        else:\n            self._name = str(name)\n\n        self._must_cancel = False\n        self._fut_waiter = None\n        self._coro = coro\n        self._context = contextvars.copy_context()\n\n        self._loop.call_soon(self.__step, context=self._context)\n        _register_task(self)\n\n    def __del__(self):\n        if self._state == futures._PENDING and self._log_destroy_pending:\n            context = {\n                'task': self,\n                'message': 'Task was destroyed but it is pending!',\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        super().__del__()\n\n    def _repr_info(self):\n        return base_tasks._task_repr_info(self)\n\n    def get_coro(self):\n        return self._coro\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, value):\n        self._name = str(value)\n\n    def set_result(self, result):\n        raise RuntimeError('Task does not support set_result operation')\n\n    def set_exception(self, exception):\n        raise RuntimeError('Task does not support set_exception operation')\n\n    def get_stack(self, *, limit=None):\n        \"\"\"Return the list of stack frames for this task's coroutine.\n\n        If the coroutine is not done, this returns the stack where it is\n        suspended.  If the coroutine has completed successfully or was\n        cancelled, this returns an empty list.  If the coroutine was\n        terminated by an exception, this returns the list of traceback\n        frames.\n\n        The frames are always ordered from oldest to newest.\n\n        The optional limit gives the maximum number of frames to\n        return; by default all available frames are returned.  Its\n        meaning differs depending on whether a stack or a traceback is\n        returned: the newest frames of a stack are returned, but the\n        oldest frames of a traceback are returned.  (This matches the\n        behavior of the traceback module.)\n\n        For reasons beyond our control, only one stack frame is\n        returned for a suspended coroutine.\n        \"\"\"\n        return base_tasks._task_get_stack(self, limit)\n\n    def print_stack(self, *, limit=None, file=None):\n        \"\"\"Print the stack or traceback for this task's coroutine.\n\n        This produces output similar to that of the traceback module,\n        for the frames retrieved by get_stack().  The limit argument\n        is passed to get_stack().  The file argument is an I/O stream\n        to which the output is written; by default output is written\n        to sys.stderr.\n        \"\"\"\n        return base_tasks._task_print_stack(self, limit, file)\n\n    def cancel(self):\n        \"\"\"Request that this task cancel itself.\n\n        This arranges for a CancelledError to be thrown into the\n        wrapped coroutine on the next cycle through the event loop.\n        The coroutine then has a chance to clean up or even deny\n        the request using try/except/finally.\n\n        Unlike Future.cancel, this does not guarantee that the\n        task will be cancelled: the exception might be caught and\n        acted upon, delaying cancellation of the task or preventing\n        cancellation completely.  The task may also return a value or\n        raise a different exception.\n\n        Immediately after this method is called, Task.cancelled() will\n        not return True (unless the task was already cancelled).  A\n        task will be marked as cancelled when the wrapped coroutine\n        terminates with a CancelledError exception (even if cancel()\n        was not called).\n        \"\"\"\n        self._log_traceback = False\n        if self.done():\n            return False\n        if self._fut_waiter is not None:\n            if self._fut_waiter.cancel():\n                # Leave self._fut_waiter; it may be a Task that\n                # catches and ignores the cancellation so we may have\n                # to cancel it again later.\n                return True\n        # It must be the case that self.__step is already scheduled.\n        self._must_cancel = True\n        return True\n\n    def __step(self, exc=None):\n        if self.done():\n            raise exceptions.InvalidStateError(\n                f'_step(): already done: {self!r}, {exc!r}')\n        if self._must_cancel:\n            if not isinstance(exc, exceptions.CancelledError):\n                exc = exceptions.CancelledError()\n            self._must_cancel = False\n        coro = self._coro\n        self._fut_waiter = None\n\n        _enter_task(self._loop, self)\n        # Call either coro.throw(exc) or coro.send(None).\n        try:\n            if exc is None:\n                # We use the `send` method directly, because coroutines\n                # don't have `__iter__` and `__next__` methods.\n                result = coro.send(None)\n            else:\n                result = coro.throw(exc)\n        except StopIteration as exc:\n            if self._must_cancel:\n                # Task is cancelled right before coro stops.\n                self._must_cancel = False\n                super().cancel()\n            else:\n                super().set_result(exc.value)\n        except exceptions.CancelledError:\n            super().cancel()  # I.e., Future.cancel(self).\n        except (KeyboardInterrupt, SystemExit) as exc:\n            super().set_exception(exc)\n            raise\n        except BaseException as exc:\n            super().set_exception(exc)\n        else:\n            blocking = getattr(result, '_asyncio_future_blocking', None)\n            if blocking is not None:\n                # Yielded Future must come from Future.__iter__().\n                if futures._get_loop(result) is not self._loop:\n                    new_exc = RuntimeError(\n                        f'Task {self!r} got Future '\n                        f'{result!r} attached to a different loop')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n                elif blocking:\n                    if result is self:\n                        new_exc = RuntimeError(\n                            f'Task cannot await on itself: {self!r}')\n                        self._loop.call_soon(\n                            self.__step, new_exc, context=self._context)\n                    else:\n                        result._asyncio_future_blocking = False\n                        result.add_done_callback(\n                            self.__wakeup, context=self._context)\n                        self._fut_waiter = result\n                        if self._must_cancel:\n                            if self._fut_waiter.cancel():\n                                self._must_cancel = False\n                else:\n                    new_exc = RuntimeError(\n                        f'yield was used instead of yield from '\n                        f'in task {self!r} with {result!r}')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n\n            elif result is None:\n                # Bare yield relinquishes control for one event loop iteration.\n                self._loop.call_soon(self.__step, context=self._context)\n            elif inspect.isgenerator(result):\n                # Yielding a generator is just wrong.\n                new_exc = RuntimeError(\n                    f'yield was used instead of yield from for '\n                    f'generator in task {self!r} with {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n            else:\n                # Yielding something else is an error.\n                new_exc = RuntimeError(f'Task got bad yield: {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n        finally:\n            _leave_task(self._loop, self)\n            self = None  # Needed to break cycles when an exception occurs.\n\n    def __wakeup(self, future):\n        try:\n            future.result()\n        except BaseException as exc:\n            # This may also be a cancellation.\n            self.__step(exc)\n        else:\n            # Don't pass the value of `future.result()` explicitly,\n            # as `Future.__iter__` and `Future.__await__` don't need it.\n            # If we call `_step(value, None)` instead of `_step()`,\n            # Python eval loop would use `.send(value)` method call,\n            # instead of `__next__()`, which is slower for futures\n            # that return non-generator iterators from their `__iter__`.\n            self.__step()\n        self = None  # Needed to break cycles when an exception occurs.\n\n\n_PyTask = Task\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CTask is needed for tests.\n    Task = _CTask = _asyncio.Task\n\n\ndef create_task(coro, *, name=None):\n    \"\"\"Schedule the execution of a coroutine object in a spawn task.\n\n    Return a Task object.\n    \"\"\"\n    loop = events.get_running_loop()\n    task = loop.create_task(coro)\n    _set_task_name(task, name)\n    return task\n\n\n# wait() and as_completed() similar to those in PEP 3148.\n\nFIRST_COMPLETED = concurrent.futures.FIRST_COMPLETED\nFIRST_EXCEPTION = concurrent.futures.FIRST_EXCEPTION\nALL_COMPLETED = concurrent.futures.ALL_COMPLETED\n\n\nasync def wait(fs, *, loop=None, timeout=None, return_when=ALL_COMPLETED):\n    \"\"\"Wait for the Futures and coroutines given by fs to complete.\n\n    The sequence futures must not be empty.\n\n    Coroutines will be wrapped in Tasks.\n\n    Returns two sets of Future: (done, pending).\n\n    Usage:\n\n        done, pending = await asyncio.wait(fs)\n\n    Note: This does not raise TimeoutError! Futures that aren't done\n    when the timeout occurs are returned in the second set.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect a list of futures, not {type(fs).__name__}\")\n    if not fs:\n        raise ValueError('Set of coroutines/Futures is empty.')\n    if return_when not in (FIRST_COMPLETED, FIRST_EXCEPTION, ALL_COMPLETED):\n        raise ValueError(f'Invalid return_when value: {return_when}')\n\n    if loop is None:\n        loop = events.get_running_loop()\n    else:\n        warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                      \"and scheduled for removal in Python 3.10.\",\n                      DeprecationWarning, stacklevel=2)\n\n    fs = {ensure_future(f, loop=loop) for f in set(fs)}\n\n    return await _wait(fs, timeout, return_when, loop)\n\n\ndef _release_waiter(waiter, *args):\n    if not waiter.done():\n        waiter.set_result(None)\n\n\nasync def wait_for(fut, timeout, *, loop=None):\n    \"\"\"Wait for the single Future or coroutine to complete, with timeout.\n\n    Coroutine will be wrapped in Task.\n\n    Returns result of the Future or coroutine.  When a timeout occurs,\n    it cancels the task and raises TimeoutError.  To avoid the task\n    cancellation, wrap it in shield().\n\n    If the wait is cancelled, the task is also cancelled.\n\n    This function is a coroutine.\n    \"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    else:\n        warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                      \"and scheduled for removal in Python 3.10.\",\n                      DeprecationWarning, stacklevel=2)\n\n    if timeout is None:\n        return await fut\n\n    if timeout <= 0:\n        fut = ensure_future(fut, loop=loop)\n\n        if fut.done():\n            return fut.result()\n\n        await _cancel_and_wait(fut, loop=loop)\n        try:\n            fut.result()\n        except exceptions.CancelledError as exc:\n            raise exceptions.TimeoutError() from exc\n        else:\n            raise exceptions.TimeoutError()\n\n    waiter = loop.create_future()\n    timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    cb = functools.partial(_release_waiter, waiter)\n\n    fut = ensure_future(fut, loop=loop)\n    fut.add_done_callback(cb)\n\n    try:\n        # wait until the future completes or the timeout\n        try:\n            await waiter\n        except exceptions.CancelledError:\n            if fut.done():\n                return fut.result()\n            else:\n                fut.remove_done_callback(cb)\n                fut.cancel()\n                raise\n\n        if fut.done():\n            return fut.result()\n        else:\n            fut.remove_done_callback(cb)\n            # We must ensure that the task is not running\n            # after wait_for() returns.\n            # See https://bugs.python.org/issue32751\n            await _cancel_and_wait(fut, loop=loop)\n            raise exceptions.TimeoutError()\n    finally:\n        timeout_handle.cancel()\n\n\nasync def _wait(fs, timeout, return_when, loop):\n    \"\"\"Internal helper for wait().\n\n    The fs argument must be a collection of Futures.\n    \"\"\"\n    assert fs, 'Set of Futures is empty.'\n    waiter = loop.create_future()\n    timeout_handle = None\n    if timeout is not None:\n        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    counter = len(fs)\n\n    def _on_completion(f):\n        nonlocal counter\n        counter -= 1\n        if (counter <= 0 or\n            return_when == FIRST_COMPLETED or\n            return_when == FIRST_EXCEPTION and (not f.cancelled() and\n                                                f.exception() is not None)):\n            if timeout_handle is not None:\n                timeout_handle.cancel()\n            if not waiter.done():\n                waiter.set_result(None)\n\n    for f in fs:\n        f.add_done_callback(_on_completion)\n\n    try:\n        await waiter\n    finally:\n        if timeout_handle is not None:\n            timeout_handle.cancel()\n        for f in fs:\n            f.remove_done_callback(_on_completion)\n\n    done, pending = set(), set()\n    for f in fs:\n        if f.done():\n            done.add(f)\n        else:\n            pending.add(f)\n    return done, pending\n\n\nasync def _cancel_and_wait(fut, loop):\n    \"\"\"Cancel the *fut* future or task and wait until it completes.\"\"\"\n\n    waiter = loop.create_future()\n    cb = functools.partial(_release_waiter, waiter)\n    fut.add_done_callback(cb)\n\n    try:\n        fut.cancel()\n        # We cannot wait on *fut* directly to make\n        # sure _cancel_and_wait itself is reliably cancellable.\n        await waiter\n    finally:\n        fut.remove_done_callback(cb)\n\n\n# This is *not* a @coroutine!  It is just an iterator (yielding Futures).\ndef as_completed(fs, *, loop=None, timeout=None):\n    \"\"\"Return an iterator whose values are coroutines.\n\n    When waiting for the yielded coroutines you'll get the results (or\n    exceptions!) of the original Futures (or coroutines), in the order\n    in which and as soon as they complete.\n\n    This differs from PEP 3148; the proper way to use this is:\n\n        for f in as_completed(fs):\n            result = await f  # The 'await' may raise.\n            # Use result.\n\n    If a timeout is specified, the 'await' will raise\n    TimeoutError when the timeout occurs before all Futures are done.\n\n    Note: The futures 'f' are not necessarily members of fs.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect a list of futures, not {type(fs).__name__}\")\n\n    from .queues import Queue  # Import here to avoid circular import problem.\n    done = Queue(loop=loop)\n\n    if loop is None:\n        loop = events.get_event_loop()\n    else:\n        warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                      \"and scheduled for removal in Python 3.10.\",\n                      DeprecationWarning, stacklevel=2)\n    todo = {ensure_future(f, loop=loop) for f in set(fs)}\n    timeout_handle = None\n\n    def _on_timeout():\n        for f in todo:\n            f.remove_done_callback(_on_completion)\n            done.put_nowait(None)  # Queue a dummy value for _wait_for_one().\n        todo.clear()  # Can't do todo.remove(f) in the loop.\n\n    def _on_completion(f):\n        if not todo:\n            return  # _on_timeout() was here first.\n        todo.remove(f)\n        done.put_nowait(f)\n        if not todo and timeout_handle is not None:\n            timeout_handle.cancel()\n\n    async def _wait_for_one():\n        f = await done.get()\n        if f is None:\n            # Dummy value from _on_timeout().\n            raise exceptions.TimeoutError\n        return f.result()  # May raise f.exception().\n\n    for f in todo:\n        f.add_done_callback(_on_completion)\n    if todo and timeout is not None:\n        timeout_handle = loop.call_later(timeout, _on_timeout)\n    for _ in range(len(todo)):\n        yield _wait_for_one()\n\n\n@types.coroutine\ndef __sleep0():\n    \"\"\"Skip one event loop run cycle.\n\n    This is a private helper for 'asyncio.sleep()', used\n    when the 'delay' is set to 0.  It uses a bare 'yield'\n    expression (which Task.__step knows how to handle)\n    instead of creating a Future object.\n    \"\"\"\n    yield\n\n\nasync def sleep(delay, result=None, *, loop=None):\n    \"\"\"Coroutine that completes after a given time (in seconds).\"\"\"\n    if delay <= 0:\n        await __sleep0()\n        return result\n\n    if loop is None:\n        loop = events.get_running_loop()\n    else:\n        warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                      \"and scheduled for removal in Python 3.10.\",\n                      DeprecationWarning, stacklevel=2)\n\n    future = loop.create_future()\n    h = loop.call_later(delay,\n                        futures._set_result_unless_cancelled,\n                        future, result)\n    try:\n        return await future\n    finally:\n        h.cancel()\n\n\ndef ensure_future(coro_or_future, *, loop=None):\n    \"\"\"Wrap a coroutine or an awaitable in a future.\n\n    If the argument is a Future, it is returned directly.\n    \"\"\"\n    if coroutines.iscoroutine(coro_or_future):\n        if loop is None:\n            loop = events.get_event_loop()\n        task = loop.create_task(coro_or_future)\n        if task._source_traceback:\n            del task._source_traceback[-1]\n        return task\n    elif futures.isfuture(coro_or_future):\n        if loop is not None and loop is not futures._get_loop(coro_or_future):\n            raise ValueError('The future belongs to a different loop than '\n                             'the one specified as the loop argument')\n        return coro_or_future\n    elif inspect.isawaitable(coro_or_future):\n        return ensure_future(_wrap_awaitable(coro_or_future), loop=loop)\n    else:\n        raise TypeError('An asyncio.Future, a coroutine or an awaitable is '\n                        'required')\n\n\n@types.coroutine\ndef _wrap_awaitable(awaitable):\n    \"\"\"Helper for asyncio.ensure_future().\n\n    Wraps awaitable (an object with __await__) into a coroutine\n    that will later be wrapped in a Task by ensure_future().\n    \"\"\"\n    return (yield from awaitable.__await__())\n\n_wrap_awaitable._is_coroutine = _is_coroutine\n\n\nclass _GatheringFuture(futures.Future):\n    \"\"\"Helper for gather().\n\n    This overrides cancel() to cancel all the children and act more\n    like Task.cancel(), which doesn't immediately mark itself as\n    cancelled.\n    \"\"\"\n\n    def __init__(self, children, *, loop=None):\n        super().__init__(loop=loop)\n        self._children = children\n        self._cancel_requested = False\n\n    def cancel(self):\n        if self.done():\n            return False\n        ret = False\n        for child in self._children:\n            if child.cancel():\n                ret = True\n        if ret:\n            # If any child tasks were actually cancelled, we should\n            # propagate the cancellation request regardless of\n            # *return_exceptions* argument.  See issue 32684.\n            self._cancel_requested = True\n        return ret\n\n\ndef gather(*coros_or_futures, loop=None, return_exceptions=False):\n    \"\"\"Return a future aggregating results from the given coroutines/futures.\n\n    Coroutines will be wrapped in a future and scheduled in the event\n    loop. They will not necessarily be scheduled in the same order as\n    passed in.\n\n    All futures must share the same event loop.  If all the tasks are\n    done successfully, the returned future's result is the list of\n    results (in the order of the original sequence, not necessarily\n    the order of results arrival).  If *return_exceptions* is True,\n    exceptions in the tasks are treated the same as successful\n    results, and gathered in the result list; otherwise, the first\n    raised exception will be immediately propagated to the returned\n    future.\n\n    Cancellation: if the outer Future is cancelled, all children (that\n    have not completed yet) are also cancelled.  If any child is\n    cancelled, this is treated as if it raised CancelledError --\n    the outer Future is *not* cancelled in this case.  (This is to\n    prevent the cancellation of one child to cause other children to\n    be cancelled.)\n\n    If *return_exceptions* is False, cancelling gather() after it\n    has been marked done won't cancel any submitted awaitables.\n    For instance, gather can be marked done after propagating an\n    exception to the caller, therefore, calling ``gather.cancel()``\n    after catching an exception (raised by one of the awaitables) from\n    gather won't cancel any other awaitables.\n    \"\"\"\n    if not coros_or_futures:\n        if loop is None:\n            loop = events.get_event_loop()\n        else:\n            warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                          \"and scheduled for removal in Python 3.10.\",\n                          DeprecationWarning, stacklevel=2)\n        outer = loop.create_future()\n        outer.set_result([])\n        return outer\n\n    def _done_callback(fut):\n        nonlocal nfinished\n        nfinished += 1\n\n        if outer.done():\n            if not fut.cancelled():\n                # Mark exception retrieved.\n                fut.exception()\n            return\n\n        if not return_exceptions:\n            if fut.cancelled():\n                # Check if 'fut' is cancelled first, as\n                # 'fut.exception()' will *raise* a CancelledError\n                # instead of returning it.\n                exc = exceptions.CancelledError()\n                outer.set_exception(exc)\n                return\n            else:\n                exc = fut.exception()\n                if exc is not None:\n                    outer.set_exception(exc)\n                    return\n\n        if nfinished == nfuts:\n            # All futures are done; create a list of results\n            # and set it to the 'outer' future.\n            results = []\n\n            for fut in children:\n                if fut.cancelled():\n                    # Check if 'fut' is cancelled first, as\n                    # 'fut.exception()' will *raise* a CancelledError\n                    # instead of returning it.\n                    res = exceptions.CancelledError()\n                else:\n                    res = fut.exception()\n                    if res is None:\n                        res = fut.result()\n                results.append(res)\n\n            if outer._cancel_requested:\n                # If gather is being cancelled we must propagate the\n                # cancellation regardless of *return_exceptions* argument.\n                # See issue 32684.\n                outer.set_exception(exceptions.CancelledError())\n            else:\n                outer.set_result(results)\n\n    arg_to_fut = {}\n    children = []\n    nfuts = 0\n    nfinished = 0\n    for arg in coros_or_futures:\n        if arg not in arg_to_fut:\n            fut = ensure_future(arg, loop=loop)\n            if loop is None:\n                loop = futures._get_loop(fut)\n            if fut is not arg:\n                # 'arg' was not a Future, therefore, 'fut' is a new\n                # Future created specifically for 'arg'.  Since the caller\n                # can't control it, disable the \"destroy pending task\"\n                # warning.\n                fut._log_destroy_pending = False\n\n            nfuts += 1\n            arg_to_fut[arg] = fut\n            fut.add_done_callback(_done_callback)\n\n        else:\n            # There's a duplicate Future object in coros_or_futures.\n            fut = arg_to_fut[arg]\n\n        children.append(fut)\n\n    outer = _GatheringFuture(children, loop=loop)\n    return outer\n\n\ndef shield(arg, *, loop=None):\n    \"\"\"Wait for a future, shielding it from cancellation.\n\n    The statement\n\n        res = await shield(something())\n\n    is exactly equivalent to the statement\n\n        res = await something()\n\n    *except* that if the coroutine containing it is cancelled, the\n    task running in something() is not cancelled.  From the POV of\n    something(), the cancellation did not happen.  But its caller is\n    still cancelled, so the yield-from expression still raises\n    CancelledError.  Note: If something() is cancelled by other means\n    this will still cancel shield().\n\n    If you want to completely ignore cancellation (not recommended)\n    you can combine shield() with a try/except clause, as follows:\n\n        try:\n            res = await shield(something())\n        except CancelledError:\n            res = None\n    \"\"\"\n    if loop is not None:\n        warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                      \"and scheduled for removal in Python 3.10.\",\n                      DeprecationWarning, stacklevel=2)\n    inner = ensure_future(arg, loop=loop)\n    if inner.done():\n        # Shortcut.\n        return inner\n    loop = futures._get_loop(inner)\n    outer = loop.create_future()\n\n    def _inner_done_callback(inner):\n        if outer.cancelled():\n            if not inner.cancelled():\n                # Mark inner's result as retrieved.\n                inner.exception()\n            return\n\n        if inner.cancelled():\n            outer.cancel()\n        else:\n            exc = inner.exception()\n            if exc is not None:\n                outer.set_exception(exc)\n            else:\n                outer.set_result(inner.result())\n\n\n    def _outer_done_callback(outer):\n        if not inner.done():\n            inner.remove_done_callback(_inner_done_callback)\n\n    inner.add_done_callback(_inner_done_callback)\n    outer.add_done_callback(_outer_done_callback)\n    return outer\n\n\ndef run_coroutine_threadsafe(coro, loop):\n    \"\"\"Submit a coroutine object to a given event loop.\n\n    Return a concurrent.futures.Future to access the result.\n    \"\"\"\n    if not coroutines.iscoroutine(coro):\n        raise TypeError('A coroutine object is required')\n    future = concurrent.futures.Future()\n\n    def callback():\n        try:\n            futures._chain_future(ensure_future(coro, loop=loop), future)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            if future.set_running_or_notify_cancel():\n                future.set_exception(exc)\n            raise\n\n    loop.call_soon_threadsafe(callback)\n    return future\n\n\n# WeakSet containing all alive tasks.\n_all_tasks = weakref.WeakSet()\n\n# Dictionary containing tasks that are currently active in\n# all running event loops.  {EventLoop: Task}\n_current_tasks = {}\n\n\ndef _register_task(task):\n    \"\"\"Register a new task in asyncio as executed by loop.\"\"\"\n    _all_tasks.add(task)\n\n\ndef _enter_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not None:\n        raise RuntimeError(f\"Cannot enter into task {task!r} while another \"\n                           f\"task {current_task!r} is being executed.\")\n    _current_tasks[loop] = task\n\n\ndef _leave_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not task:\n        raise RuntimeError(f\"Leaving task {task!r} does not match \"\n                           f\"the current task {current_task!r}.\")\n    del _current_tasks[loop]\n\n\ndef _unregister_task(task):\n    \"\"\"Unregister a task.\"\"\"\n    _all_tasks.discard(task)\n\n\n_py_register_task = _register_task\n_py_unregister_task = _unregister_task\n_py_enter_task = _enter_task\n_py_leave_task = _leave_task\n\n\ntry:\n    from _asyncio import (_register_task, _unregister_task,\n                          _enter_task, _leave_task,\n                          _all_tasks, _current_tasks)\nexcept ImportError:\n    pass\nelse:\n    _c_register_task = _register_task\n    _c_unregister_task = _unregister_task\n    _c_enter_task = _enter_task\n    _c_leave_task = _leave_task\n", 981], "/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py": ["import asyncio\n\nasync def io_task():\n    await asyncio.sleep(0.01)\n\nasync def main():\n    t1 = asyncio.create_task(io_task())\n    t2 = asyncio.create_task(io_task())\n    t3 = asyncio.create_task(io_task())\n\n    await t1\n    await t2\n    await t3\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n", 17], "/usr/lib/python3.8/asyncio/futures.py": ["\"\"\"A Future class similar to the one in PEP 3148.\"\"\"\n\n__all__ = (\n    'Future', 'wrap_future', 'isfuture',\n)\n\nimport concurrent.futures\nimport contextvars\nimport logging\nimport sys\n\nfrom . import base_futures\nfrom . import events\nfrom . import exceptions\nfrom . import format_helpers\n\n\nisfuture = base_futures.isfuture\n\n\n_PENDING = base_futures._PENDING\n_CANCELLED = base_futures._CANCELLED\n_FINISHED = base_futures._FINISHED\n\n\nSTACK_DEBUG = logging.DEBUG - 1  # heavy-duty debugging\n\n\nclass Future:\n    \"\"\"This class is *almost* compatible with concurrent.futures.Future.\n\n    Differences:\n\n    - This class is not thread-safe.\n\n    - result() and exception() do not take a timeout argument and\n      raise an exception when the future isn't done yet.\n\n    - Callbacks registered with add_done_callback() are always called\n      via the event loop's call_soon().\n\n    - This class is not compatible with the wait() and as_completed()\n      methods in the concurrent.futures package.\n\n    (In Python 3.4 or later we may be able to unify the implementations.)\n    \"\"\"\n\n    # Class variables serving as defaults for instance variables.\n    _state = _PENDING\n    _result = None\n    _exception = None\n    _loop = None\n    _source_traceback = None\n\n    # This field is used for a dual purpose:\n    # - Its presence is a marker to declare that a class implements\n    #   the Future protocol (i.e. is intended to be duck-type compatible).\n    #   The value must also be not-None, to enable a subclass to declare\n    #   that it is not compatible by setting this to None.\n    # - It is set by __iter__() below so that Task._step() can tell\n    #   the difference between\n    #   `await Future()` or`yield from Future()` (correct) vs.\n    #   `yield Future()` (incorrect).\n    _asyncio_future_blocking = False\n\n    __log_traceback = False\n\n    def __init__(self, *, loop=None):\n        \"\"\"Initialize the future.\n\n        The optional event_loop argument allows explicitly setting the event\n        loop object used by the future. If it's not provided, the future uses\n        the default event loop.\n        \"\"\"\n        if loop is None:\n            self._loop = events.get_event_loop()\n        else:\n            self._loop = loop\n        self._callbacks = []\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n\n    _repr_info = base_futures._future_repr_info\n\n    def __repr__(self):\n        return '<{} {}>'.format(self.__class__.__name__,\n                                ' '.join(self._repr_info()))\n\n    def __del__(self):\n        if not self.__log_traceback:\n            # set_exception() was not called, or result() or exception()\n            # has consumed the exception\n            return\n        exc = self._exception\n        context = {\n            'message':\n                f'{self.__class__.__name__} exception was never retrieved',\n            'exception': exc,\n            'future': self,\n        }\n        if self._source_traceback:\n            context['source_traceback'] = self._source_traceback\n        self._loop.call_exception_handler(context)\n\n    @property\n    def _log_traceback(self):\n        return self.__log_traceback\n\n    @_log_traceback.setter\n    def _log_traceback(self, val):\n        if bool(val):\n            raise ValueError('_log_traceback can only be set to False')\n        self.__log_traceback = False\n\n    def get_loop(self):\n        \"\"\"Return the event loop the Future is bound to.\"\"\"\n        loop = self._loop\n        if loop is None:\n            raise RuntimeError(\"Future object is not initialized.\")\n        return loop\n\n    def cancel(self):\n        \"\"\"Cancel the future and schedule callbacks.\n\n        If the future is already done or cancelled, return False.  Otherwise,\n        change the future's state to cancelled, schedule the callbacks and\n        return True.\n        \"\"\"\n        self.__log_traceback = False\n        if self._state != _PENDING:\n            return False\n        self._state = _CANCELLED\n        self.__schedule_callbacks()\n        return True\n\n    def __schedule_callbacks(self):\n        \"\"\"Internal: Ask the event loop to call all callbacks.\n\n        The callbacks are scheduled to be called as soon as possible. Also\n        clears the callback list.\n        \"\"\"\n        callbacks = self._callbacks[:]\n        if not callbacks:\n            return\n\n        self._callbacks[:] = []\n        for callback, ctx in callbacks:\n            self._loop.call_soon(callback, self, context=ctx)\n\n    def cancelled(self):\n        \"\"\"Return True if the future was cancelled.\"\"\"\n        return self._state == _CANCELLED\n\n    # Don't implement running(); see http://bugs.python.org/issue18699\n\n    def done(self):\n        \"\"\"Return True if the future is done.\n\n        Done means either that a result / exception are available, or that the\n        future was cancelled.\n        \"\"\"\n        return self._state != _PENDING\n\n    def result(self):\n        \"\"\"Return the result this future represents.\n\n        If the future has been cancelled, raises CancelledError.  If the\n        future's result isn't yet available, raises InvalidStateError.  If\n        the future is done and has an exception set, this exception is raised.\n        \"\"\"\n        if self._state == _CANCELLED:\n            raise exceptions.CancelledError\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Result is not ready.')\n        self.__log_traceback = False\n        if self._exception is not None:\n            raise self._exception\n        return self._result\n\n    def exception(self):\n        \"\"\"Return the exception that was set on this future.\n\n        The exception (or None if no exception was set) is returned only if\n        the future is done.  If the future has been cancelled, raises\n        CancelledError.  If the future isn't done yet, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state == _CANCELLED:\n            raise exceptions.CancelledError\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Exception is not set.')\n        self.__log_traceback = False\n        return self._exception\n\n    def add_done_callback(self, fn, *, context=None):\n        \"\"\"Add a callback to be run when the future becomes done.\n\n        The callback is called with a single argument - the future object. If\n        the future is already done when this is called, the callback is\n        scheduled with call_soon.\n        \"\"\"\n        if self._state != _PENDING:\n            self._loop.call_soon(fn, self, context=context)\n        else:\n            if context is None:\n                context = contextvars.copy_context()\n            self._callbacks.append((fn, context))\n\n    # New method not in PEP 3148.\n\n    def remove_done_callback(self, fn):\n        \"\"\"Remove all instances of a callback from the \"call when done\" list.\n\n        Returns the number of callbacks removed.\n        \"\"\"\n        filtered_callbacks = [(f, ctx)\n                              for (f, ctx) in self._callbacks\n                              if f != fn]\n        removed_count = len(self._callbacks) - len(filtered_callbacks)\n        if removed_count:\n            self._callbacks[:] = filtered_callbacks\n        return removed_count\n\n    # So-called internal methods (note: no set_running_or_notify_cancel()).\n\n    def set_result(self, result):\n        \"\"\"Mark the future done and set its result.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        self._result = result\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n\n    def set_exception(self, exception):\n        \"\"\"Mark the future done and set an exception.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        if isinstance(exception, type):\n            exception = exception()\n        if type(exception) is StopIteration:\n            raise TypeError(\"StopIteration interacts badly with generators \"\n                            \"and cannot be raised into a Future\")\n        self._exception = exception\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n        self.__log_traceback = True\n\n    def __await__(self):\n        if not self.done():\n            self._asyncio_future_blocking = True\n            yield self  # This tells Task to wait for completion.\n        if not self.done():\n            raise RuntimeError(\"await wasn't used with future\")\n        return self.result()  # May raise too.\n\n    __iter__ = __await__  # make compatible with 'yield from'.\n\n\n# Needed for testing purposes.\n_PyFuture = Future\n\n\ndef _get_loop(fut):\n    # Tries to call Future.get_loop() if it's available.\n    # Otherwise fallbacks to using the old '_loop' property.\n    try:\n        get_loop = fut.get_loop\n    except AttributeError:\n        pass\n    else:\n        return get_loop()\n    return fut._loop\n\n\ndef _set_result_unless_cancelled(fut, result):\n    \"\"\"Helper setting the result only if the future was not cancelled.\"\"\"\n    if fut.cancelled():\n        return\n    fut.set_result(result)\n\n\ndef _convert_future_exc(exc):\n    exc_class = type(exc)\n    if exc_class is concurrent.futures.CancelledError:\n        return exceptions.CancelledError(*exc.args)\n    elif exc_class is concurrent.futures.TimeoutError:\n        return exceptions.TimeoutError(*exc.args)\n    elif exc_class is concurrent.futures.InvalidStateError:\n        return exceptions.InvalidStateError(*exc.args)\n    else:\n        return exc\n\n\ndef _set_concurrent_future_state(concurrent, source):\n    \"\"\"Copy state from a future to a concurrent.futures.Future.\"\"\"\n    assert source.done()\n    if source.cancelled():\n        concurrent.cancel()\n    if not concurrent.set_running_or_notify_cancel():\n        return\n    exception = source.exception()\n    if exception is not None:\n        concurrent.set_exception(_convert_future_exc(exception))\n    else:\n        result = source.result()\n        concurrent.set_result(result)\n\n\ndef _copy_future_state(source, dest):\n    \"\"\"Internal helper to copy state from another Future.\n\n    The other Future may be a concurrent.futures.Future.\n    \"\"\"\n    assert source.done()\n    if dest.cancelled():\n        return\n    assert not dest.done()\n    if source.cancelled():\n        dest.cancel()\n    else:\n        exception = source.exception()\n        if exception is not None:\n            dest.set_exception(_convert_future_exc(exception))\n        else:\n            result = source.result()\n            dest.set_result(result)\n\n\ndef _chain_future(source, destination):\n    \"\"\"Chain two futures so that when one completes, so does the other.\n\n    The result (or exception) of source will be copied to destination.\n    If destination is cancelled, source gets cancelled too.\n    Compatible with both asyncio.Future and concurrent.futures.Future.\n    \"\"\"\n    if not isfuture(source) and not isinstance(source,\n                                               concurrent.futures.Future):\n        raise TypeError('A future is required for source argument')\n    if not isfuture(destination) and not isinstance(destination,\n                                                    concurrent.futures.Future):\n        raise TypeError('A future is required for destination argument')\n    source_loop = _get_loop(source) if isfuture(source) else None\n    dest_loop = _get_loop(destination) if isfuture(destination) else None\n\n    def _set_state(future, other):\n        if isfuture(future):\n            _copy_future_state(other, future)\n        else:\n            _set_concurrent_future_state(future, other)\n\n    def _call_check_cancel(destination):\n        if destination.cancelled():\n            if source_loop is None or source_loop is dest_loop:\n                source.cancel()\n            else:\n                source_loop.call_soon_threadsafe(source.cancel)\n\n    def _call_set_state(source):\n        if (destination.cancelled() and\n                dest_loop is not None and dest_loop.is_closed()):\n            return\n        if dest_loop is None or dest_loop is source_loop:\n            _set_state(destination, source)\n        else:\n            dest_loop.call_soon_threadsafe(_set_state, destination, source)\n\n    destination.add_done_callback(_call_check_cancel)\n    source.add_done_callback(_call_set_state)\n\n\ndef wrap_future(future, *, loop=None):\n    \"\"\"Wrap concurrent.futures.Future object.\"\"\"\n    if isfuture(future):\n        return future\n    assert isinstance(future, concurrent.futures.Future), \\\n        f'concurrent.futures.Future is expected, got {future!r}'\n    if loop is None:\n        loop = events.get_event_loop()\n    new_future = loop.create_future()\n    _chain_future(future, new_future)\n    return new_future\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CFuture is needed for tests.\n    Future = _CFuture = _asyncio.Future\n", 399], "/usr/lib/python3.8/asyncio/runners.py": ["__all__ = 'run',\n\nfrom . import coroutines\nfrom . import events\nfrom . import tasks\n\n\ndef run(main, *, debug=None):\n    \"\"\"Execute the coroutine and return the result.\n\n    This function runs the passed coroutine, taking care of\n    managing the asyncio event loop and finalizing asynchronous\n    generators.\n\n    This function cannot be called when another asyncio event loop is\n    running in the same thread.\n\n    If debug is True, the event loop will be run in debug mode.\n\n    This function always creates a new event loop and closes it at the end.\n    It should be used as a main entry point for asyncio programs, and should\n    ideally only be called once.\n\n    Example:\n\n        async def main():\n            await asyncio.sleep(1)\n            print('hello')\n\n        asyncio.run(main())\n    \"\"\"\n    if events._get_running_loop() is not None:\n        raise RuntimeError(\n            \"asyncio.run() cannot be called from a running event loop\")\n\n    if not coroutines.iscoroutine(main):\n        raise ValueError(\"a coroutine was expected, got {!r}\".format(main))\n\n    loop = events.new_event_loop()\n    try:\n        events.set_event_loop(loop)\n        if debug is not None:\n            loop.set_debug(debug)\n        return loop.run_until_complete(main)\n    finally:\n        try:\n            _cancel_all_tasks(loop)\n            loop.run_until_complete(loop.shutdown_asyncgens())\n        finally:\n            events.set_event_loop(None)\n            loop.close()\n\n\ndef _cancel_all_tasks(loop):\n    to_cancel = tasks.all_tasks(loop)\n    if not to_cancel:\n        return\n\n    for task in to_cancel:\n        task.cancel()\n\n    loop.run_until_complete(\n        tasks.gather(*to_cancel, loop=loop, return_exceptions=True))\n\n    for task in to_cancel:\n        if task.cancelled():\n            continue\n        if task.exception() is not None:\n            loop.call_exception_handler({\n                'message': 'unhandled exception during asyncio.run() shutdown',\n                'exception': task.exception(),\n                'task': task,\n            })\n", 73]}, "functions": {"iscoroutine (/usr/lib/python3.8/asyncio/coroutines.py:177)": ["/usr/lib/python3.8/asyncio/coroutines.py", 177], "__init__ (/usr/lib/python3.8/asyncio/events.py:625)": ["/usr/lib/python3.8/asyncio/events.py", 625], "__init__ (/usr/lib/python3.8/asyncio/unix_events.py:1341)": ["/usr/lib/python3.8/asyncio/unix_events.py", 1341], "_init_event_loop_policy (/usr/lib/python3.8/asyncio/events.py:711)": ["/usr/lib/python3.8/asyncio/events.py", 711], "get_event_loop_policy (/usr/lib/python3.8/asyncio/events.py:719)": ["/usr/lib/python3.8/asyncio/events.py", 719], "encode (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/os.py:748)": ["/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/os.py", 748], "__getitem__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/os.py:670)": ["/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/os.py", 670], "get (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_collections_abc.py:657)": ["/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_collections_abc.py", 657], "_is_debug_mode (/usr/lib/python3.8/asyncio/coroutines.py:18)": ["/usr/lib/python3.8/asyncio/coroutines.py", 18], "is_running (/usr/lib/python3.8/asyncio/base_events.py:658)": ["/usr/lib/python3.8/asyncio/base_events.py", 658], "set_debug (/usr/lib/python3.8/asyncio/base_events.py:1880)": ["/usr/lib/python3.8/asyncio/base_events.py", 1880], "__init__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:36)": ["/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py", 36], "__init__ (/usr/lib/python3.8/asyncio/base_events.py:386)": ["/usr/lib/python3.8/asyncio/base_events.py", 386], "__init__ (/usr/lib/python3.8/selectors.py:63)": ["/usr/lib/python3.8/selectors.py", 63], "__init__ (/usr/lib/python3.8/selectors.py:209)": ["/usr/lib/python3.8/selectors.py", 209], "__init__ (/usr/lib/python3.8/selectors.py:347)": ["/usr/lib/python3.8/selectors.py", 347], "_acquireLock (/usr/lib/python3.8/logging/__init__.py:214)": ["/usr/lib/python3.8/logging/__init__.py", 214], "getEffectiveLevel (/usr/lib/python3.8/logging/__init__.py:1663)": ["/usr/lib/python3.8/logging/__init__.py", 1663], "_releaseLock (/usr/lib/python3.8/logging/__init__.py:223)": ["/usr/lib/python3.8/logging/__init__.py", 223], "isEnabledFor (/usr/lib/python3.8/logging/__init__.py:1677)": ["/usr/lib/python3.8/logging/__init__.py", 1677], "debug (/usr/lib/python3.8/logging/__init__.py:1412)": ["/usr/lib/python3.8/logging/__init__.py", 1412], "__init__ (/usr/lib/python3.8/socket.py:219)": ["/usr/lib/python3.8/socket.py", 219], "socketpair (/usr/lib/python3.8/socket.py:558)": ["/usr/lib/python3.8/socket.py", 558], "_check_closed (/usr/lib/python3.8/asyncio/base_events.py:506)": ["/usr/lib/python3.8/asyncio/base_events.py", 506], "get_debug (/usr/lib/python3.8/asyncio/base_events.py:1877)": ["/usr/lib/python3.8/asyncio/base_events.py", 1877], "__init__ (/usr/lib/python3.8/asyncio/events.py:32)": ["/usr/lib/python3.8/asyncio/events.py", 32], "get_map (/usr/lib/python3.8/selectors.py:272)": ["/usr/lib/python3.8/selectors.py", 272], "_fileobj_to_fd (/usr/lib/python3.8/selectors.py:21)": ["/usr/lib/python3.8/selectors.py", 21], "_fileobj_lookup (/usr/lib/python3.8/selectors.py:215)": ["/usr/lib/python3.8/selectors.py", 215], "__getitem__ (/usr/lib/python3.8/selectors.py:69)": ["/usr/lib/python3.8/selectors.py", 69], "get_key (/usr/lib/python3.8/selectors.py:180)": ["/usr/lib/python3.8/selectors.py", 180], "register (/usr/lib/python3.8/selectors.py:234)": ["/usr/lib/python3.8/selectors.py", 234], "register (/usr/lib/python3.8/selectors.py:351)": ["/usr/lib/python3.8/selectors.py", 351], "_add_reader (/usr/lib/python3.8/asyncio/selector_events.py:257)": ["/usr/lib/python3.8/asyncio/selector_events.py", 257], "_make_self_pipe (/usr/lib/python3.8/asyncio/selector_events.py:106)": ["/usr/lib/python3.8/asyncio/selector_events.py", 106], "update (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/weakref.py:284)": ["/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/weakref.py", 284], "__init__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/weakref.py:102)": ["/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/weakref.py", 102], "__init__ (/usr/lib/python3.8/asyncio/selector_events.py:54)": ["/usr/lib/python3.8/asyncio/selector_events.py", 54], "__init__ (/usr/lib/python3.8/asyncio/unix_events.py:53)": ["/usr/lib/python3.8/asyncio/unix_events.py", 53], "new_event_loop (/usr/lib/python3.8/asyncio/events.py:650)": ["/usr/lib/python3.8/asyncio/events.py", 650], "new_event_loop (/usr/lib/python3.8/asyncio/events.py:756)": ["/usr/lib/python3.8/asyncio/events.py", 756], "set_event_loop (/usr/lib/python3.8/asyncio/events.py:644)": ["/usr/lib/python3.8/asyncio/events.py", 644], "set_event_loop (/usr/lib/python3.8/asyncio/unix_events.py:1353)": ["/usr/lib/python3.8/asyncio/unix_events.py", 1353], "set_event_loop (/usr/lib/python3.8/asyncio/events.py:751)": ["/usr/lib/python3.8/asyncio/events.py", 751], "_check_running (/usr/lib/python3.8/asyncio/base_events.py:550)": ["/usr/lib/python3.8/asyncio/base_events.py", 550], "isfuture (/usr/lib/python3.8/asyncio/base_futures.py:13)": ["/usr/lib/python3.8/asyncio/base_futures.py", 13], "_call_soon (/usr/lib/python3.8/asyncio/base_events.py:738)": ["/usr/lib/python3.8/asyncio/base_events.py", 738], "call_soon (/usr/lib/python3.8/asyncio/base_events.py:709)": ["/usr/lib/python3.8/asyncio/base_events.py", 709], "add (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:81)": ["/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py", 81], "create_task (/usr/lib/python3.8/asyncio/base_events.py:424)": ["/usr/lib/python3.8/asyncio/base_events.py", 424], "ensure_future (/usr/lib/python3.8/asyncio/tasks.py:661)": ["/usr/lib/python3.8/asyncio/tasks.py", 661], "_set_coroutine_origin_tracking (/usr/lib/python3.8/asyncio/base_events.py:1862)": ["/usr/lib/python3.8/asyncio/base_events.py", 1862], "select (/usr/lib/python3.8/selectors.py:451)": ["/usr/lib/python3.8/selectors.py", 451], "_process_events (/usr/lib/python3.8/asyncio/selector_events.py:586)": ["/usr/lib/python3.8/asyncio/selector_events.py", 586], "time (/usr/lib/python3.8/asyncio/base_events.py:662)": ["/usr/lib/python3.8/asyncio/base_events.py", 662], "_set_task_name (/usr/lib/python3.8/asyncio/tasks.py:88)": ["/usr/lib/python3.8/asyncio/tasks.py", 88], "create_task (/usr/lib/python3.8/asyncio/tasks.py:376)": ["/usr/lib/python3.8/asyncio/tasks.py", 376], "main (/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py:6)": ["/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py", 6], "_run (/usr/lib/python3.8/asyncio/events.py:79)": ["/usr/lib/python3.8/asyncio/events.py", 79], "_run_once (/usr/lib/python3.8/asyncio/base_events.py:1784)": ["/usr/lib/python3.8/asyncio/base_events.py", 1784], "create_future (/usr/lib/python3.8/asyncio/base_events.py:420)": ["/usr/lib/python3.8/asyncio/base_events.py", 420], "__init__ (/usr/lib/python3.8/asyncio/events.py:104)": ["/usr/lib/python3.8/asyncio/events.py", 104], "call_at (/usr/lib/python3.8/asyncio/base_events.py:693)": ["/usr/lib/python3.8/asyncio/base_events.py", 693], "call_later (/usr/lib/python3.8/asyncio/base_events.py:671)": ["/usr/lib/python3.8/asyncio/base_events.py", 671], "sleep (/usr/lib/python3.8/asyncio/tasks.py:638)": ["/usr/lib/python3.8/asyncio/tasks.py", 638], "io_task (/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py:3)": ["/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py", 3], "__lt__ (/usr/lib/python3.8/asyncio/events.py:121)": ["/usr/lib/python3.8/asyncio/events.py", 121], "_set_result_unless_cancelled (/usr/lib/python3.8/asyncio/futures.py:284)": ["/usr/lib/python3.8/asyncio/futures.py", 284], "_timer_handle_cancelled (/usr/lib/python3.8/asyncio/base_events.py:1779)": ["/usr/lib/python3.8/asyncio/base_events.py", 1779], "cancel (/usr/lib/python3.8/asyncio/events.py:65)": ["/usr/lib/python3.8/asyncio/events.py", 65], "cancel (/usr/lib/python3.8/asyncio/events.py:149)": ["/usr/lib/python3.8/asyncio/events.py", 149], "_get_loop (/usr/lib/python3.8/asyncio/futures.py:272)": ["/usr/lib/python3.8/asyncio/futures.py", 272], "stop (/usr/lib/python3.8/asyncio/base_events.py:618)": ["/usr/lib/python3.8/asyncio/base_events.py", 618], "_run_until_complete_cb (/usr/lib/python3.8/asyncio/base_events.py:184)": ["/usr/lib/python3.8/asyncio/base_events.py", 184], "run_forever (/usr/lib/python3.8/asyncio/base_events.py:557)": ["/usr/lib/python3.8/asyncio/base_events.py", 557], "run_until_complete (/usr/lib/python3.8/asyncio/base_events.py:580)": ["/usr/lib/python3.8/asyncio/base_events.py", 580], "__len__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:67)": ["/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py", 67], "__init__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:16)": ["/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py", 16], "__enter__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:20)": ["/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py", 20], "__iter__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:58)": ["/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py", 58], "_commit_removals (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:52)": ["/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py", 52], "__exit__ (/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py:26)": ["/home/gaogaotiantian/programs/codesnap/venv3.8/lib/python3.8/_weakrefset.py", 26], "<setcomp> (/usr/lib/python3.8/asyncio/tasks.py:60)": ["/usr/lib/python3.8/asyncio/tasks.py", 60], "all_tasks (/usr/lib/python3.8/asyncio/tasks.py:41)": ["/usr/lib/python3.8/asyncio/tasks.py", 41], "_cancel_all_tasks (/usr/lib/python3.8/asyncio/runners.py:54)": ["/usr/lib/python3.8/asyncio/runners.py", 54], "shutdown_asyncgens (/usr/lib/python3.8/asyncio/base_events.py:524)": ["/usr/lib/python3.8/asyncio/base_events.py", 524], "is_closed (/usr/lib/python3.8/asyncio/base_events.py:648)": ["/usr/lib/python3.8/asyncio/base_events.py", 648], "unregister (/usr/lib/python3.8/selectors.py:247)": ["/usr/lib/python3.8/selectors.py", 247], "unregister (/usr/lib/python3.8/selectors.py:365)": ["/usr/lib/python3.8/selectors.py", 365], "_remove_reader (/usr/lib/python3.8/asyncio/selector_events.py:272)": ["/usr/lib/python3.8/asyncio/selector_events.py", 272], "_real_close (/usr/lib/python3.8/socket.py:492)": ["/usr/lib/python3.8/socket.py", 492], "close (/usr/lib/python3.8/socket.py:496)": ["/usr/lib/python3.8/socket.py", 496], "_close_self_pipe (/usr/lib/python3.8/asyncio/selector_events.py:98)": ["/usr/lib/python3.8/asyncio/selector_events.py", 98], "close (/usr/lib/python3.8/asyncio/base_events.py:626)": ["/usr/lib/python3.8/asyncio/base_events.py", 626], "close (/usr/lib/python3.8/selectors.py:268)": ["/usr/lib/python3.8/selectors.py", 268], "close (/usr/lib/python3.8/selectors.py:483)": ["/usr/lib/python3.8/selectors.py", 483], "close (/usr/lib/python3.8/asyncio/selector_events.py:87)": ["/usr/lib/python3.8/asyncio/selector_events.py", 87], "close (/usr/lib/python3.8/asyncio/unix_events.py:57)": ["/usr/lib/python3.8/asyncio/unix_events.py", 57], "run (/usr/lib/python3.8/asyncio/runners.py:8)": ["/usr/lib/python3.8/asyncio/runners.py", 8], "<module> (/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py:1)": ["/home/gaogaotiantian/programs/codesnap/example/src/async_simple.py", 1]}}}